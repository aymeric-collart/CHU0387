---
title: "Weeks 6 & 7: Building a corpus from media platforms"
format: 
  html:
    output-file: Weeks6_7.html
    number-sections: true
    number-depth: 2
    toc: true
---

```{=html}
<style>
hr.rounded {
  border-top: 1px solid #E5E4E2;
  border-radius: 5px;
}
</style>
```

<br>

::: {.callout-warning icon="false"}
## Objectives

These two weeks, you will learn how to use R to build a corpus from online Taiwanese newspapers.
:::

# Building your own corpus: Conceptual and technical considerations

There are many tools that we can use now to build a corpus based on data coming from the Internet, and this includes R packages. But many concepts need to be clarified before getting into the coding part, such as:

-   Being aware of the *steps* you are going through if you had to manually copy and paste the data you want into an Excel file, and document each step with as many details as possible,

-   Understanding the *structure* of a website, how to *access* the structure of the website, and *where* the information you want are,

-   Finally, *mapping* the R functions to the information you want.

These are the steps we will be going through these two weeks. We will work with one Taiwanese newspaper called "ETtoday" (<https://www.ettoday.net/>). We will be working with this link, which references all articles since 2011: <https://www.ettoday.net/news/news-list.htm>.

![](assets/images/ETtoday_logo.png)

::: {.callout-warning icon="false"}
## More about ETtoday (source: Wikipedia)

ETtoday is one of the main newspapers in Taiwan. It has been officially founded in 2011, and it publishes numerous articles on many topics online every day. Some may say that it is not politically neutral, so it is something to keep in mind as this may affect the results depending on your research question.
:::

# Document your own manual steps

## What is the task

First, you need to be very clear about what you want to do, otherwise you can literally spend hours doing a lot of things but actually achieving nothing.

So here is your first task:

1.  Go to the ETtoday website, and search for one article.

2.  Describe each step you take to get to an article, and document the changes that happen regarding **the address of the website**.

## Documentation of the manual steps

This task seems quite easy, but it is crucial. First, you went to the website by clicking on the link. Then, you selected the date of the news you were interested in. Third, you selected the type of news (politics, society, etc.). A list of articles was already there on the website, and you were just filtering them. And to get into one article, you just clicked on one of the titles below the search bar.

This procedure looks like this:

```{r setup, include = FALSE}
library(DiagrammeR)
```

```{r mermaid-example, echo=FALSE, fig.align='center'}
    mermaid("
    graph TD
        A[ETtoday website with list of articles] -- Select date --> B{Filtered articles};
        A -- Select type --> B{Filtered articles};
        B --> C[Click on the title of one article];
        C --> D[New webpage: Individual article];
        D --> E[Copy and paste the title, date and content of the article into Excel, Word, etc.]
    ")
```

Now we will do the same, but pay more attention to **the web address**, and track the changes that occur. For instance, the first step is just accessing the website. The web address is "<https://www.ettoday.net/news/news-list.htm>". What is the web address when you select a particular date, let's say October 28th, 2024? And when you select a particular type of article, "politics" for example? And finally, when you choose an individual article?

Here is what we get:

-   First step: <https://www.ettoday.net/news/news-list.htm>

-   Second step (filtered articles): <https://www.ettoday.net/news/news-list-2024-10-28-1.htm>

-   Third step: (individual article): <https://www.ettoday.net/news/20241028/2843953.htm>

::: callout-tip
## Think about it

-   First, we can see that filtering the data changes the address of the website in a very systematic way. Now, try to change the address to access the list of articles published on September 27th, 2024. You will get something like that: <https://www.ettoday.net/news/news-list-2024-09-27-1.htm>
-   Second, try to change the last number. You will see that it changes the type of articles.

**So this means that you can filter the list of articles as you wish just by changing the web address!**

-   Third, if we look at the web address of the individual article, we can retrieve the date of publication of the article very easily... but it is not quite possible to guess the numbers after that.
:::

No need to despair! The fact that this article is found in the list means that ***the web address of the individual article*** must be somewhere on the website... but where? This is the moment when it is important to **understand the structure of the website**.

# Understanding the structure of the website

## How to access the structure of the website?

Accessing the structure of a website is actually very easy. You just need to open the webpage, for example this one: <https://www.ettoday.net/news/news-list-2024-10-28-1.htm>. Then, you look for a blank space, you right-click, and you select "View Page Source" (it is possible that it will not be in English, or that the wording is different depending on the system of your computer).

Then, it will open a new page which can be very scary... but this is actually the same, just the structure of the previous page!

Here is the animated figure of these steps:

![](assets/images/ETtoday_ListArticles_Structure.gif)

## What are all these lines about?

This version of the website is really where you can find all the information that you need. For example, please have a look at lines 514 to 541 (see the image below).

![](assets/images/ETtoday_ListArticleTypes.png)

This must be more familiar to you: these are the web addresses that you obtain when you filter different types of articles!

Now, please have a look at line 549... you will see that you have the web addresses of each individual article listed on the webpage!

![](assets/images/ETtoday_ListAddressIndividualArticles.png)

::: callout-tip
## How did I know what lines to search for?

There's nothing magical, and I am surely not a psychic who can guess the line numbers. So how to find them? I just use the "search" function (for Windows: "ctrl + f", for Mac: "command + f"). And then I look for some keywords, such as the names of the types of articles, or the web address of the individual articles.
:::

## Terminology of the "structural lines"

Let's call these lines the "structural lines". These are just lines of code used to create the webpage. There are different programming languages, but for our purpose, we will only need to look at the lines written in **html language**.

Let's just have a look at line 549 and decompose it.

```{r, eval=FALSE}
<div class="part_list_2">
  <h3>
    <span class="date">2024/10/28 20:45</span>
    <em class="tag c_news">政治</em>
    <a target="_blank" href="https://www.ettoday.net/news/20241028/2843953.htm">陳吉仲列超思案被告！黃國昌「哇一聲」：北檢終於醒來啦</a>
  </h3>
</div>
```

This line corresponds to this part of the website:

![](assets/images/ETtoday_LineExample.png)

Now we can decompose it:

-   The part with "\<div\>\</div\>" means that there is a block on the webpage. This block corresponds to the one where all the individual articles are listed. There are many blocks on a webpage, so we can give them different names to avoid confusing them. Here, the block is called "part_list_2".

-   The part with "\<h3\>\</h3\>" corresponds to one line in the block. What is between "\<h3\>" and "\</h3\>" is everything we want to put in that line. Here there are three elements:

    -   The first element is a "\<span\>\</span\>" element, and its name is "date". What is between "\<span\>" and "\</span\>" is the text to display, here: "2024/10/28 20:45".

    -   The second element is an "\<em\>\</em\>" element (why "em" and not "span"? "em" stands for "emphasis", so they look different on the website). Its name is "tag c_news".

    -   The third element is an "\<a\>\</a\>" element. This is used to write a text with a link, and the link is specified with "href". The term "target="\_blank"" means that it opens in a new page when you click on it.

::: callout-important
This seems to be a lot and quite complicated.

So just remember this:

-   There are **big** **blocks** with different names (*div*, *h3*, *span*, etc.). Let's call them ***nodes***.

-   Inside nodes, you can specify the **parameters** and **names** specific to the **nodes**.

That's all!
:::

# Let's get R do the job for us!

## Overview of the steps

In Section 2.2, we wrote down the several steps we have been going through to copy and paste the content of one article. In the end, we want to repeat this step many times to get the data from more than one article... and now you can imagine why it is a better idea to process through these steps automatically instead of doing it manually for weeks and months.

::: callout-tip
## Think about it

If you followed this rationale, now you can guess what *coding* means. It is just about *translating* our behavior to a language that the computer can understand!

In other words, we are going to *map* the steps described in Section 2.2 to R functions to perform the same task. And then we will require R to repeat the same task with different web addresses.
:::

An efficient way to be clear about the steps to proceed with when doing programming language is to translate the steps we went through manually into words that make sense for the computer. For instance, when we select the dates or article types for filtering, we actually store them somewhere as variables. This gives us the annotated figure below.

![](assets/images/ProcessWebScraping_ArticlesNewspaper.png)

Now let's see how to implement these steps with R.

## Preliminary step: Libraries needed

For this task, we will only need four packages:

```{r, eval=FALSE}
library(rvest)
library(dplyr)
library(xml2)
library(openxlsx)
```

## Step 1: Record the web addresses that list the pages of the individual articles based on the filters

## Step 2: Retrieve the URLs of the individual articles

## Step 3: Retrieve the titles, date, types and content of the articles

## Step 4: Save the data into Excel and R files

# Markdown document, PDF output file, RData and Excel files of the scraped data

You can find the pre-filled Markdown document of this section <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ETToday_Scrape.Rmd" target="_blank">here</a>. <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ETToday_Scrape.pdf" target="_blank">Here</a> is the PDF output of the same document.

Please note that **it takes several hours to run the Markdown document**. In the end it will download data from more than 20000 pages. In case, the RData output file can be downloaded <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_CorpusCourse.Rdata" target="_blank">here</a>. The Excel file corresponding to this dataset is <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_CorpusCourse.xlsx" target="_blank">here</a>.
