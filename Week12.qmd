---
title: "Week 12: Data analysis"
format: 
  html:
    output-file: Week12.html
    number-sections: true
    number-depth: 2
    toc: true
---

```{=html}
<style>
hr.rounded {
  border-top: 1px solid #E5E4E2;
  border-radius: 5px;
}
</style>
```

<br>

::: {.callout-warning icon="false"}
## Objectives

This week, you will learn how to analyze your data quickly and automatically with R.

!! This is an overview of how the analysis process, without reference to statistical modeling!
:::

::: {.callout-note icon="false"}
The data analysis is exemplified with the ETtoday dataset that we cleaned last week. In case, you can download the R data using this link: <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_CorpusCourse_CLEAN.Rdata" target="_blank">here</a>
:::

# How to analyze corpus data?

Let's recap first what we learned and did in the past weeks:

-   We learned what a corpus linguistics study is, the questions we can ask, and the types of corpora we can/should select to conduct the study;

-   We learned how to automatically scrape data from the Internet, with examples from different sources;

-   Last week, we learned how to "clean" the raw data, how to prepare the data for further analyses.

And this is the main objective for this week: **How to analyze the data that we prepared?**

Before getting into the details, we need to keep in mind the overall workflow of a corpus study. (You can zoom in)

```{r mermaid-example, echo=FALSE, fig.align='center'}
    DiagrammeR::mermaid(diagram = "
    graph TD
        A[Research question] --> B{Do I need corpus data};
        B -- No --> C[Corpus study maybe not appropriate];
        B -- Yes --> D[Proceed to the corpus study!];
        D --> E[Select the right corpus];
        E --> F[Select the analysis method];
        F -- Word lists, KWIC, collocates, clusters, random forests, etc. --> G[Proceed to the automatic analysis];
        G --> H[Interpret the results according to the research question];
    ")

```

::: {.callout-note icon="true"}
There are many many analyses methods. Their common point is that they involve automatic processing, which is very helpful when we are dealing with large datasets. Some analyses methods are quite straightforward and traditional (e.g., frequency tables), while some others are more on the computational side. Always remember to select the analysis method that is appropriate for your research question!
:::

For this class, two methods will be presented: Word lists and KWIC (Key Word In Context). These are classic yet very useful methods, and easy to understand and implement for any kind of research.

## KWIC (Key Word In Context)

The first type of analysis to introduce is called **Key Word In Context**, more often found under the acronym ***kwic***. The idea is that we can better understand the meaning of a word based on how it is used in a sentence. KWIC analyses are therefore very suitable **when we target a specific word or syntactic construction**.

KWIC analyses can be used for:

-   *Semantic analyses*: Idea that the meaning of a specific word can be retrieved from its use in a sentences, based on the meaning of its *neighbors*.

-   *Morphosyntactic analyses*: Idea that the morphosyntactic context where a word or a construction is used is helpful in understanding its specific morphological and syntactic features, as well as the meaning it conveys.

Here is an example below, with the Mandarin word *keai* 'cute' (notice that these are fake data just for an illustration):

+----------------+-------------------+---------------+---------------+
| Sentence index | Before keyword    | Keyword       | After keyword |
+================+===================+===============+===============+
| Sentence 1     | hen               | keai          | de mao        |
|                |                   |               |               |
|                | (very)            | (cute)        | (DE cat)      |
+----------------+-------------------+---------------+---------------+
| Sentence 2     | feichang          | keai          | de gou        |
|                |                   |               |               |
|                | (very, extremely) | (cute)        | (DE dog)      |
+----------------+-------------------+---------------+---------------+

Based on these data, and assuming that these are the most frequent instances that we found in the corpus, we can infer that:

-   *Semantically*, the word *keai* is most often used to describe *animals* or *pets*, based on the following segment;

-   *Morphosyntactically*, the word *keai* behaves as an adjective (or stative verb, depending on the analysis), based on the preceding segment.

## Word lists based on frequency

Another extremely common way to analyse corpus data is to count the frequency of each word, in order to have an idea of the most frequently used ones. It is easy to understand: we just need to count how many times a word occurred in the dataset we have. Hopefully, we obtain a list like the one below (again, these are fake data):

+----------------+---------------+
| Word           | Frequency     |
+================+===============+
| keai (cute)    | 368           |
+----------------+---------------+
| kuaile (happy) | 354           |
+----------------+---------------+

The reality is that it is a little bit more complex that it seems to be, and we need to keep several remarks in mind:

-   Without any further data handling, it is more than likely that the most frequent words are (a) punctuation marks, and (b) grammatical markers (the so-called 'closed-class' words), since they are limited and appear obligatory in each sentence. The bad news is that you need further steps to obtain the table you wish for. The good news is that you can use this piece of information as a sanitary check. If you compute the frequency tables and it is not the case that grammatical words are most frequent, then something bad happened!

-   Defining what a "word" is is not easy. In English, the simplest way is to say that words can be separated with a space (even if this too simple definition is misleading). In Mandarin, there are no spaces between words... People created packages with dictionaries where words are listed such that we can still cut the sentences into words, but be aware that less common or newly created ones will not be detected! If your research question is really about new words, then you may consider adding them in the computer's dictionary beforehand.

-   Frequency tables can be further annotated, as you can add the rank of the word, the frequency in terms of percentage in addition to raw count numbers, etc.

## Combining KWIC and frequency-based word lists

Every kind of analysis has pros and cons, and we cannot say that one is better than another. Again, there are just better suited ways to analyze your data according to your research question. This even means that you can combine two types of analyses to obtain more insights!

For example, you can first proceed with a KWIC analysis, and you obtain the table as above. Then, in a second step, you can create the frequency table of the first word following or preceding the keyword. So it is a "KWIC + word list" analysis!

# Let's do the job with R

Now we have a better ideas of how to analyze the data *conceptually*. But how to do it *technically*? This is what this section is about.

First, you can download the script <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/Markdown_Week12_Analyzing_data.Rmd" target="_blank">here</a>, and follow the steps below to understand how it works. But before explaining this script, more remarks are needed.

## The *quanteda* package to conduct corpus analyses

We are very lucky that many smart and generous people around the world created R packages especially to deal with corpus data, and such packages are still being updated at the moment I am writing this section.

We are going to use the package called "quanteda". You can find more information by clicking on this [link](https://quanteda.io/).

![](assets/images/Quanteda.png)

We will also use another package developed by the same team, called "quanteda.textstats".

```{r, eval=FALSE}
install.packages("quanteda")
install.packages("quanteda.textstats")
```

## Some notes when dealing with Mandarin data

As I mentioned earlier, dealing with Mandarin characters is quite tricky, especially when we need to cut the sentences into words. Again, people developed packages especially for that. The most popular one is the "jiebaR" packages ([link](https://github.com/qinwf/jiebaR)). However, it is not possible to install this package directly from R. So you will need to install the "devtools" package first, such that you can install the "jiebaR" and "jiebaRD" directly from the source (Github website).

```{r, eval=FALSE}
install.packages("devtools")
library(devtools)
install_github("qinwf/jiebaRD")
install_github("qinwf/jiebaR")
```

## Explanation of the Markdown script

\[Disclaimer: The explanation of the R codes is made by <i>Gemini</i> as an illustration of the use of such tools to decode a script.\]

### Prepare the environment

This section loads the necessary packages for text analysis. Notably, it introduces quanteda, a powerful package for managing textual data, and jiebaR, which is essential for segmenting (cutting) Chinese text.

#### Load the libraries

```{r, eval=FALSE}
library(quanteda)           # Main package for quantitative text analysis
library(quanteda.textstats) # Statistics for quanteda (frequencies, etc.)
library(jiebaR)             # Chinese text segmentation
library(tidytext)           # Tidy tools for text mining
library(dplyr)              # Data manipulation
library(openxlsx)           # Saving Excel files
```

#### Load the originally scraped data

This loads the .Rdata file containing the cleaned Article_total2 dataframe from the previous week's cleaning process.

```{r, eval=FALSE}
load(file = "ArticleETToday_CorpusCourse_CLEAN.Rdata")
```

### Key Word In Context (KWIC)

This section performs a KWIC analysis on the character "有" *you* (to have). The goal is to identify what words appear immediately after "有" (*you*) and filter for cases where that following word is a verb.

#### Set the segmenter (for Chinese)

Two different "workers" (segmentation engines) are initialized:

-   seg_word: Cuts sentences into individual words.

-   seg_POS: Cuts sentences and tags the Part-Of-Speech (POS) for each word.

```{r, eval=FALSE}
seg_word <- worker(bylines = T, symbol=T)
seg_POS <- worker(type = "tag", symbol = F)
```

#### Prepare the dataset for the analyses

Before analysis, the text must be tokenized. It creates a unique docname for every row. It uses segment() to cut the Chinese text in the body column. It converts the result into a tokens object (the format quanteda requires).

```{r, eval=FALSE}
Article_total2$docname <- paste0("text", 1:nrow(Article_total2))

Article_tokens <- Article_total2$body %>%
  segment(jiebar = seg_word) %>%
  as.tokens 
```

#### Perform the KWIC segmentation

##### Corpus with POS information on the following word

This is a clever workaround to get specific POS tags for context words:

-   KWIC Run 1: It looks for "有" with a window = 1. This isolates the single word immediately before and after.

-   Tagging: It takes the post column (the word after "有") and runs the POS segmenter on it.

-   Cleaning: It converts the list of tags into a dataframe and cleans up the tag names (e.g., removing numbers).

-   Merging: It joins this POS information back to the original KWIC data.

```{r, eval=FALSE}
kwic_data <- kwic(Article_tokens,
                  pattern = "有",
                  window = 1)

RightPost_Annot <- segment(kwic_data$post, seg_POS)

## Convert list to dataframe
RightPost_Annot <- do.call(rbind, lapply(RightPost_Annot, as.data.frame))

## Add POS column and clean row names
RightPost_Annot <- cbind(POS = rownames(RightPost_Annot), RightPost_Annot)
rownames(RightPost_Annot) <- 1:nrow(RightPost_Annot)
names(RightPost_Annot)[2] <- "RightPost"
RightPost_Annot$POS <- gsub("[0-9]+", "", RightPost_Annot$POS)

## Remove duplicates to ensure clean join
RightPost_Annot <- RightPost_Annot[!duplicated(RightPost_Annot), ]
names(RightPost_Annot)[2] <- "post"

## Join POS data back to KWIC data
kwic_data <- right_join(kwic_data, RightPost_Annot, by = "post")
```

##### Corpus with longer sentences

Since a window of 1 is too short to understand the meaning, a second KWIC analysis is run with a window = 15 to capture the full sentence context.

```{r, eval=FALSE}
kwic_data2 <- kwic(Article_tokens,
                  pattern = "有",
                  window = 15)
```

##### Combine the two datasets together

The script now merges the "POS info" with the "Sentence context":

-   It creates a unique Index key (combining document name and position) to match the exact same instance of the word "有" across both datasets.

-   It uses right_join to merge them, ensuring we have both the grammatical category of the following word and the full sentence.

```{r, eval=FALSE}
### Prepare the dataset with longer sentences
kwic_data2 <- as.data.frame(kwic_data2)

kwic_data2$Index <- paste0(kwic_data2$docname,
                           kwic_data2$from)

kwic_data2_selected <- kwic_data2 %>% 
  select(docname, pre, post, Index)

### Prepare the dataset with the POS infomation
kwic_data <- as.data.frame(kwic_data)

names(kwic_data)[6] <- "post_1word"

kwic_data_selected <- kwic_data %>% 
  select(docname, from, to, post_1word, keyword, POS)

kwic_data_selected$Index <- paste0(kwic_data_selected$docname,
                                   kwic_data_selected$from)

### Join the two datasets
kwic_data <- right_join(kwic_data_selected,
                   kwic_data2_selected,
                   by = "Index")

### Reorder columns for readability
kwic_data <- kwic_data %>% 
  relocate(keyword, .after = pre) %>%
  relocate(post_1word, .after = keyword)
```

##### Select the sentences we are interested in

This filters the data to keep only rows where the word following "有" is a Verb (POS == "v"). It then creates a frequency table of these specific verbs.

```{r, eval=FALSE}
kwic_you_verb <- kwic_data[kwic_data$POS == "v", ]

table_YouVerb <- table(kwic_you_verb$post_1word)
table_YouVerb <- as.data.frame(table_YouVerb)
names(table_YouVerb)[1] <- "Verb"

## Sort by frequency
table_YouVerb <- table_YouVerb %>% arrange(desc(Freq))
head(table_YouVerb, 10)
```

#### Save the data

Saves the specific "You + Verb" dataset to Excel and RData formats.

```{r, eval=FALSE}
write.xlsx(kwic_you_verb, "ArticleETToday_KWIC_You.xlsx")
save(kwic_you_verb, file = "ArticleETToday_KWIC_You.Rdata")
```

### Frequency tables

This section calculates which words appear most often in the entire corpus, with specific cleaning steps.

#### Create the overall frequency table

It converts the tokens into a Document-Feature Matrix (DFM) and then calculates statistics.

```{r, eval=FALSE}
Article_tokens_frequency <- dfm(Article_tokens)
Article_tokens_frequency <- textstat_frequency(Article_tokens_frequency)
head(Article_tokens_frequency, 100)
```

#### Clean it up a little bit

The raw frequency list includes punctuation and numbers. This code manually removes them using grep to exclude specific characters (comma, period, quotes, etc.) and digits.

```{r, eval=FALSE}
## Remove punctuation
table_FreqWord <- Article_tokens_frequency[-grep("，", Article_tokens_frequency$feature),]
table_FreqWord <- table_FreqWord[-grep("。", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("、", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("「", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("」", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("（", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("）", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("？", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("；", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("！", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("《", table_FreqWord$feature),]
table_FreqWord <- table_FreqWord[-grep("》", table_FreqWord$feature),]

## Remove numbers
table_FreqWord <- table_FreqWord[-grep("[[:digit:]]", table_FreqWord$feature),]
```

#### Final table, addition of the percentage

Calculates the relative frequency (percentage) of the top words.

```{r, eval=FALSE}
table_FreqWord_Top100 <- head(table_FreqWord, 100)
table_FreqWord_Top100$percentage <- round(table_FreqWord_Top100$frequency/sum(table_FreqWord$frequency)*100, 4)
```

#### Select only the 100 most frequent nouns

The goal is to find the top nouns. However, POS tagging the entire corpus is computationally expensive.

Strategy: Take the top 500 most frequent words first.

-   Tagging: POS tag only those 500 words.

-   Filtering: Keep only those tagged as "n" (noun).

##### Set segmenter and Annotate

This tags the list of frequent words.

```{r, eval=FALSE}
seg_POS_ByLines <- worker(type = "tag", bylines = FALSE, symbol = F)
table_FreqWord_Top500 <- head(table_FreqWord, 500)

Top500_WordFreqPOS <- segment(table_FreqWord_Top500$feature, seg_POS_ByLines)

## Convert to dataframe
Top500_WordFreqPOS_Annotated <- do.call(rbind, 
                                        lapply(Top500_WordFreqPOS, 
                                               as.data.frame))

Top500_WordFreqPOS_Annotated <- cbind(POS = rownames(Top500_WordFreqPOS_Annotated),
                                      Top500_WordFreqPOS_Annotated)

rownames(Top500_WordFreqPOS_Annotated) <- 1:nrow(Top500_WordFreqPOS_Annotated)

names(Top500_WordFreqPOS_Annotated)[2] <- "Word"

Top500_WordFreqPOS_Annotated$POS <- gsub("[0-9]+", "", Top500_WordFreqPOS_Annotated$POS)
```

##### Extract the nouns

Filters for POS == "n", joins this back to the frequency table to ensure we have the counts, calculates percentages, and keeps the top 100.

```{r, eval=FALSE}
TopFreqNoun <- Top500_WordFreqPOS_Annotated[Top500_WordFreqPOS_Annotated$POS == "n", ]
TopFreqNoun$Index <- "TopNouns"

## Join back to original frequency table to get counts
names(table_FreqWord)[1] <- "Word"
TopFreqNoun <- right_join(TopFreqNoun, table_FreqWord, by = "Word")

## Filter and Sort
TopFreqNoun <- TopFreqNoun[+grep("TopNouns", TopFreqNoun$Index),]
table_FreqNoun_Top100 <- head(TopFreqNoun, 100) %>% arrange(desc(frequency))
```

### Save the data

Saves the top 100 nouns list to Excel and RData.

```{r, eval=FALSE}
write.xlsx(table_FreqNoun_Top100, "ArticleETToday_Top100nouns.xlsx")
save(table_FreqNoun_Top100, file = "ArticleETToday_Top100nouns.Rdata")
```

# Markdown document, PDF output file, RData and Excel files of the scraped data

You can find the pre-filled Markdown document of this section <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/Markdown_Week12_Analyzing_data.Rmd" target="_blank">here</a>. <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/Markdown_Week12_Analyzing_data.pdf" target="_blank">Here</a> is the PDF output of the same document.

The RData output file can be downloaded <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_KWIC_You.Rdata" target="_blank">here for the KWIC analysis</a>, and <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_Top100nouns.Rdata" target="_blank">here for the frequency analysis</a>. The corresponding Excel files are <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_KWIC_You.xlsx" target="_blank">here (KWIC analysis)</a> and <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_Top100nouns.xlsx" target="_blank">here (frequency analysis)</a>.
