---
title: "Week 12: Data analysis"
format: 
  html:
    output-file: Week12.html
    number-sections: true
    number-depth: 2
    toc: true
---

```{=html}
<style>
hr.rounded {
  border-top: 1px solid #E5E4E2;
  border-radius: 5px;
}
</style>
```

<br>

::: {.callout-warning icon="false"}
## Objectives

This week, you will learn how to analyze your data quickly and automatically with R.

!! This is an overview of how the analysis process, without reference to statistical modeling!
:::

::: {.callout-note icon="false"}
The data analysis is exemplified with the ETtoday dataset that we cleaned last week. In case, you can download the R data using this link: <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_CorpusCourse_CLEAN.Rdata" target="_blank">here</a>
:::

# How to analyze corpus data?

Let's recap first what we learned and did in the past weeks:

-   We learned what a corpus linguistics study is, the questions we can ask, and the types of corpora we can/should select to conduct the study;

-   We learned how to automatically scrape data from the Internet, with examples from different sources;

-   Last week, we learned how to "clean" the raw data, how to prepare the data for further analyses.

And this is the main objective for this week: **How to analyze the data that we prepared?**

Before getting into the details, we need to keep in mind the overall workflow of a corpus study. (You can zoom in)

```{r mermaid-example, echo=FALSE, fig.align='center'}
    DiagrammeR::mermaid(diagram = "
    graph TD
        A[Research question] --> B{Do I need corpus data};
        B -- No --> C[Corpus study maybe not appropriate];
        B -- Yes --> D[Proceed to the corpus study!];
        D --> E[Select the right corpus];
        E --> F[Select the analysis method];
        F -- Word lists, KWIC, collocates, clusters, random forests, etc. --> G[Proceed to the automatic analysis];
        G --> H[Interpret the results according to the research question];
    ")

```

::: {.callout-note icon="true"}
There are many many analyses methods. Their common point is that they involve automatic processing, which is very helpful when we are dealing with large datasets. Some analyses methods are quite straightforward and traditional (e.g., frequency tables), while some others are more on the computational side. Always remember to select the analysis method that is appropriate for your research question!
:::

For this class, two methods will be presented: Word lists and KWIC (Key Word In Context). These are classic yet very useful methods, and easy to understand and implement for any kind of research.

## KWIC (Key Word In Context)

The first type of analysis to introduce is called **Key Word In Context**, more often found under the acronym ***kwic***. The idea is that we can better understand the meaning of a word based on how it is used in a sentence. KWIC analyses are therefore very suitable **when we target a specific word or syntactic construction**.

KWIC analyses can be used for:

-   *Semantic analyses*: Idea that the meaning of a specific word can be retrieved from its use in a sentences, based on the meaning of its *neighbors*.

-   *Morphosyntactic analyses*: Idea that the morphosyntactic context where a word or a construction is used is helpful in understanding its specific morphological and syntactic features, as well as the meaning it conveys.

Here is an example below, with the Mandarin word *keai* 'cute' (notice that these are fake data just for an illustration):

+----------------+-------------------+--------------+---------------+
| Sentence index | Before keyword    | Keyword      | After keyword |
+================+===================+==============+===============+
| Sentence 1     | hen               | keai         | de mao        |
|                |                   |              |               |
|                | (very)            | (cute)       | (DE cat)      |
+----------------+-------------------+--------------+---------------+
| Sentence 2     | feichang          | keai         | de gou        |
|                |                   |              |               |
|                | (very, extremely) | (cute)       | (DE dog)      |
+----------------+-------------------+--------------+---------------+

Based on these data, and assuming that these are the most frequent instances that we found in the corpus, we can infer that:

-   *Semantically*, the word *keai* is most often used to describe *animals* or *pets*, based on the following segment;

-   *Morphosyntactically*, the word *keai* behaves as an adjective (or stative verb, depending on the analysis), based on the preceding segment.

## Word lists based on frequency

# Let's do the job with R

## The *quanteda* package to conduct corpus analyses

## Some notes when dealing with Mandarin data

## Explanation of the Markdown script

# Markdown document, PDF output file, RData and Excel files of the scraped data

You can find the pre-filled Markdown document of this section <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/Markdown_Week12_Analyzing_data.Rmd" target="_blank">here</a>. <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/Markdown_Week12_Analyzing_data.pdf" target="_blank">Here</a> is the PDF output of the same document.

The RData output file can be downloaded <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_KWIC_You.Rdata" target="_blank">here for the KWIC analysis</a>, and <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_Top100nouns.Rdata" target="_blank">here for the frequency analysis</a>. The corresponding Excel files are <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_KWIC_You.xlsx" target="_blank">here (KWIC analysis)</a> and <a href="https://github.com/aymeric-collart/CHU0387/blob/main/assets/files/_Internal/ArticleETToday_Top100nouns.xlsx" target="_blank">here (frequency analysis)</a>.
