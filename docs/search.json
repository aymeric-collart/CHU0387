[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The workflow of a typical corpus-based study is generally as in the image below.\n\nThis course follows this workflow. I will show you how to conduct a corpus investigation step-by-step, and you will be able to do so by yourself!\n\n\n\nThe weekly organization of this course can be found in the Table of Contents on the left in addition to the table below.\n\n\n\n\n\n\n\nWeek\nContent\n\n\n\n\nWeek 1\nIntroductory week\n\n\nPart 1\nPrinciples of corpus studies\n\n\nWeek 2\nWhat is a corpus, and their role to understand language and literature\n\n\nWeek 3\nTypes of corpora: Selecting or building a corpus?\n\n\nPart 2\nBuilding a corpus from the Internet with modern tools\n\n\nWeek 4\nIntroduction to R (1/2): Installation and first steps\n\n\nWeek 5\nIntroduction to R (2/2): Basic functions and how to create reproducible data\n\n\nWeek 6\nExample 1: Building a corpus from media platforms (1/2)\n\n\nWeek 7\nExample 1: Building a corpus from media platforms (2/2)\n\n\nWeek 8\nExample 2: Building a corpus from literature sources (1/2)\n\n\nWeek 9\nExample 2: Building a corpus from literature sources (2/2)\n\n\nWeek 10\nMidterm presentations, group and individual meetings in the classroom\n\n\nPart 3\nSimple preprocessing and analyzes of corpus data\n\n\nWeek 11\nData preprocessing: Tidying data\n\n\nWeek 12\nData analysis\n\n\nWeek 13\nData visualization\n\n\nWeek 14\nPresenting a corpus-based study\n\n\nWeek 15\nGroup and individual meetings in the classroom (assistance for final projects)\n\n\nWeek 16\nStudent‚Äôs presentations of individual/group projects\n\n\n\n\n\n\n\n\nWe will only use some parts of these two books:\n1- Paquot, M. & Gries, S. T. (2020) A practical handbook of corpus linguistics. Cham: Springer. Normally, you should be able to download this book freely with this link: Link to the book\n2- O‚ÄôKeeffe, A., & McCarthy, M. J. (2022). The Routledge handbook of corpus linguistics (2nd edition). London/New York: Routledge. Link to the book\n\n\n\n\n\n\n\n\n\n\n\n(a) Paquot & Gries (2020)\n\n\n\n\n\n\n\n\n\n\n\n(b) O‚ÄôKeeffe & McCarthy (2022)\n\n\n\n\n\n\n\nFigure¬†1: Required textbooks\n\n\n\nOnline resources:\n1- R website: Link to the R website (freely downloadable)\n2- RStudio interface: Link to the RStudio website (freely downloadable)\n3- R for data science handbook: Link to the handbook (freely downloadable)\n\n\n\nThis course is task-oriented, so there are many aspects of corpus linguistics that will not be covered. If you are interested in knowing more, you can read the book published by Anatol Stefanowitsch in 2020, available for free here.",
    "crumbs": [
      "Welcome page",
      "Week 1: Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-are-the-steps-of-a-corpus-study",
    "href": "intro.html#what-are-the-steps-of-a-corpus-study",
    "title": "Introduction",
    "section": "",
    "text": "The workflow of a typical corpus-based study is generally as in the image below.\n\nThis course follows this workflow. I will show you how to conduct a corpus investigation step-by-step, and you will be able to do so by yourself!",
    "crumbs": [
      "Welcome page",
      "Week 1: Introduction"
    ]
  },
  {
    "objectID": "intro.html#syllabus-of-the-course",
    "href": "intro.html#syllabus-of-the-course",
    "title": "Introduction",
    "section": "",
    "text": "The weekly organization of this course can be found in the Table of Contents on the left in addition to the table below.\n\n\n\n\n\n\n\nWeek\nContent\n\n\n\n\nWeek 1\nIntroductory week\n\n\nPart 1\nPrinciples of corpus studies\n\n\nWeek 2\nWhat is a corpus, and their role to understand language and literature\n\n\nWeek 3\nTypes of corpora: Selecting or building a corpus?\n\n\nPart 2\nBuilding a corpus from the Internet with modern tools\n\n\nWeek 4\nIntroduction to R (1/2): Installation and first steps\n\n\nWeek 5\nIntroduction to R (2/2): Basic functions and how to create reproducible data\n\n\nWeek 6\nExample 1: Building a corpus from media platforms (1/2)\n\n\nWeek 7\nExample 1: Building a corpus from media platforms (2/2)\n\n\nWeek 8\nExample 2: Building a corpus from literature sources (1/2)\n\n\nWeek 9\nExample 2: Building a corpus from literature sources (2/2)\n\n\nWeek 10\nMidterm presentations, group and individual meetings in the classroom\n\n\nPart 3\nSimple preprocessing and analyzes of corpus data\n\n\nWeek 11\nData preprocessing: Tidying data\n\n\nWeek 12\nData analysis\n\n\nWeek 13\nData visualization\n\n\nWeek 14\nPresenting a corpus-based study\n\n\nWeek 15\nGroup and individual meetings in the classroom (assistance for final projects)\n\n\nWeek 16\nStudent‚Äôs presentations of individual/group projects",
    "crumbs": [
      "Welcome page",
      "Week 1: Introduction"
    ]
  },
  {
    "objectID": "intro.html#required-textbooks-and-additional-references",
    "href": "intro.html#required-textbooks-and-additional-references",
    "title": "Introduction",
    "section": "",
    "text": "We will only use some parts of these two books:\n1- Paquot, M. & Gries, S. T. (2020) A practical handbook of corpus linguistics. Cham: Springer. Normally, you should be able to download this book freely with this link: Link to the book\n2- O‚ÄôKeeffe, A., & McCarthy, M. J. (2022). The Routledge handbook of corpus linguistics (2nd edition). London/New York: Routledge. Link to the book\n\n\n\n\n\n\n\n\n\n\n\n(a) Paquot & Gries (2020)\n\n\n\n\n\n\n\n\n\n\n\n(b) O‚ÄôKeeffe & McCarthy (2022)\n\n\n\n\n\n\n\nFigure¬†1: Required textbooks\n\n\n\nOnline resources:\n1- R website: Link to the R website (freely downloadable)\n2- RStudio interface: Link to the RStudio website (freely downloadable)\n3- R for data science handbook: Link to the handbook (freely downloadable)\n\n\n\nThis course is task-oriented, so there are many aspects of corpus linguistics that will not be covered. If you are interested in knowing more, you can read the book published by Anatol Stefanowitsch in 2020, available for free here.",
    "crumbs": [
      "Welcome page",
      "Week 1: Introduction"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome page",
    "section": "",
    "text": "Welcome to the website of the course \"Language, Society and Literature with Corpus and Modern Technology\" (code: CHU03887)\n\n\n\n\n\n\n\nNote\n\n\n\nThe content for the the data analysis is online now!\nStay tuned for further developments!\n\n\n\n\n\n ¬† üè´ ¬† Description and objectives of the course\n\nThis course introduces the principles of corpus studies assisted by modern technology with an emphasis on hands-on practice, and how corpus data can be useful in understanding how language is used in the media or in the literature. The course is divided into three parts: (a) fundamental concepts and practices when working with a corpus, (b) the process of building a corpus from texts on the Internet with modern technology, and (c) the handling, analyses and visualization of the corpus data. By the end of the course, the students are expected to acquire the basic knowledge of working with a corpus and will obtain a first approach to programming languages.\n\n ¬† üìã ¬† General information for the 2025 Fall class\n\n\nTime: Wednesday, 10:20 to 12:10\nPlace: ‚ÄúPu‚Äù building, classroom 302\nThe first class starts September 3.\n\n\n ¬† üí¨ ¬† Q&A\n\n\n\n1- Do I need any background?\nYou do not need any particular background.\nThis course will start with the very basics. If you are already familiar with coding and/or the RStudio interface, it will of course be easier for you.\n\n\n2- Which public?\nIn addition to the students enrolled in the course, anyone interested in having a first hands-on experience can follow the content of the website."
  },
  {
    "objectID": "Weeks6_7.html#what-is-the-task",
    "href": "Weeks6_7.html#what-is-the-task",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "2.1 What is the task",
    "text": "2.1 What is the task\nFirst, you need to be very clear about what you want to do, otherwise you can literally spend hours doing a lot of things but actually achieving nothing.\nSo here is your first task:\n\nGo to the ETtoday website, and search for one article.\nDescribe each step you take to get to an article, and document the changes that happen regarding the address of the website.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#documentation-of-the-manual-steps",
    "href": "Weeks6_7.html#documentation-of-the-manual-steps",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "2.2 Documentation of the manual steps",
    "text": "2.2 Documentation of the manual steps\nThis task seems quite easy, but it is crucial. First, you went to the website by clicking on the link. Then, you selected the date of the news you were interested in. Third, you selected the type of news (politics, society, etc.). A list of articles was already there on the website, and you were just filtering them. And to get into one article, you just clicked on one of the titles below the search bar.\nThis procedure looks like this:\n\n\n\n\n\n\nNow we will do the same, but pay more attention to the web address, and track the changes that occur. For instance, the first step is just accessing the website. The web address is ‚Äúhttps://www.ettoday.net/news/news-list.htm‚Äù. What is the web address when you select a particular date, let‚Äôs say October 28th, 2024? And when you select a particular type of article, ‚Äúpolitics‚Äù for example? And finally, when you choose an individual article?\nHere is what we get:\n\nFirst step: https://www.ettoday.net/news/news-list.htm\nSecond step (filtered articles): https://www.ettoday.net/news/news-list-2024-10-28-1.htm\nThird step: (individual article): https://www.ettoday.net/news/20241028/2843953.htm\n\n\n\n\n\n\n\nThink about it\n\n\n\n\nFirst, we can see that filtering the data changes the address of the website in a very systematic way. Now, try to change the address to access the list of articles published on September 27th, 2024. You will get something like that: https://www.ettoday.net/news/news-list-2024-09-27-1.htm\nSecond, try to change the last number. You will see that it changes the type of articles.\n\nSo this means that you can filter the list of articles as you wish just by changing the web address!\n\nThird, if we look at the web address of the individual article, we can retrieve the date of publication of the article very easily‚Ä¶ but it is not quite possible to guess the numbers after that.\n\n\n\nNo need to despair! The fact that this article is found in the list means that the web address of the individual article must be somewhere on the website‚Ä¶ but where? This is the moment when it is important to understand the structure of the website.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#how-to-access-the-structure-of-the-website",
    "href": "Weeks6_7.html#how-to-access-the-structure-of-the-website",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "3.1 How to access the structure of the website?",
    "text": "3.1 How to access the structure of the website?\nAccessing the structure of a website is actually very easy. You just need to open the webpage, for example this one: https://www.ettoday.net/news/news-list-2024-10-28-1.htm. Then, you look for a blank space, you right-click, and you select ‚ÄúView Page Source‚Äù (it is possible that it will not be in English, or that the wording is different depending on the system of your computer).\nThen, it will open a new page which can be very scary‚Ä¶ but this is actually the same, just the structure of the previous page!\nHere is the animated figure of these steps:",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#what-are-all-these-lines-about",
    "href": "Weeks6_7.html#what-are-all-these-lines-about",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "3.2 What are all these lines about?",
    "text": "3.2 What are all these lines about?\nThis version of the website is really where you can find all the information that you need. For example, please have a look at lines 514 to 541 (see the image below).\n\nThis must be more familiar to you: these are the web addresses that you obtain when you filter different types of articles!\nNow, please have a look at line 549‚Ä¶ you will see that you have the web addresses of each individual article listed on the webpage!\n\n\n\n\n\n\n\nHow did I know what lines to search for?\n\n\n\nThere‚Äôs nothing magical, and I am surely not a psychic who can guess the line numbers. So how to find them? I just use the ‚Äúsearch‚Äù function (for Windows: ‚Äúctrl + f‚Äù, for Mac: ‚Äúcommand + f‚Äù). And then I look for some keywords, such as the names of the types of articles, or the web address of the individual articles.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#terminology-of-the-structural-lines",
    "href": "Weeks6_7.html#terminology-of-the-structural-lines",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "3.3 Terminology of the ‚Äústructural lines‚Äù",
    "text": "3.3 Terminology of the ‚Äústructural lines‚Äù\nLet‚Äôs call these lines the ‚Äústructural lines‚Äù. These are just lines of code used to create the webpage. There are different programming languages, but for our purpose, we will only need to look at the lines written in html language.\nLet‚Äôs just have a look at line 549 and decompose it.\n\n&lt;div class=\"part_list_2\"&gt;\n  &lt;h3&gt;\n    &lt;span class=\"date\"&gt;2024/10/28 20:45&lt;/span&gt;\n    &lt;em class=\"tag c_news\"&gt;ÊîøÊ≤ª&lt;/em&gt;\n    &lt;a target=\"_blank\" href=\"https://www.ettoday.net/news/20241028/2843953.htm\"&gt;Èô≥Âêâ‰ª≤ÂàóË∂ÖÊÄùÊ°àË¢´ÂëäÔºÅÈªÉÂúãÊòå„ÄåÂìá‰∏ÄËÅ≤„ÄçÔºöÂåóÊ™¢ÁµÇÊñºÈÜí‰æÜÂï¶&lt;/a&gt;\n  &lt;/h3&gt;\n&lt;/div&gt;\n\nThis line corresponds to this part of the website:\n\nNow we can decompose it:\n\nThe part with ‚Äú&lt;div&gt;&lt;/div&gt;‚Äù means that there is a block on the webpage. This block corresponds to the one where all the individual articles are listed. There are many blocks on a webpage, so we can give them different names to avoid confusing them. Here, the block is called ‚Äúpart_list_2‚Äù.\nThe part with ‚Äú&lt;h3&gt;&lt;/h3&gt;‚Äù corresponds to one line in the block. What is between ‚Äú&lt;h3&gt;‚Äù and ‚Äú&lt;/h3&gt;‚Äù is everything we want to put in that line. Here there are three elements:\n\nThe first element is a ‚Äú&lt;span&gt;&lt;/span&gt;‚Äù element, and its name is ‚Äúdate‚Äù. What is between ‚Äú&lt;span&gt;‚Äù and ‚Äú&lt;/span&gt;‚Äù is the text to display, here: ‚Äú2024/10/28 20:45‚Äù.\nThe second element is an ‚Äú&lt;em&gt;&lt;/em&gt;‚Äù element (why ‚Äúem‚Äù and not ‚Äúspan‚Äù? ‚Äúem‚Äù stands for ‚Äúemphasis‚Äù, so they look different on the website). Its name is ‚Äútag c_news‚Äù.\nThe third element is an ‚Äú&lt;a&gt;&lt;/a&gt;‚Äù element. This is used to write a text with a link, and the link is specified with ‚Äúhref‚Äù. The term ‚Äútarget=‚Äù_blank‚Äù‚Äù means that it opens in a new page when you click on it.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis seems to be a lot and quite complicated.\nSo just remember this:\n\nThere are big blocks with different names (div, h3, span, etc.). Let‚Äôs call them nodes.\nInside nodes, you can specify the parameters and names specific to the nodes.\n\nThat‚Äôs all!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#overview-of-the-steps",
    "href": "Weeks6_7.html#overview-of-the-steps",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.1 Overview of the steps",
    "text": "4.1 Overview of the steps\nIn Section 2.2, we wrote down the several steps we have been going through to copy and paste the content of one article. In the end, we want to repeat this step many times to get the data from more than one article‚Ä¶ and now you can imagine why it is a better idea to process through these steps automatically instead of doing it manually for weeks and months.\n\n\n\n\n\n\nThink about it\n\n\n\nIf you followed this rationale, now you can guess what coding means. It is just about translating our behavior to a language that the computer can understand!\nIn other words, we are going to map the steps described in Section 2.2 to R functions to perform the same task. And then we will require R to repeat the same task with different web addresses.\n\n\nAn efficient way to be clear about the steps to proceed with when doing programming language is to translate the steps we went through manually into words that make sense for the computer. For instance, when we select the dates or article types for filtering, we actually store them somewhere as variables. This gives us the annotated figure below.\n\nNow let‚Äôs see how to implement these steps with R.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#preliminary-step-libraries-needed",
    "href": "Weeks6_7.html#preliminary-step-libraries-needed",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.2 Preliminary step: Libraries needed",
    "text": "4.2 Preliminary step: Libraries needed\nFor this task, we will only need four packages:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(xml2)\nlibrary(openxlsx)",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#step-1-record-the-web-addresses-that-list-the-pages-of-the-individual-articles-based-on-the-filters",
    "href": "Weeks6_7.html#step-1-record-the-web-addresses-that-list-the-pages-of-the-individual-articles-based-on-the-filters",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.3 Step 1: Record the web addresses that list the pages of the individual articles based on the filters",
    "text": "4.3 Step 1: Record the web addresses that list the pages of the individual articles based on the filters\nThe first step in our task is to record the web addresses. Let‚Äôs have a first recap:\n\nThe main address is: https://www.ettoday.net/news/news-list.htm\nThe web address with filters for dates and categories looks like this: https://www.ettoday.net/news/news-list-2024-10-28-1.htm\n\nNow it is like playing lego: How to obtain the address with filters based on the first address? The answer is as follows:\n\n\"https://www.ettoday.net/news/news-list\" + \"-\" + date + \"-\" + category + \".htm\"\n\nWhere dates are like ‚Äú2024-10-28‚Äù and categoriesare just numbers. How to do so with R? Nothing easier!\n\nFirst, store the base address as a variable:\n\n\nETTodayPageUrl &lt;- \"https://www.ettoday.net/news/news-list\" \n\n\nThen, store the year, month and day of your interest, and paste them together. To paste, we use the function ‚Äúpaste0()‚Äù:\n\n\nYear &lt;- 2024\nMonth &lt;- 10\nDay &lt;- 28\n\ndate &lt;- paste0(Year,\n               \"-\",\n               Month,\n               \"-\",\n               Day)\ndate\n\n[1] \"2024-10-28\"\n\n\n\nNow we select the category. Let‚Äôs say we want articles talking about politics, coded as ‚Äú1‚Äù on ETtoday:\n\n\ncategory &lt;- 1\n\n\nFinally, we paste these variables together to obtain the address:\n\n\n## Template: \"https://www.ettoday.net/news/news-list\" + \"-\" + date + \"-\" + category + \".htm\"\nURL &lt;- paste0(ETTodayPageUrl,\n              \"-\",\n              date,\n              \"-\",\n              category,\n              \".htm\")\nURL\n\n[1] \"https://www.ettoday.net/news/news-list-2024-10-28-1.htm\"\n\n\n\n\n\n\n\n\nThink about it\n\n\n\nYou may think that it is a complex way to do‚Ä¶ why not writing the web address as a variable instead of pasting the elements? In other words, why not writing this instead:\nURL &lt;- ‚Äúhttps://www.ettoday.net/news/news-list-2024-10-28-1.htm‚Äù\nThink about it. Doing so, if you want the data for the whole year 2024, and two categories, it means that you will have to do the same more than 700 times‚Ä¶ so it seems to be a simpler solution, and it is true if we are working with only one page, but not necessarily with big data!\n\n\nWe have the address corresponding to one page with the code above. If we want the web addresses for the whole year, we will need to repeat the code, and changing the date every time. In programming language, there is a command that tells a computer to repeat the same set of instructions for each item in a list. It is a way to automate a repetitive task. This command is called for loop.\n\n\n\n\n\n\nWhat is the concept of for loop?\n\n\n\nYou have a list (your friends at the table) and an action (giving one card). The for loop is the instruction you give yourself: ‚ÄúFor each friend at the table, give them one card.‚Äù\nYou go to the first friend and give them a card. Then you automatically move to the second friend and do the same thing. You continue this process until every friend in the group has received a card. The loop then stops on its own.\nEvery for loop has three basic components:\n\nThe List (or Collection): The group of items you want to work through. This could be a list of names, numbers, or any collection of data.\nThe Action (or Task): The specific job you want to do for every single item in the list.\nThe Loop: The process that automatically takes one item, performs the action, and then moves on to the next item until the list is finished.\n\n\n\nLet‚Äôs have a look at how to apply this concept to our case.\n\nFirst, we start with the base address, as we have already done above:\n\n\nETTodayPageUrl &lt;- \"https://www.ettoday.net/news/news-list\" \n\n\nThen we create the List. Since we want to go through the articles published in 2024, we want Year 2024, Months from January (1) to December (12), and Days from 1 to 31 (some months do not have 31 days, but it is not crucial for our purpose). This is how we write it:\n\n\nYear &lt;- 2024\nMonth &lt;- 1:12\nDay &lt;- 1:31\n\nYear\n\n[1] 2024\n\nMonth\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nDay\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31\n\n\n\nWe have the List. Now we need the Action. What we want is a list (or table) with all the dates. To do so:\n\nWe create an empty table,\nWe paste Year, Month and Day to obtain the date,\nWe change the class of the variable to match the class of the empty table,\nWe add the individual date into the table\n\n\nThe code looks like that:\n\nDates_total &lt;- data.frame(matrix(ncol = 1, nrow = 0))\n\n# The Action is below\nDates &lt;- paste0(Year, \"-\", Month, \"-\", Day)\nDates &lt;- as.data.frame(Dates)\nDates_total &lt;- rbind(Dates_total, Dates)\n\n# A look at the results\nDates_total\n\n        Dates\n1    2024-1-1\n2    2024-2-2\n3    2024-3-3\n4    2024-4-4\n5    2024-5-5\n6    2024-6-6\n7    2024-7-7\n8    2024-8-8\n9    2024-9-9\n10 2024-10-10\n11 2024-11-11\n12 2024-12-12\n13  2024-1-13\n14  2024-2-14\n15  2024-3-15\n16  2024-4-16\n17  2024-5-17\n18  2024-6-18\n19  2024-7-19\n20  2024-8-20\n21  2024-9-21\n22 2024-10-22\n23 2024-11-23\n24 2024-12-24\n25  2024-1-25\n26  2024-2-26\n27  2024-3-27\n28  2024-4-28\n29  2024-5-29\n30  2024-6-30\n31  2024-7-31\n\n\n\nSo far it gives us an incomplete list of dates. This is because we need to add the third element: The Loop. Here is my way to create the loop:\n\nWe have the Action for one subset of the data only, which means that we need to have a process for each day, and each month. The rationale goes like this:\n\nLoop 1: ‚ÄúFor Day number 1, please assign Month number 1, and Year 2024‚Äù\nLoop 2: ‚ÄúFor Day number 2, please assign Month number 1, and Year 2024‚Äù\n‚Ä¶ ‚Ä¶\nLoop 31: ‚ÄúFor Day number 31, please assign Month number 1, and Year 2024‚Äù\nLoop 32: ‚ÄúFor Day number 1, please assign Month number 2, and Year 2024‚Äù\n\nThis means that we have a first round of Loop for Day, followed by a second round of Loop for Month, followed by a third round of Loop for Year.\nThis is why in the code below, there are 3 for loop commands, and different variables are assigned to them:\n\n\n\nDates_total &lt;- data.frame(matrix(ncol = 1, nrow = 0))\n\nfor (a in 1:length(Day)){\n  for (b in 1:length(Month)){\n    for (c in min(Year):max(Year)){\n      Dates &lt;- paste0(c, \"-\", b, \"-\", a)\n      Dates &lt;- as.data.frame(Dates)\n      Dates_total &lt;- rbind(Dates_total, Dates)\n    }\n  }\n}\n\nDates_total &lt;- as.character(Dates_total$Dates)\n\nknitr::kable(head(Dates_total)) # Use of the kable() function from the 'knitr' package to have a prettier output\n\n\n\n\nx\n\n\n\n\n2024-1-1\n\n\n2024-2-1\n\n\n2024-3-1\n\n\n2024-4-1\n\n\n2024-5-1\n\n\n2024-6-1\n\n\n\n\n\n\nNow we apply the same logic to add the Categories. Let‚Äôs say we want articles about ‚ÄúPolitics‚Äù and ‚ÄúSociety‚Äù.\n\nIn that case, the List is about categories coded as ‚Äú1‚Äù and ‚Äú6‚Äù\nThe Action is to paste the different parts of the web address\nThe Loop is to paste all the dates (372 dates) to the 2 categories.\nThis gives the following code:\n\n\n\n# \"6.htm\" = \"Society\"\n# \"1.htm\" = \"Politics\"\n# \"2.htm\" = \"International\"\n# \"5.htm\" = \"Life\"\nCategory_total &lt;- c(\"6\", \"1\")\n\nURL_total &lt;- data.frame(matrix(ncol = 1, nrow = 0))\n  \nfor (a in 1:length(Category_total)){\n  URL_temp &lt;- paste0(ETTodayPageUrl, \"-\", Dates_total, \"-\", Category_total[a], \".htm\")\n  URL_temp &lt;- as.data.frame(URL_temp)\n  URL_total &lt;- rbind(URL_total, URL_temp)\n}\n\nURL_total &lt;- as.character(URL_total$URL_temp)\nknitr::kable(head(URL_total)) # Use of the kable() function from the 'knitr' package to have a prettier output\n\n\n\n\nx\n\n\n\n\nhttps://www.ettoday.net/news/news-list-2024-1-1-6.htm\n\n\nhttps://www.ettoday.net/news/news-list-2024-2-1-6.htm\n\n\nhttps://www.ettoday.net/news/news-list-2024-3-1-6.htm\n\n\nhttps://www.ettoday.net/news/news-list-2024-4-1-6.htm\n\n\nhttps://www.ettoday.net/news/news-list-2024-5-1-6.htm\n\n\nhttps://www.ettoday.net/news/news-list-2024-6-1-6.htm",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#step-2-retrieve-the-urls-of-the-individual-articles",
    "href": "Weeks6_7.html#step-2-retrieve-the-urls-of-the-individual-articles",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.4 Step 2: Retrieve the URLs of the individual articles",
    "text": "4.4 Step 2: Retrieve the URLs of the individual articles\nNow we have the web addresses that list the webpages of individual articles, filtered by dates and categories. The next step is to find the web addresses of these individual articles. There are several sub-steps:\n\nFirst, R needs to read the webpage with the list of addresses. Please be aware that when R reads the webpage, we are talking about the structure of the webpage!\n\nThe R function to do so is called ‚Äúread_html()‚Äù.\nWe write the address of the webpage to read between the parentheses. Here, we already have the list from the previous step, and we stored them under the variable ‚ÄúURL_total‚Äù.\nWe just test with only one page for the moment. This is why in the code below, we write ‚ÄúURL_total[1]‚Äù. It means ‚Äúthe first data in the list called URL_total‚Äù.\n\n\n\nIndPage &lt;- read_html(URL_total[1])\nIndPage\n\n{html_document}\n&lt;html lang=\"zh-Hant\" prefix=\"og:http://ogp.me/ns#\" xmlns:wb=\"http://open.weibo.com/wb\" itemscope=\"\" itemtype=\"http://schema.org/WebSite\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\r\\n\\n&lt;!-- FB Quote ÈÅ∏ÂèñÊñáÂ≠óÂàÜ‰∫´ ÈñãÂßã--&gt;\\n&lt;div class=\"fb-quote\"&gt;&lt;/div&gt;\\n&lt;!- ...\n\n\n\nThe second step is to retrieve the information that we need. We have already done that in Section 3, and if you remember well, we found the line corresponding to the data we want:\n\n\n&lt;div class=\"part_list_2\"&gt;\n  &lt;h3&gt;\n    &lt;span class=\"date\"&gt;2024/10/28 20:45&lt;/span&gt;\n    &lt;em class=\"tag c_news\"&gt;ÊîøÊ≤ª&lt;/em&gt;\n    &lt;a target=\"_blank\" href=\"https://www.ettoday.net/news/20241028/2843953.htm\"&gt;Èô≥Âêâ‰ª≤ÂàóË∂ÖÊÄùÊ°àË¢´ÂëäÔºÅÈªÉÂúãÊòå„ÄåÂìá‰∏ÄËÅ≤„ÄçÔºöÂåóÊ™¢ÁµÇÊñºÈÜí‰æÜÂï¶&lt;/a&gt;\n  &lt;/h3&gt;\n&lt;/div&gt;\n\nIf you need to describe, you have to say something like that:\n\"The address of the individual article is listed after the 'href' attribute, which is in the &lt;a&gt; node, which is in the &lt;h3&gt; node, which is in the &lt;div&gt; node whose class is 'part_list_2' \".\nHow to translate into R? We will use two functions:\n\n\n\n\n\n\n\n\nFunction\nUse\nExample in our case\n\n\n\n\nhtml_nodes()\nUsed to identify the elements within a node\nhtml_nodes(‚Äúdiv‚Äù) = retrieves all the nodes starting with ‚Äúdiv‚Äù\nhtml_nodes(‚Äúdiv.something‚Äù) = retrieves only the nodes starting with ‚Äúdiv‚Äù, where ‚Äòclass = ‚Äúsomething‚Äù‚Äô (in other words, it filters specific ‚Äúdiv‚Äù nodes)\n\n\nhtml_attr()\nUsed to identify the content of the specification of a node\nhtml_attr(‚Äúclass‚Äù) = retrieves the element written after ‚Äúclass‚Äù. For example, in the case below, it will return ‚ÄúNameOfTheClass‚Äù.\n&lt;div class=‚ÄúNameOfTheClass‚Äù&gt;\n\n\n\nNow that we know how to use the code, we can translate the description we wrote as follows:\n\nnews_url &lt;- IndPage %&gt;% \n  html_nodes(\"div.part_list_2\") %&gt;% \n  html_nodes(\"h3\") %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\")\n\nknitr::kable(head(news_url)) # Use of the kable() function from the 'knitr' package to have a prettier output on the website\n\n\n\n\nx\n\n\n\n\nhttps://www.ettoday.net/news/20240101/2655300.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655313.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655305.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655303.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655277.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655264.htm\n\n\n\n\n\n\n\n\n\n\n\nThink about it\n\n\n\nLet‚Äôs say that you wanted the date or the category of the articles, how would you change this code?\n\n\n\nCongratulations! You have the list of the addresses from one page! Now we need to repeat the same process to the other 743 lists‚Ä¶ and anytime you need to repeat a task over and over, this means for loop command! Here are the elements we need:\n\nFirst, we create an empty data frame in which we will store all the web addresses of the individual articles\nThe List is the addresses of the webpages where we can find the individual articles filtered by date and category, which we stored in the variable called ‚ÄúURL_total‚Äù,\nThe Action is to find the web addresses of the individual articles, as we did just before,\nThe Loop is to perform the Action for all the filtered pages, and to paste the individual web addresses in the empty data frame we just created.\nThis gives the following code:\n\n\n\n## This step can last several minutes\nURL_IndArticles &lt;- data.frame(matrix(ncol = 1, nrow = 0))\n\nfor (i in 1:length(URL_total)){\n  IndPage &lt;- read_html(URL_total[i])\n  news_url &lt;- IndPage %&gt;% \n    html_nodes(\"div.part_list_2\") %&gt;% \n    html_nodes(\"h3\") %&gt;%\n    html_nodes(\"a\") %&gt;%\n    html_attr(\"href\")\n  \n  news_url &lt;- as.data.frame(news_url)\n  URL_IndArticles &lt;- rbind(URL_IndArticles, news_url)\n}\n\n\nWe are not completely done. It is possible that for some reasons, some articles are repeated. So the next step is to remove all the repeated web addresses, by keeping only one instance. Here is how we do that: We use the ‚Äòunique()‚Äô function:\n\n\narticle_overview_unique &lt;- unique(URL_IndArticles)\n\nknitr::kable(head(article_overview_unique)) # Use of the kable() function from the 'knitr' package to have a prettier output on the website\n\n\n\n\nnews_url\n\n\n\n\nhttps://www.ettoday.net/news/20240101/2655300.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655313.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655305.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655303.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655277.htm\n\n\nhttps://www.ettoday.net/news/20240101/2655264.htm",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#step-3-retrieve-the-titles-date-types-and-content-of-the-articles",
    "href": "Weeks6_7.html#step-3-retrieve-the-titles-date-types-and-content-of-the-articles",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.5 Step 3: Retrieve the titles, date, types and content of the articles",
    "text": "4.5 Step 3: Retrieve the titles, date, types and content of the articles\nWe are already half-way to the end! Now that we have the list of the web addresses of the articles published in 2024, we can scrape their content in addition to other information we might need, including the date of publication and the title of the article.\n\nTest with one article\nAs in the previous step, it is safer to test first with one article. The reason is that as some steps require quite a long time to be completed, so it is better to identify potential problems before by testing with only one subset of the data. Again, we have to go through different sub-steps:\n\nFirst, we use the ‚Äúread_html()‚Äù function to read the structure of the web page of one article:\n\n\nOneArticle &lt;- read_html(article_overview_unique$news_url[1])\n\n\nAfter ‚Äúplaying hide and seek‚Äù with the structure of the page, we can identify the nodes under which the content we want can be found. Let‚Äôs start with the date of publication, which is here:\n\n\n&lt;time class=\"date\" itemprop=\"datePublished\" datetime=\"2024-01-01T22:47:00+08:00\"&gt;                                    2024Âπ¥01Êúà01Êó• 22:47                                       &lt;/time&gt;\n\nWe can see that it is inside the &lt;time&gt; node (between &lt;time&gt; and &lt;/time&gt;). Therefore, in addition to the ‚Äúhtml_nodes()‚Äù function, we specify that we want the text within, hence the use of ‚Äúhtml_text()‚Äù function. The option ‚Äútrim = TRUE‚Äù is just to remove coding format in the text.\nFinally, we store this piece of information in the variable called ‚Äúnews_time‚Äù.\n\n# Date of publication\nnews_time &lt;- OneArticle %&gt;% \n  html_nodes(\"time\") %&gt;% \n  html_text(trim = TRUE)\n\n\nAnd now we proceed the same way for the category of the article, and its title:\n\n\n# Category of news\nnews_class &lt;- OneArticle %&gt;% \n  html_nodes(\".part_menu_5\") %&gt;% \n  html_nodes(\"strong\") %&gt;%\n  html_text(trim = TRUE)\n\n# title \nnews_title &lt;- OneArticle %&gt;% \n  html_nodes(\"h1\") %&gt;% \n  html_text(trim = TRUE)\n\n# content \nnews_body &lt;- OneArticle %&gt;% \n  html_nodes('div[itemprop=\"articleBody\"]') %&gt;% \n  html_text(trim = TRUE)\n\n\nOnce it is done, we store these five variables in the same table:\n\n\narticle &lt;- (data.frame(time = news_time,\n                       class = news_class,\n                       title = news_title,\n                       body = news_body,\n                       url = article_overview_unique$news_url[1]))\n\n\nAnd now we can have a look at what we get with one article:\n\n\n# Use of the paged_table() function from the 'rmarkdown' package to have a readable output\nrmarkdown::paged_table(article)\n\n\n  \n\n\n\n\n\nRepeat the same process with all the articles\nAnd if you were following the rationale of these two weeks‚Äô tutorial, you know that when we repeat the same action again and again, it is a better idea to use the for loop command:\n\nFirst, we create an empty data frame in which we will store all the information we want. Let‚Äôs call it ‚ÄúArticle_total‚Äù.\nThe List is the addresses of the individual articles, which we stored in the variable called ‚Äúarticle_overview_unique$news_url‚Äù,\nThe Action is to find the date of publication, the category of article, the content of the article, its title, and its web address, as we did just before,\nThe Loop is to perform the Action for individual articles, and to include them in the empty data frame we just created.\nYou will remark something new: ‚ÄútryCatch({‚Äù, and at the end ‚Äú}, error=function(e){cat(‚ÄùERROR :‚Äú,conditionMessage(e),‚Äù‚Äú)})‚Äù. This is because the for loop command stops anytime an error occurs (which can happen with a broken link, missing information on one article, etc.).\n\nWrapping the Action with the ‚ÄútryCatch({})‚Äù function is just to say that the for loop can ignore the errors and keep proceeding to the next article,\nThe ‚Äúerror=function(e){cat(‚ÄùERROR :‚Äú,conditionMessage(e),‚Äù‚Äú)})‚Äù part is just to report the error message in the console.\nThis gives the following code:\n\n\n\nArticle_total &lt;- data.frame()\n\nfor (j in 1:length(article_overview_unique$news_url)){\n  tryCatch({\n    temp &lt;- read_html(article_overview_unique$news_url[j])\n    \n    # Date of publication\n    news_time &lt;- temp %&gt;% \n      html_nodes(\"time\") %&gt;% \n      html_text(trim = TRUE)\n    \n    # Category of news\n    news_class &lt;- temp %&gt;% \n      html_nodes(\".part_menu_5\") %&gt;% \n      html_nodes(\"strong\") %&gt;%\n      html_text(trim = TRUE)\n    \n    # title \n    news_title &lt;- temp %&gt;% \n      html_nodes(\"h1\") %&gt;% \n      html_text(trim = TRUE)\n    \n    # content \n    news_body &lt;- temp %&gt;% \n      html_nodes('div[itemprop=\"articleBody\"]') %&gt;% \n      html_text(trim = TRUE)\n    \n    # url\n    news_url &lt;- temp %&gt;% \n      html_nodes(\"div.block div.block_content div.part_list_2 h3\") %&gt;% \n      html_nodes(\"a\") %&gt;%\n      html_attr(\"href\")\n    \n    Article &lt;- (data.frame(time = news_time,\n                           class = news_class,\n                           title = news_title,\n                           body = news_body,\n                           url = article_overview_unique$news_url[j]))\n    Article_total &lt;- rbind(Article_total, Article)\n  }, error=function(e){cat(\"ERROR :\",conditionMessage(e), \"\\n\")})\n}\n\n# Use of the paged_table() function from the 'rmarkdown' package to have a readable output\nrmarkdown::paged_table(head(Article_total))\n\n\n  \n\n\n\n\n\n\n\n\n\nBe careful!!\n\n\n\nThis step can take a very long time! For me, scrapping the articles published in 2024 in these two categories can take 4 hours! You cannot use R when it is working. But you can still use your computer for any other task. So plan accordingly!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Weeks6_7.html#step-4-save-the-data-into-excel-and-r-files",
    "href": "Weeks6_7.html#step-4-save-the-data-into-excel-and-r-files",
    "title": "Weeks 6 & 7: Building a corpus from media platforms",
    "section": "4.6 Step 4: Save the data into Excel and R files",
    "text": "4.6 Step 4: Save the data into Excel and R files\nThe last steps are straightforward: We export the dataset into an Excel document in addition to an RData file:\n\nwrite.xlsx(Article_total, \"ArticleETToday_CorpusCourse.xlsx\")\n\n\nsave(Article_total, file = \"ArticleETToday_CorpusCourse.Rdata\")",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 6 & 7: Building a corpus from media platforms"
    ]
  },
  {
    "objectID": "Week5_1.html",
    "href": "Week5_1.html",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "",
    "text": "Now I assume that R and RStudio are correctly installed on your computer. You can start by opening RStudio. You will see a lot of things on your screen, many blocks, many menus, and it can be quite overwhelming. No worries! This is exactly what this section is about!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#general-overview-of-the-interface-blocks-blocks-blocks-and-blocks",
    "href": "Week5_1.html#general-overview-of-the-interface-blocks-blocks-blocks-and-blocks",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.1 General overview of the interface: Blocks, blocks, blocks and blocks",
    "text": "1.1 General overview of the interface: Blocks, blocks, blocks and blocks\nYou may see the following picture on your screen after opening RStudio:\n\n\n\nThe best way to apprehend RStudio is to is to see the interface as a collection of blocks, which are here for different purposes.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you open RStudio for the very first time, you should get something similar to what is in the image above, with three blocks in the same order. But it‚Äôs also possible that you have something else. If that‚Äôs the case, this is completely normal and this won‚Äôt affect anything for the following steps!\n\n\nThere are four main blocks, as in the following image. I give them unofficial names so that it will be easier to refer to them. There is one block that I call the ‚Äòcomputer block‚Äô, a second one the ‚Äòdata block‚Äô, a third one the ‚Äòscript block‚Äô, and finally the last one, the ‚Äòconsole block‚Äô.\n\n\n\nYou may not be able to see the ‚Äòscript block‚Äô on your computer for the moment, and again, this is completely normal! Just follow the steps below.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#the-computer-block",
    "href": "Week5_1.html#the-computer-block",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.2 The computer block",
    "text": "1.2 The computer block\nThe ‚Äòcomputer block‚Äô is the one on the bottom right side. I call it the ‚Äòcomputer block‚Äô but it‚Äôs actually much more than that, but let‚Äôs keep it simple for the moment. First, click on the Files button. You will see that there is a list of files and folders from your computer. This is actually the interface you can use to communicate and navigate directly with your computer!\n\n\n\n\n\n\nNote\n\n\n\nAgain, look at all the options and play with them to understand what everything‚Äôs about:\n\nClick on the files. How do these open? In RStudio? Directly on your computer?\nTry to go to other folders on your computer using the computer block. You‚Äôll see there is no mystery, this is just like navigating on your own computer as you usually do!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#the-variable-block",
    "href": "Week5_1.html#the-variable-block",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.3 The variable block",
    "text": "1.3 The variable block\nThe ‚Äòvariable block‚Äô is the one on the top right side, as in the image below.\n\n\n\nBasically, the most important tab is the ‚Äòenvironment‚Äô one. This is where the variables we create when coding will be stored. The term ‚Äòvariable‚Äô can be quite abstract at the moment, so let‚Äôs demonstrate this with our first code. R can be used as a simple calculator. In the ‚Äòconsole block‚Äô, just write the following code, end press ‚Äòenter‚Äô:\n\n4+2\n\nNormally, you‚Äôll see this:\n\n4+2\n\n[1] 6\n\n\nAnd indeed, ‚Äò4 + 2‚Äô equals 6. Now, let‚Äôs play a game. You want to obtain the number 8, but you have to use ‚Äò4 + 2‚Äô. One option is to add 2 one more time, and you will write the following code:\n\n4+2+2\n\n[1] 8\n\n\nThe game may continue, and you will need to write even more. This is when the variables can be useful. Try the following code:\n\na &lt;- 4+2\n\nUnlike what you have seen before, nothing appears in the console anymore. But there is something new in the variable block:\n\n\n\nYou stocked the calculation ‚Äò4+2‚Äô as a variable, and you can call it anytime you want! For example, you can call it by simply writing a in the console, and you can also add 2 to it by writing a + 2, and you‚Äôll see something as follows:\n\n&gt; a\n[1] 6\n&gt; a+2\n[1] 8\n\nThis example is just here to show what the ‚Äòvariables‚Äô are. With just simple calculations like the ones above, it seems to be quite useless. But you‚Äôll realize very quickly through this tutorial that variables are not only useful, but also necessary to handle more data.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#the-console-block",
    "href": "Week5_1.html#the-console-block",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.4 The console block",
    "text": "1.4 The console block\nNow let‚Äôs talk very briefly about the ‚Äòconsole block‚Äô. You just had a glimpse of it when reading the section on the variable block. Actually, there‚Äôs not a lot more to say about it. The console block is the place where the code is written and being run.\nIn addition to the ‚ÄòConsole‚Äô tab, there are also the ‚ÄòTerminal‚Äô tab and the ‚ÄòBackground Jobs‚Äô tab. Despite their importance, we won‚Äôt have have to use them in this tutorial. So let‚Äôs just skip them!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#the-script-block",
    "href": "Week5_1.html#the-script-block",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.5 The script block",
    "text": "1.5 The script block\nThe ‚Äòscript‚Äô block is where you‚Äôll work most of the time. You can see it as the Word or the Notepad of RStudio. You will better understand the importance of the scripts, one more time, with an example. If you‚Äôve followed Section 1.3, you can see that you have the same code as the code below in your console.\n\n&gt; 4+2\n[1] 6\n&gt; a &lt;- 4+2\n&gt; a\n[1] 6\n&gt; a+2\n[1] 8\n\nNow, just close R and reopen it. All the codes disappeared! We usually use R for much more than a simple calculator, and we always end up with hundreds of lines of codes. Imagine if you need to rewrite everything each time you close and reopen R‚Ä¶ and this is where the scripts are useful!\nLet‚Äôs do a simple exercise with the script. First, open a new script by clicking on ‚ÄúNew File‚Äù, and then the first option, called ‚ÄúR Script‚Äù.\n\n\n\nWrite the lines below in the script block:\n\n4+2\na &lt;- 4+2\na\na+2\n\nAfter that, select everything, and click on ‚ÄòRun‚Äô. You can also take a look at the short video below.\n\n\n\nThis is how you run a script! Alternatively, you can select everything (or just the line you interested in), and press ‚Äòenter‚Äô on your keyboard. Now you can save your script (‚ÄúFile &gt; Save‚Äù) and close R. Find the file you just saved on your computer and open it. Your code is still here!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#exercise-play-with-your-blocks",
    "href": "Week5_1.html#exercise-play-with-your-blocks",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.6 Exercise: Play with your blocks!",
    "text": "1.6 Exercise: Play with your blocks!\nYour four blocks are like construction toys you can play with. You can make them disappear and then reappear, you can change their size as you wish, and you can even change the background color. In other words, make RStudio your own!\nThis is how you can change the size of the blocks with your mouse:\n\n\n\nThis is how you change the background color of RStudio:\nPath: Tools &gt; Global Options... &gt; Appearance &gt; Choose the theme you like in 'Editor theme'\n\n\n\n\n\n\n\n\n\nAnecdote\n\n\n\n\n\nI personally change the sizes of the blocks dynamically, depending on the task I‚Äôm doing. For example, if I‚Äôm writing more complex lines of codes which could be confusing, I enlarge the script block to be sure I‚Äôm not missing any parenthesis or comma. When I‚Äôm drawing plots, I make the computer block bigger to visualize more clearly what I‚Äôm doing, and it helps me to know if I want to add or change anything on the plot. Everyone‚Äôs experience is different, it just depends on what your preferences are. So, really, make it your own!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#bonus-more-about-the-script-block",
    "href": "Week5_1.html#bonus-more-about-the-script-block",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "1.7 Bonus: More about the script block",
    "text": "1.7 Bonus: More about the script block\nAs you opened a new script, you may have realized that you had the choice between many options, as below.\n\n\n\nWe only used the ‚ÄúR Script‚Äù type, but you can actually do much more depending on what you want to do in your project. For example, you have ‚ÄúQuarto Document‚Äù or ‚ÄúR Markdown‚Äù which are like ‚ÄúR Script‚Äù. The difference is that R Scripts only save your codes, while Quarto Documents or R Markdowns allow more options, and especially to render out in a PDF or markdown file. By doing so, we can share our code along with the output of the code in a beautiful way, which is very important for Open Science!\nYou‚Äôll also remark that even if we are using R, it is actually possible to write with other programming languages, such as HTML or Python. While this is not the case for R Scripts, it is possible with other types of scripts. You can even design a whole website using R, and ask R to transfer your data online‚Ä¶ This is indeed how I am designing the website of the tutorial!\nScript files have many very useful options, so many that I decided to dedicate a whole section to it. Jump here (link provided later) if you want to know more!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#what-is-the-concept-of-the-libraries",
    "href": "Week5_1.html#what-is-the-concept-of-the-libraries",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "2.1 What is the concept of the ‚Äúlibraries‚Äù?",
    "text": "2.1 What is the concept of the ‚Äúlibraries‚Äù?\nYou can do many things with R. Among others:\n\nBasic calculations\nData manipulation\nDraw figures or plots of your data\nCreate geographical maps\nRun simple and advanced statistics (summaries, t-tests, ANOVA, LMR, GLMR, Bayesian statistics, etc.)\nDesign a whole website\nPreprocess neuroimaging data (EEG, fMRI, etc.)\nDownload data from the web to be further manipulated\nCreate applications\n\nAnd many many other possibilities. You may have a sense of it, the more complex the task is, the more codes it requires. For example, it is not the same to calculate ‚Äò2+4‚Äô and putting statistical data on the map of Europe!\nIt can be overwhelming, and we can even have the feeling that quantitative research is not for us after all. Good news! We actually don‚Äôt need to write all the codes from scratch!\n\n\n\n\n\n\nMore on the nature of the R project\n\n\n\nR is free and open source, it is a collaborative concept. And it is very important to be aware of this: Nobody, nobody, writes all the raw codes for each task in one script. Just like we read articles and books instead of doing all the research on the field when we want to know more about something, we can rely on other people‚Äôs work when writing codes. People wrote, and more importantly, published, articles and books so that we can have access to more knowledge. People wrote, and more importantly, shared online, lines and lines of codes and functions such that we can have access to them, use them, instead of rewriting everything.\n\n\nWe need to introduce some concepts now:\n\nLines of codes\nFunctions\nPackages/Libraries\n\n\n\n\n\n\n\n\nName\nConcept\n\n\n\n\nLine of codes\nJust the code you write in the script/console to perform something specific. They are usually the most simple.\n\n\nFunction\nSometimes, you need more than one line of code to perform a task. You may even want to perform the exact same task several times, but with different data. Of course, you can just copy and paste the same code everytime, but you can also choose to simplify it into one line of code. This is what a function is: a cluster of lines of codes\n\n\nLibrary/Package\nAnd very often, you‚Äôll need to perform many tasks which require more than one function. You can choose to write all the functions by hand anytime you need them. Or you can just have them already loaded somewhere in R, such that you can refer to them later when you need them. This is what a library or package is: a cluster of functions\n\n\n\n\nThere are many many packages available for free with R. You can find them here. And they are even more that are not directly available from the R website, but developped and distributed by other people (generally on Github). They always come with a logo, as below.\n\n\n\n\nWe will very often refer to some of them, such as ‚Äúreadxl‚Äù to import data from Excel, ‚Äúggplot2‚Äù to draw figures, ‚Äútidyr‚Äù and ‚Äúdplyr‚Äù to manipulate data, etc.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#how-to-install-and-use-libraries",
    "href": "Week5_1.html#how-to-install-and-use-libraries",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "2.2 How to install and use libraries?",
    "text": "2.2 How to install and use libraries?\nInstalling libraries is quite easy. You have two ways to do so:\n\nUsing the Computer block\nSo far, we‚Äôve only mentioned the Computer block as the block where you can browse the folders and open the files on your computer. This block actually has more functions. If you look at the tabs, you will remark that we‚Äôve only talked about the ‚ÄúFiles‚Äù.\nNow we will turn to the ‚ÄúPackages‚Äù tab, as in the image below. What you need to do is to click on ‚ÄúInstall‚Äù, you write and select the name of the library you want to install, and finally click on ‚ÄúInstall‚Äù. That‚Äôs it!\n\n\n\n\nGive R a couple of minutes while it‚Äôs downloading the data and installing on your computer.\n\nUsing the Console block\nAlternatively, you can download/install packages directly from the Console block. You just need to write one line of code:\n\n\ninstall.packages(\"[Name of the package you wish to install]\")\n\nFor instance, if you want to install the package ‚Äòggplot2‚Äô, which is used to draw figures, here is what you need to write:\n\ninstall.packages(\"ggplot2\")\n\nYou will remark that for installing the packages, you need to write the name of the package between quotation marks\nNow that you have downloaded the packages you need, this does not mean that you can use the functions in the libraries right away. The packages are installed, but you need to ‚Äúcall‚Äù them, or to load them, such that they are ready to use. Note that no quotation marks are used with this function.\n\nlibrary([Name of the package])\nlibrary(ggplot2)\n\n\n\n\n\n\n\nThink about it\n\n\n\nLet‚Äôs keep the metaphor of the articles and books published by other people. You want to prepare a memorable dinner, and to do so you need to buy a book with recipies that you can‚Äôt find online. To do so, you go to the bookstore and buy this wonderful book. Back home, you just put it on your bookshelf. The step of buying the book and bringing it back home corresponds to the ‚Äòinstall.packages()‚Äô step.\nDoes having the book at home mean that you just need to go to your kitchen and start cooking? Of course not! You need to have this book opened with you in the kitchen to follow the recipies! This is exactly what we are doing when we load the packages in the ‚Äòlibrary()‚Äô step!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#first-things-first-the-working-directory",
    "href": "Week5_1.html#first-things-first-the-working-directory",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "3.1 First things first: The ‚Äòworking directory‚Äô",
    "text": "3.1 First things first: The ‚Äòworking directory‚Äô\nThis is one of the most important step, and also something that I always (and everybody should) check before anything else. The ‚Äòworking directory‚Äô refers to the folder in your computer R is looking into. While you can navigate in the folders of your computer using the Computer block, this does not mean that R remembers where you are. To do so, you have to tell R:\n\nThis is THE folder where the files I will load are, THE folder where my script will be saved, THE folder where all my files will be saved\n\nThis is what the ‚Äòworking directory‚Äô, it is THE folder. Now, how to set your working directory, THE folder?\n\nUse the Computer block to navigate in your computer until you are in the folder you wish to work in,\nClick on the small image of the wheel, and then click on ‚ÄúSave as Working Directory‚Äù\n\n\n\n\nAnd you are done! You will see that a line of code appeared in the Console block, actually setting the working directory.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#your-first-script-some-syntax-and-vocabulary-of-r",
    "href": "Week5_1.html#your-first-script-some-syntax-and-vocabulary-of-r",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "3.2 Your first script: Some syntax and vocabulary of R",
    "text": "3.2 Your first script: Some syntax and vocabulary of R\n\n3.2.1 Add comments\nA useful tip when writing R code is the possibility to add comments. I use it all the time such that I can annotate why I used this function, the size I want to print my figure, etc. Comments are marked with the hashtag sign ‚Äú#‚Äù. See the example below. Note that you can add as many hashtag signs as you want. I personally have the habit to use three hashtag signs, but you could also just use one.\n\n    ### This is a line of comment. I won't be run in the console, even if it is copied there.\n    2+4 ### We can also add comments after a code. In that case, it will only run the code at the beginning of the line, and ignore everything after the # sign\n\n[1] 6\n\n\nNow that you know how to add comments in the script you have opened, just comment every line, even the ones that you created yourself!\n\n\n3.2.2 Basic calculations\nWe have already referred to basic calculations in the previous subsections. These so-called basic calculations can even be complex, as below.\n\n2 + 343 * 34 / (5 - 342 + 45 * 36)\n\n[1] 11.08963\n\n\n\n\n3.2.3 Assigning variables\nWe have also already mentioned how to assign variables in previous subsections. Let‚Äôs recall it below.\n\na &lt;- 4+2\n\nAnother way to assign variables is by using the equal sign ‚Äú=‚Äù. You just obtain the same results. I prefer using ‚Äú&lt;-‚Äù because I find it visually more salient, but it depends on one‚Äôs preference.\n\na = 4+2\n\nYou can also assign strings of letters. For example, you can try to add ‚ÄúThis is my first script‚Äù to the variable ‚Äúb‚Äù.\n\nb &lt;- This is my first script\n\nYou will remark that‚Ä¶ it didn‚Äôt work! You surely had the following message:\nError: unexpected symbol in \"b &lt;- This is\"\nThis is ‚ÄúThis is my first script‚Äù has never been defined in R. In other words, R does not know what you are talking about! If you want to add strings of words, you need to use the quotation marks, as below.\n\nb &lt;- \"This is my first script\"\n\nYou can also assign tables, datasets, plots, etc. You can even assign variables to variables. Try this below:\n\nb &lt;- a\n\nWhat happened here? You had a variable called ‚Äòa‚Äô, which had the value ‚Äú6‚Äù. You also had the variable called ‚Äòb‚Äô, which had the value ‚ÄúThis is my first script‚Äù. In the above line of code, you are asking R to assign the value of the variable a to b‚Ä¶ in other words, the value of ‚Äòb‚Äô becomes the same as ‚Äòa‚Äô!\n\n\n3.2.4 Load data from your computer to R\nYou may have already collected data, and you want to import your dataset into R. For this section, let‚Äôs use the data from the survey of ‚ÄúGreat American Coffee Taste Test‚Äù. You can download the file here. (source of the data: https://mavenanalytics.io/data-playground)\n\n\n\n\n\n\nCaution\n\n\n\nDon‚Äôt forget to place this file in the folder you are working in, and to set this folder as your working directory!\n\n\nNow you have two ways to import the data into R:\n\nwith a line of code, or\nwith the R interface.\n\nLet‚Äôs first do it with the code below.\n\ndata &lt;- read.csv(\"/Users/aymeric/Documents/GithubWebsites/CHU03887/assets/files/GACTT_RESULTS_ANONYMIZED_v2.csv\", header=TRUE)\n\nNow let‚Äôs decompose this:\n\nread.csv(). This is the function which is used to import the file, which is a CSV file.\nThere are two elements inside the function, which are seperated with a comma:\n\n‚Äú~/assets/files/GACTT_RESULTS_ANONYMIZED_v2.csv‚Äù. This is the path to access to the file, as well as the full name of the file\nheader=TRUE. This means the first line of the table corresponds to the titles of the columns (you can try and change to ‚Äúheader = FALSE‚Äù to see what happens!).\n\nFinally, the dataset is assigned to the variable called ‚Äúdata‚Äù.\n\n\n\n\n\n\n\nThink about it\n\n\n\nWhy do we need to assign the data to a variable? As I always say, computers are very powerful and very stupid at the same time: they are here to do in a very short amount of time exactly what you tell them to do. In other words, if you only use the ‚Äúread.csv()‚Äù function, it will only read the data, and forget it, that‚Äôs it!\nBut what we want to do actually is to read the data and save them into R, such that we can do further manipulations. This is why we need to ‚Äúsave‚Äù them using variables!\n\n\nThe second way is to use the R interface. To do so, check you Variable block. You have a tab called ‚ÄúImport Dataset‚Äù. Just click on it, and then ‚ÄúFrom Text (base)‚Ä¶‚Äù. A new window will open, and you just have to choose the file you want to import.\nNow you have a new window where you are asked to set the options to import the file:\n\nYou can change the name of the variable it will be assigned to. By default, it is the name of the file. Try to change to ‚Äúdata‚Äù.\nYou can also set the option that the first line corresponds to the names or labels of the columns. Where you see ‚ÄúHeading‚Äù, click on ‚ÄúYes‚Äù.\nAnd finally, you can click on ‚ÄúImport‚Äù.\n\n\n\n\nAnd you are done, now you have a whole dataset ready for manipulation in R!\n\n\n3.2.5 Data description and summary\nI assume here that the dataset we just imported is still present in your environment. Now we will learn quick ways to describe and summarize the data we have.\nThe first step is to understand the structure of the dataset. This can be easily done with the ‚Äòstr()‚Äô function.\n\nstr(data)\n\nYou will obtain the following results. For clarity, I only give the first four lines here.\n'data.frame':   4042 obs. of  111 variables:\n $ Submission.ID  : chr  \"gMR29l\" \"BkPN0e\" \"W5G8jj\" \"4xWgGr\" ...\n $ What.is.your.age.  : chr  \"18-24 years old\" \"25-34 years old\" \"25-34 years old\" \"35-44 years old\" ...\n $ How.many.cups.of.coffee.do.you.typically.drink.per.day.   : chr  \"\" \"\" \"\" \"\" ...\n $ Where.do.you.typically.drink.coffee.  : chr  \"\" \"\" \"\" \"\" ...\nThis is how to read the data:\n\nThe first line indicates the nature of the dataset. This is a ‚Äúdata frame‚Äù, with 4042 rows and 111 columns. Afterwards, the str() function describes these 111 columns.\nThe first column is called ‚ÄúSubmission.ID‚Äù. The rows consist of strings of characters, and the str() function shows the data of the first four lines.\nThe second column is labelled ‚ÄúWhat.is.your.age.‚Äù, and again it consists of rows of strings of characters.\nThe same logic applies for all the lines.\n\nYou will remark that we are facing a problem. In this survey, the participants were asked their age, and then their results were annotated as ‚Äú18-24 years old‚Äù, ‚Äú25-34 years old‚Äù, etc. In other words, there are groups of age, but when the data were imported, R considered them as strings of characters. Now we need to tell R that there are groups. This is done with the ‚Äúas.factor()‚Äù function.\n\ndata$What.is.your.age. &lt;- as.factor(data$What.is.your.age.)\n\nNow let‚Äôs decompose this line:\n\nIn the as.factor() function, we told R the data from which column needed to be transformed into groups. We know that this is the column called ‚ÄúWhat.is.your.age.‚Äù in the dataset called ‚Äúdata‚Äù. In R language, we write as follows: data$What.is.your.age.. The dollar sign is here to translate ‚Äúinside of‚Äù\nIf we just write this part, R will only transform this column as a group, but it will not save it. So we need to save this operation in a variable, and as we want to replace the column considered as strings of characters, we do not have to create a new variable: we just use this column!\n\nActually, the same problem occurs for the two following columns:\n\nHow.many.cups.of.coffee.do.you.typically.drink.per.day., and\nWhere.do.you.typically.drink.coffee.\n\nSo we need to do the same changes.\n\ndata$How.many.cups.of.coffee.do.you.typically.drink.per.day. &lt;- as.factor(data$How.many.cups.of.coffee.do.you.typically.drink.per.day.)\n\ndata$Where.do.you.typically.drink.coffee. &lt;- as.factor(data$Where.do.you.typically.drink.coffee.)\n\nAnd now let‚Äôs run the str() function again. Here are the results of the first four lines:\n'data.frame':   4042 obs. of  111 variables:\n $ Submission.ID  : chr  \"gMR29l\" \"BkPN0e\" \"W5G8jj\" \"4xWgGr\" ...\n $ What.is.your.age. : Factor w/ 8 levels \"\",\"&lt;18 years old\",..: 4 5 5 6 5 8 4 1 5 1 ...\n $ How.many.cups.of.coffee.do.you.typically.drink.per.day. : Factor w/ 7 levels \"\",\"1\",\"2\",\"3\",..: 1 1 1 1 1 1 1 1 6 1 ...\n $ Where.do.you.typically.drink.coffee. : Factor w/ 66 levels \"\",\"At a cafe\",..: 1 1 1 1 1 1 10 1 2 1 ...\nThis is much more informative! For instance, this is telling us that there are 8 groups of age (in R, these are called ‚Äúlevels‚Äù). But maybe we would like to know the number of people per group among the 4042 participants. This can be done with the summary() function.\n\nsummary(data)\n\nThis is what you obtain in the Console block:\n\n\n\nHow to read this? It is telling us that there are 1986 respondents in the ‚Äú25-34 years old‚Äù group, 960 in the ‚Äú35-44 years old‚Äù group, etc. Also that there are 1277 people drinking 1 cup of coffee per day, 1663 people 2 cups of coffee per day, etc.\n\n\n3.2.6 Transforming the data: add, delete, change\nTransforming the data is the task we spend the most of out time when doing the research. This can be tedious, but it is crucial to plot the right data and to conduct the right analyses. It is also very important to be aware of what we are doing when transforming the data. The aim is to make them more easily readable for R.\n\n\n\n\n\n\nCaution\n\n\n\nData transformation or data manipulation does not mean new data creation or data selection! We make changes to highlight some results, we select for ease of clarity, but such changes and selections are actually transformations and selection of the form, not the content!\n\n\nYou are actually already familiar with data transformation. In the previous section we have changed the class of the data of three columns: strings of characters to factors. I will exemplify here other changes we can make, but the list is not exhaustive at all.\nYou have surely remarked that there are many columns in the dataset we loaded. Generally, we collect more than we need, in case that in the end, we may need these data. But sometimes, having very large datasets can make us quite confused when trying to read and interpret the data. Therefore, sometimes, we would like a dataset with only the comumns we need. For instance, we have talked of three columns so far in the previous section. We want to select them, in addition to the column indicating the ID of the respondents. We can do so with the code below.\n\ndata2 &lt;- data[,c(\"Submission.ID\",\n                 \"What.is.your.age.\",\n                 \"How.many.cups.of.coffee.do.you.typically.drink.per.day.\",\n                 \"Where.do.you.typically.drink.coffee.\")]\n\nAgain, let‚Äôs decompose this:\n\nThe dataset from which we want to select data is called ‚Äòdata‚Äô. We select data from it by adding brackets, therefore ‚Äúdata[ ]‚Äù.\nThe columns are selected by adding a comma first (there are reasons for this, but it is not important for our purpose now), and then adding the name of the column. For example, we may have written ‚Äúdata[,‚ÄùSubmission.ID‚Äù]‚Äú, and this selects the columns called ‚ÄòSubmission.ID‚Äô.\nBut we want to select more than one column. Unfortunately, we cannot write the names of all the columns directly. We need to inform R first that there is a list of columns. We use ‚Äúc()‚Äù to do so.\nFinally, we save this into a variable. We may have saved this to ‚Äúdata‚Äù (i.e.¬†‚Äúdata &lt;- data[‚Ä¶.]‚Äù), but when doing so, we would have overwritten our original data. And if in the end, we realize that we need other columns, we have to redo everything! This is why it is good to have the habit to save into other variables when performing transformations of this kind.\n\nNow we will learn another trick: How to check which groups we have. In other words, how many groups of age are there in the data we have? This can be done as below:\n\nlevels(data2$What.is.your.age.)\n\n[1] \"\"                \"&lt;18 years old\"   \"&gt;65 years old\"   \"18-24 years old\"\n[5] \"25-34 years old\" \"35-44 years old\" \"45-54 years old\" \"55-64 years old\"\n\n\nWe use the level() function, and we inform R which columns we want to look at inside the parentheses. We obtain the results as above.\nThere are 8 groups, but the first one corresponds to people who did not reply. We would like to remove this in order to have a full dataset. If we do not want to do it with other libraries, we can do it as follows:\n\ndata2 &lt;- subset(data2, What.is.your.age. != \"\")\n\ndata2$What.is.your.age. &lt;- droplevels(data2$What.is.your.age.)\n\n\nIn the first line, we are telling R to remove the rows where the group is ‚Äú‚Äú.\n\nTo do so, we use the subset() function. In the parentheses, we set the parameters to tell R which rows to remove: dataset name (‚Äúdata2‚Äù), column (‚ÄúWhat.is.your.age.‚Äù), level (‚Äú‚Äú).\nNote that we used the sign ‚Äú!=‚Äù. This sign means ‚Äúremove‚Äù. If we change this sign to ‚Äú==‚Äù, it means ‚Äúselect‚Äù!\n\nWe only removed the rows corresponding to ‚Äú‚Äú, but not the label. This is the purpose of the second line.\n\nNow you can run the ‚Äúlevels(data2$What.is.your.age.)‚Äù again. You will see that the level ‚Äú‚Äù has indeed been removed!\n\nlevels(data2$What.is.your.age.)\n\n[1] \"&lt;18 years old\"   \"&gt;65 years old\"   \"18-24 years old\" \"25-34 years old\"\n[5] \"35-44 years old\" \"45-54 years old\" \"55-64 years old\"\n\n\nIf we look at the column called ‚ÄúHow.many.cups.of.coffee.do.you.typically.drink.per.day.‚Äù, there is the same problem. We just have to run the same codes to solve this!\n\ndata2 &lt;- subset(data2, How.many.cups.of.coffee.do.you.typically.drink.per.day. != \"\")\n\ndata2$How.many.cups.of.coffee.do.you.typically.drink.per.day. &lt;- droplevels(data2$How.many.cups.of.coffee.do.you.typically.drink.per.day.)\n\nNow we would like to summarize the number of observations per group. And I will have to make an exception to what I said earlier: We are using the package called ‚Äúdplyr‚Äù. So you will need to download it beforehand.\nHere is the code you need to write:\n\nlibrary(dplyr)\n\ndata3 &lt;- data2 %&gt;% \n  group_by(What.is.your.age., How.many.cups.of.coffee.do.you.typically.drink.per.day.) %&gt;% \n  summarize(Count = n())\n\nLet‚Äôs decompose this:\n\nIn the first line, we just loaded the library we need.\nThe sign ‚Äú%&gt;%‚Äù is here to say we are going to run the codes below it based on the dataset called ‚Äúdata2‚Äù\nThe ‚Äúgroup_by()‚Äù function is here to tell R which columns we will take into consideration. Notice that we do not need to write ‚Äúdata2$What.is.your.age.‚Äù since we used the ‚Äú%&gt;%‚Äù sign.\nFinally, the last line of code is the transformation we are performing: We want to summarize the data. Inside the ‚Äúsummarize()‚Äù function, we need to give the details of what we want to do (notice that you can also write ‚Äúsummarise()‚Äù!).\n\nThe ‚Äún()‚Äù function means that we just want to count the number of observations per group,\nAnd the observations will be found in the ‚ÄúCount‚Äù column. Note that you can replace ‚ÄúCount‚Äù by any name you want!\n\nAnd finally, we store this summary table into a new variable, which we call ‚Äúdata3‚Äù.\n\nYou should obtain something like this in the picture below:\n\n\n\nThere are of course many more ways to transform the data, summarize them, etc. This subsection was only a snapshot of what is possible to do with a very minimum number of lines of codes!\n\n\n3.2.7 Plotting the data\nNow that we have summarized data, we can plot to see what they look like. We will use a very straightforward function, called‚Ä¶ ‚Äúplot()‚Äù! Let‚Äôs see what it does with the codes below.\n\nplot(x = data3$How.many.cups.of.coffee.do.you.typically.drink.per.day., y = data3$Count)\n\n\n\n\n\n\n\nplot(x = data3$What.is.your.age., y = data3$Count)\n\n\n\n\n\n\n\n\nWe ran into a minor problem: R sorted the groups in alphabetical order, which is not quite straightforward! So we will need to reorder the levels first. Let‚Äôs do it with the code below:\n\ndata3$What.is.your.age. &lt;- factor(data3$What.is.your.age., levels = c(\"&lt;18 years old\",\n    \"18-24 years old\",\n    \"25-34 years old\",\n    \"35-44 years old\",\n    \"45-54 years old\",\n    \"55-64 years old\",\n    \"&gt;65 years old\"))\n\ndata3$How.many.cups.of.coffee.do.you.typically.drink.per.day. &lt;- factor(data3$How.many.cups.of.coffee.do.you.typically.drink.per.day., levels = c(\"Less than 1\",\n         \"1\",\n         \"2\",\n         \"3\",\n         \"4\",\n         \"More than 4\"))\n\nYou can check whether this worked by using the ‚Äúlevels()‚Äù function we introduced above.\n\nlevels(data3$What.is.your.age.)\n\n[1] \"&lt;18 years old\"   \"18-24 years old\" \"25-34 years old\" \"35-44 years old\"\n[5] \"45-54 years old\" \"55-64 years old\" \"&gt;65 years old\"  \n\n\n\nlevels(data3$How.many.cups.of.coffee.do.you.typically.drink.per.day.)\n\n[1] \"Less than 1\" \"1\"           \"2\"           \"3\"           \"4\"          \n[6] \"More than 4\"\n\n\nEverything is in the order we want, now we can plot one more time, You will obtain the plots as below!\n\nplot(x = data3$What.is.your.age., y = data3$Count)\n\n\n\n\n\n\n\nplot(x = data3$How.many.cups.of.coffee.do.you.typically.drink.per.day., y = data3$Count)\n\n\n\n\n\n\n\n\nThe first plot shows that most of the participants are between 25 and 34 years old. The second plot indicates that most of the respondents drink between 1 and 2 cups of coffee everyday.\nYou might be curious whether there is an interaction, with older people drinking more coffee than younger people. In other words, you would need three variables in the codes‚Ä¶ which is very difficult to do without other packages, espectially the ‚Äúggplot2‚Äù one! You can try by yourself!\n\n\n3.2.8 Save data from R to your computer\nFor some reason, we may want to save our data in the computer. But we are actually referring to two things at the same time:\n\nSaving the R dataset, and\nExporting the R dataset such that we can open it with other programs.\n\nBoth are doable, and let‚Äôs start with the first option with the code below.\n\nsave(data3, file = \"data3.Rdata\")\n\nWe just need the ‚Äúsave()‚Äù function, inside which we first tell R the dataset we want to save, and then the name of the file on the computer. We can choose the name of the file by ourself, as long as we do not forget the ‚Äú.Rdata‚Äù extension. Once it is save, we can load the file later using the ‚Äúload()‚Äù function.\n\nload(file = \"data3.Rdata\")\n\nWe may also want to save the data as a .csv file, readable by programs such as Excel. This is done with the code below:\n\nwrite.csv(data3, \"data3.csv\", row.names = FALSE)\n\nLet‚Äôs unpack it:\n\nWe use the ‚Äúwrite.csv‚Äù function, and we provide details inside the parentheses for:\n\nThe name of the dataset we want to export (here, data3),\nThe name we want it to have in our computer (here, data3.csv), and\nthe ‚Äúrow.names = FALSE‚Äù is here to say that we do not want the index of the rows. You can try and change to ‚ÄúTRUE‚Äù to see what happens.\n\n\nIt is also possible to export to .xlsx files, more directly readable by Excel, but this requires another library. We will encounter this in the rest of the tutorial.\n\n\n3.2.9 R script of this section\nYou can find the R scripts including the codes presented in this section here. Don‚Äôt hesitate to use it and add comments for each line of code to make your own!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week5_1.html#common-mistakes-or-how-to-save-a-lot-of-time",
    "href": "Week5_1.html#common-mistakes-or-how-to-save-a-lot-of-time",
    "title": "Week 5: Introduction to R (2/2): Basic functions",
    "section": "3.3 Common mistakes, or how to save a lot of time",
    "text": "3.3 Common mistakes, or how to save a lot of time\nYou will run into many error messages, these intimidating red lines in the console block telling you that something has gone wrong. We can spend hours trying to figure out what to do to make the codes work. Again, no worries! This is part of the learning process, and to be honest, even experts can‚Äôt avoid error messages. The most important thing is to learn from them, so that we are able to understand what the problem is, and how to solve it.\nFrom my experience, there are mistakes we often make especially as first users of R. I remember that many students in the classes I attended and taught gave up on learning programming languages just because they had to spend too much time on debugging very basic mistakes, and they couldn‚Äôt focus on the opportunities R can offer. Below is a non-exhaustive list of such mistakes that I may update in the future.\n\n3.3.1 I can‚Äôt load the data from my computer\n\n\n3.3.2 I don‚Äôt have any error message, but my code won‚Äôt run\n\n\n3.3.3 ‚ÄúVariable not found‚Äù, what happened?",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): Basic functions"
    ]
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "Week 4: Introduction to the modern tool R (1/2): Installation and first steps",
    "section": "",
    "text": "So far, we have been talking about something called ‚ÄúR/RStudio‚Äù. People familiar with this program know what this is about. However, it can be quite confusing when we have never heard of it. We are actually referring to two programs:\n\n\n\n\n\n\n\nR\nRStudio\n\n\n\n\n\n\n\n\n\n\nThe main program is called R. This is where all the magic happens. RStudio is a user-friendly interface such that it is easier to work with.\nMaybe a metaphor can help to understand the difference between R and RStudio. As you are engaging yourself in this tutorial, I assume that you have a computer (and that you are using one). You can create folders, open Word or Excel documents, write in it and save them in the folders you created.\n\n\n\n\n\n\nThink about it\n\n\n\nNow it is time to reflect on what you are doing. You have done these tasks by using your mouse or your touchpad and clicking on icons (or figures) on your screen. What you may not know is that when doing so, you have run many lines of codes. Actually, it is your computer that ran them in programs able to perform computations. Your role has been to trigger these programs by using the user-friendly interface installed in your computer.\n\n\nThis is the exact same thing with R and RStudio. R will perform the computations that you ask for. And you will ask R to do stuff by clicking on icons and writing commands using the user-friendly interface of RStudio. And this is why you need to download two programs!\n\n\n\n\n\n\nAnecdote\n\n\n\nLet me share a personal anecdote when I just entered graduate school and I had my very first class of language programming for linguistic research. Just before leaving, the instructor said very quickly that we had to, I quote, ‚Äúdownload R for next time‚Äù. I had never heard of R, and to me, calling a program with only one letter seemed kind of awkward. Therefore, I assumed that I misheard the instructions, or that I missed some information. You can just imagine how surprised I was when I googled ‚ÄúR‚Äù and actually found that this existed!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 4: Introduction to the modern tool R (1/2): Installation and first steps"
    ]
  },
  {
    "objectID": "Week4.html#download-and-install-r",
    "href": "Week4.html#download-and-install-r",
    "title": "Week 4: Introduction to the modern tool R (1/2): Installation and first steps",
    "section": "2.1 Download and install R",
    "text": "2.1 Download and install R\nYou can download R for free from the official website here.\nYou have to look for the link in the middle of the first paragraph on the front page.\n\n\n\nThen, you need to choose your CRAN. Let‚Äôs choose the one in Taiwan. You will need to scroll down the page to find the link.\n\n\n\nNow you need to choose the files to download according to the system of your computer. The likeliest to use are for MacOS (if you have a MacBook computer) or for Windows (if your system is Microsoft).\n\n\n\nOn the next page, you will need to choose the subdirectory that you need. For our purposes, we‚Äôll only need the ‚Äúbase‚Äù subdirectory.\n\n\n\nWe finally got to the last page! Just click on the first link to download the files, as in the image below.\n\n\n\nJust wait until the file is downloaded, open it and follow the instructions to install R on your computer.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 4: Introduction to the modern tool R (1/2): Installation and first steps"
    ]
  },
  {
    "objectID": "Week4.html#download-and-install-rstudio",
    "href": "Week4.html#download-and-install-rstudio",
    "title": "Week 4: Introduction to the modern tool R (1/2): Installation and first steps",
    "section": "2.2 Download and install RStudio",
    "text": "2.2 Download and install RStudio\nNow that R is installed on your computer, it is time to do the same with RStudio. First, let‚Äôs go to the RStudio website by clicking here , and you will see something like that (if not, it is just that the RStudio website has changed):\n\n\n\nOnce you are on the front page, just scroll down to look for the links to download the installing files. Again, you will need to choose the right file to download according to the system of your computer: Window, macOS (for the most common), or another one.\n\n\n\nJust click on the link and the file will start to be downloaded! Again, give it some minutes, then open the file and follow the instructions to install RStudio on your computer.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 4: Introduction to the modern tool R (1/2): Installation and first steps"
    ]
  },
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "Week 2: What is a corpus, and their role to understand language and literature",
    "section": "",
    "text": "Objectives\n\n\n\nThis week, we will discuss why corpus studies are useful for: - Understanding the language we speak - Understanding the novels and stories we read\n\n\nWe will have in-class discussions based on the following chapters.\n\n\n\n\n\n\n\nBook\nChapters to read\n\n\n\n\n\n\nFor students interested in language studies: Chapter 15, Chapter 40\n\nFor students interested in literature studies: Chapter 17, Chapter 37\n\n\n\n\n\n\n\n\n\nHomeworks for next week\n\n\n\nYou have two tasks for next week:\n1/ Please read the chapters for Week 3\n2/ Start thinking about a topic that you find interesting. We can discuss your ideas next week!",
    "crumbs": [
      "Welcome page",
      "Part 1: Principles of corpus studies",
      "Week 2: What is a corpus, and their role to understand language and literature"
    ]
  },
  {
    "objectID": "Week13.html#two-packages-quanteda-and-ggplot2",
    "href": "Week13.html#two-packages-quanteda-and-ggplot2",
    "title": "Week 13: Data visualization",
    "section": "2.1 Two packages: ‚Äòquanteda‚Äô and ‚Äòggplot2‚Äô",
    "text": "2.1 Two packages: ‚Äòquanteda‚Äô and ‚Äòggplot2‚Äô\n\n‚Äòquanteda‚Äô, or more precisely, ‚Äòquanteda.textplots‚Äô\n\n\n‚Äòggplot2‚Äô",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 13: Data visualization"
    ]
  },
  {
    "objectID": "Week13.html#workflow-for-word-clouds-and-frequency-dot-plots",
    "href": "Week13.html#workflow-for-word-clouds-and-frequency-dot-plots",
    "title": "Week 13: Data visualization",
    "section": "2.2 Workflow for word clouds and frequency dot plots",
    "text": "2.2 Workflow for word clouds and frequency dot plots",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 13: Data visualization"
    ]
  },
  {
    "objectID": "Week13.html#explanation-of-the-r-code",
    "href": "Week13.html#explanation-of-the-r-code",
    "title": "Week 13: Data visualization",
    "section": "2.3 Explanation of the R code",
    "text": "2.3 Explanation of the R code",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 13: Data visualization"
    ]
  },
  {
    "objectID": "Weeks11.html",
    "href": "Weeks11.html",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "",
    "text": "This tutorial is exemplified with the data from ETToday!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#why-the-need-to-clean-the-data",
    "href": "Weeks11.html#why-the-need-to-clean-the-data",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "1.1 Why the need to clean the data?",
    "text": "1.1 Why the need to clean the data?\nWe just scraped data from the Internet. Even if we have been very careful when writing the scraping code, there are surely unnecessary data we need to remove, format problems, or further annotations to do. The reasons are quite simple: (a) while the majority of the webpages we scraped has a consistent structure, it is possible that some were slightly different, (b) unnecessary parts were coded under the same HTML nodes, such that it was not possible to scrape the data we wanted without scraping them as well, (c) it was easier to write the scraping code that way.\nThat‚Äôs why we need to go through the cleaning process before rushing into the analyses! The main problems we will encounter are:\n\nThe format of the table is not suited for analyses, so we need to arrange it\nUnnecessary data need to be removed, otherwise it will affect the results\nSimple annotations can be made, and these will be helpful for both the cleaning and analyses processes",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#some-cleaning-principles",
    "href": "Weeks11.html#some-cleaning-principles",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "1.2 Some cleaning principles",
    "text": "1.2 Some cleaning principles\n\nWe clean, we don‚Äôt fish\nThe cleaning process is here such that the data set will be ready for further analyses. But there are two important points we need to keep in mind:\n\nWe might over-clean, meaning that while we removed unnecessary data, we removed much more than what we thought. That‚Äôs why we need to check each time that we are not removing too much\nWe might be biased. We have a research question in mind, and maybe some ideas about what the results may look like. Sometimes, we are so engaged in our study that we assume the results before conducting the analyses‚Ä¶ and in that case, we may be so biased that we consider counterexamples as errors to remove before the analyses. This is completely wrong, and the cleaning process should be blind!!\n\n\n\nWe must be aware of the final format of the data set\nEspecially in R, the most common format of the data set is ‚Äú1 row = 1 data point‚Äù. A ‚Äúdata point‚Äù may be one whole text, one paragraph, one sentence, etc. Then, each column of that row is related to the same data point. In other words, it is not ideal to have the same data point on two different rows! Transforming the data towards that direction is also part of the cleaning process.\nExample of a data set with a suitable format (fake data):\n\n\n\nDate\nArticle\nSentence\n‚Ä¶ (other data)\nLink\n\n\n\n\n2024-01-01\nArticle 1\nSentence 1\n‚Ä¶\nLink 1\n\n\n2024-01-01\nArticle 1\nSentence 2\n‚Ä¶\nLink 1\n\n\n2024-01-01\nArticle 1\nSentence 3\n‚Ä¶\nLink 1\n\n\n2024-01-03\nArticle 2\nSentence 1\n‚Ä¶\nLink 2\n\n\n\n\n\nWe must be close to our data\nThere are at least two reasons for that:\n\nThe closer you are to your data, the more you know what to do to clean them\nNot all the cleaning steps are necessary, it really depends on your data set!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#a-proposed-workflow",
    "href": "Weeks11.html#a-proposed-workflow",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "1.3 A proposed workflow",
    "text": "1.3 A proposed workflow\nPlease do not hesitate to zoom in!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#playing-hide-and-seek-the-goal-of-the-cleaning-process",
    "href": "Weeks11.html#playing-hide-and-seek-the-goal-of-the-cleaning-process",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "2.1 Playing hide-and-seek: The goal of the cleaning process",
    "text": "2.1 Playing hide-and-seek: The goal of the cleaning process\nAs it is made evident from the workflow, the cleaning process is a little bit like playing hide-and-seek. We must find what we need to clean, and one way is to randomly scroll the data.\n\n\n\n\n\n\nHow to know what to clean?\n\n\n\nThe reality is that we will always miss something at some point. Most of the time, we realize that we need more cleaning during the analysis process. At that moment, we just add another step‚Ä¶ and this is why having scripts already prepared is essential, since implementing a new step is easy and not time-consuming!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#list-of-the-steps-to-clean-the-ettoday-corpus",
    "href": "Weeks11.html#list-of-the-steps-to-clean-the-ettoday-corpus",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "2.2 List of the steps to clean the ETtoday corpus",
    "text": "2.2 List of the steps to clean the ETtoday corpus\nHere is the list of the steps to go through to clean the ETtoday corpus based on my observations:\n\nTransformations:\n\nThe whole article appear in one cell. We need to transform such that one cell = one paragraph\n\nAnnotations:\n\nAnnotate based on year, month and day\n\nData removal:\n\nRemove the legends of the images\nRemove messages from ETtoday unrelated to the article (for instance: ‚Äúbe caution of your alcohol consumption‚Äù, etc.)\nRemove rows corresponding to the identity of the journalist\nRemove data from 2023\nRemove empty rows\nRemove weird scraped instances (e.g., HTML language, etc.)",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#prepare-the-environment",
    "href": "Weeks11.html#prepare-the-environment",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "3.1 Prepare the environment",
    "text": "3.1 Prepare the environment\nThis section sets up the workspace by loading necessary tools and the raw data.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(openxlsx)\n\n\nLoad the libraries:\nlibrary(dplyr) and library(tidyr): These are loaded for data manipulation. dplyr is used for filtering and mutating data, while tidyr is specifically used here to reshape the data structure (unnesting paragraphs).\nlibrary(openxlsx): Loaded to handle Excel file export at the end of the script.\n\n\nLoad the originally scraped data:\n\nload(file = \"ArticleETToday_CorpusCourse.Rdata\")\n\nThe command load(file = \"ArticleETToday_CorpusCourse.Rdata\") imports a previously saved R workspace file containing the raw scraped articles.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#examples-of-cleaningpreparing-processes",
    "href": "Weeks11.html#examples-of-cleaningpreparing-processes",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "3.2 Examples of cleaning/preparing processes",
    "text": "3.2 Examples of cleaning/preparing processes\nThis is the core of the script, where the raw text is processed into a clean format suitable for analysis.\n\nFurther annotations\nThe code extracts temporal metadata from a single string column.\n\nArticle_total$year &lt;- substr(Article_total$time, start = 1, stop = 4)\nArticle_total$month &lt;- substr(Article_total$time, start = 6, stop = 7)\nArticle_total$day &lt;- substr(Article_total$time, start = 9, stop = 10)\n\nIt creates three new columns (year, month, day) by cutting specific parts of the time column using the substr (substring) function. For example, it assumes the first 4 characters representing the year.\n\n\nData transformation\n\nSplit the articles into paragraphs\nThis is a structural transformation step.\nLogic: The original data has one row per article. This code changes the unit of analysis to one row per paragraph.\n\nArticle_total$original_article &lt;- Article_total$body\n\nArticle_total2 &lt;- Article_total %&gt;% \n  mutate(body = strsplit(as.character(body), \"\\r\\n\")) %&gt;% \n  unnest(body)\n\nCode: It splits the body text wherever it finds a carriage return code (). It then uses unnest(body) to expand these split pieces into individual rows.\nResult: If an article had 5 paragraphs, it will now occupy 5 rows in the dataframe Article_total2.\n\n\nRemove unwanted paragraphs\nThis section acts as a filter to remove ‚Äúnoise‚Äù‚Äîtext that is not part of the actual news report.\nExample 1 (Image Legends): Removes rows containing specific symbols like ‚Äú‚ñ≤‚Äù or ‚Äú‚ñº‚Äù, and lines starting with ‚ÄúÂúñÔºè‚Äù (Image) or ‚ÄúÊñáÔºè‚Äù (Text/Author), which usually indicate captions or credits.\n\nArticle_total2 &lt;- Article_total2[-grep(\"‚ñ≤\", Article_total2$body),]\nArticle_total2 &lt;- Article_total2[-grep(\"‚ñº\", Article_total2$body),]\n\n### Most of them start with the \"ÂúñÔºè\" (image) and \"ÊñáÔºè\" (text) source: Check first if that's the case\ntest &lt;- Article_total2[+grep(\"ÂúñÔºè\", Article_total2$body),]\ntest &lt;- Article_total2[+grep(\"ÊñáÔºè\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"ÂúñÔºè\", Article_total2$body),]\nArticle_total2 &lt;- Article_total2[-grep(\"ÊñáÔºè\", Article_total2$body),]\n\nExample 2 (Boilerplate/System Messages): Removes standard ETtoday interface text and links.\n\n### Most of them start with the \"‚óè\" symbol: Check first if that's the case\ntest &lt;- Article_total2[+grep(\"‚óè\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"‚óè\", Article_total2$body),]\n\n### Some start with the \"‚ñ∫\" and \"‚ñ∏\" symbol (link to other articles): Check first if that's the case\ntest &lt;- Article_total2[+grep(\"‚ñ∫\", Article_total2$body),]\n\ntest &lt;- Article_total2[+grep(\"‚ñ∏\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"‚ñ∫\", Article_total2$body),]\nArticle_total2 &lt;- Article_total2[-grep(\"‚ñ∏\", Article_total2$body),]\n\n### Most of them start with the \"‚òÖ\" symbol: Check first if that's the case\ntest &lt;- Article_total2[+grep(\"‚òÖ\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"‚òÖ\", Article_total2$body),]\n\n### Some rows are just made of the message \"„ÄêÂÖ∂‰ªñÊñ∞ËÅû„Äë\" (other news): Check first if that's the case\ntest &lt;- Article_total2[+grep(\"„ÄêÂÖ∂‰ªñÊñ∞ËÅû„Äë\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"„ÄêÂÖ∂‰ªñÊñ∞ËÅû„Äë\", Article_total2$body),]\n\n### Some rows are just made of the message \"Êõ¥Â§öÊñ∞ËÅû\" (more news): Check first if that's the case\ntest &lt;- Article_total2[+grep(\"Êõ¥Â§öÊñ∞ËÅû\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"Êõ¥Â§öÊñ∞ËÅû\", Article_total2$body),]\n\n### Some rows are just made of the message \"Âª∂‰º∏Èñ±ËÆÄ\" (read more): Check first if that's the case\ntest &lt;- Article_total2[+grep(\"Âª∂‰º∏Èñ±ËÆÄ\", Article_total2$body),]\n\n### Indeed the case: Can be removed\nArticle_total2 &lt;- Article_total2[-grep(\"Âª∂‰º∏Èñ±ËÆÄ\", Article_total2$body),]\n\nIt filters out symbols like ‚Äú‚óè‚Äù, ‚Äú‚ñ∫‚Äù, ‚Äú‚ñ∏‚Äù, and ‚Äú‚òÖ‚Äù.\nIt removes navigational phrases like ‚Äú„ÄêÂÖ∂‰ªñÊñ∞ËÅû„Äë‚Äù (Other news), ‚ÄúÊõ¥Â§öÊñ∞ËÅû‚Äù (More news), and ‚ÄúÂª∂‰º∏Èñ±ËÆÄ‚Äù (Extended reading).\nExample 3 (Journalist Identity):\nIt creates a temporary column FirstTwoCharacters to check the start of the paragraph.\n\n### Most of them start with the two characters \"Ë®òËÄÖ\" (journalist)\ntest &lt;- Article_total2\ntest$FirstTwoCharacters &lt;- substr(Article_total2$body, start = 1, stop = 2)\n## Only 178 sentences out of 361154 will be wrongly removed, quite acceptable\n\nArticle_total2$FirstTwoCharacters &lt;- substr(Article_total2$body, start = 1, stop = 2)\nArticle_total2 &lt;- Article_total2[-grep(\"Ë®òËÄÖ\", Article_total2$FirstTwoCharacters),]\n\nArticle_total2$FirstTwoCharacters &lt;- NULL\n\n\n### Some start with the four characters \"ÂØ¶ÁøíË®òËÄÖ\" (journalist-internship)\ntest &lt;- Article_total2\ntest$FirstFourCharacters &lt;- substr(Article_total2$body, start = 1, stop = 4)\ntest &lt;- test[+grep(\"ÂØ¶ÁøíË®òËÄÖ\", test$FirstFourCharacters),]\n\n## Indeed all rows need to be removed\nArticle_total2$FirstFourCharacters &lt;- substr(Article_total2$body, start = 1, stop = 4)\nArticle_total2 &lt;- Article_total2[-grep(\"ÂØ¶ÁøíË®òËÄÖ\", Article_total2$FirstFourCharacters),]\n\nArticle_total2$FirstFourCharacters &lt;- NULL\n\nIt removes rows starting with ‚ÄúË®òËÄÖ‚Äù (Reporter) or ‚ÄúÂØ¶ÁøíË®òËÄÖ‚Äù (Intern Reporter) to strip out bylines.\nExample 4 (Date Filtering):\n\nArticle_total2 &lt;- Article_total2[!(Article_total2$year==\"2023\"), ]\n\nIt removes all rows where the year is ‚Äú2023‚Äù. The comment notes this is ‚Äúcontroversial,‚Äù possibly because it indiscriminately drops data based on date rather than content quality.\n\n\nRemove empty rows This section deals with ‚Äúinvisible‚Äù noise.\nIterative Cleaning: The user manually identified various forms of empty space (empty strings ‚Äú‚Äú, single spaces‚Äù ‚Äú, multiple spaces‚Äù ‚Äú, and specific whitespace characters found at specific row indices like 29 or 13974).\nProcess: The code repeatedly checks for these specific empty patterns and removes them to ensure the final dataset contains only text with content.\n\n## Test the code to make sure we are not removing too much\ntest &lt;- Article_total2[(Article_total2$body==\"\"), ]\n\n## Indeed all the rows correspond to empty paragraphs\n\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\"\"), ]\n\n## Visual inspection: Still empty rows, such as line number 29\nArticle_total2$body[29] ## corresponds to a space\n\n## Test the code to make sure we are not removing too much\ntest &lt;- Article_total2[(Article_total2$body==\" \"), ]\n\n## Indeed all the rows correspond to paragraphs with a space\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\" \"), ]\n\n## Potentially more than one space: Test the code to make sure we are not removing too much\ntest &lt;- Article_total2[(Article_total2$body==\"  \"), ] #test two spaces\ntest &lt;- Article_total2[(Article_total2$body==\"   \"), ] #test three spaces\ntest &lt;- Article_total2[(Article_total2$body==\"    \"), ] #test four spaces\ntest &lt;- Article_total2[(Article_total2$body==\"     \"), ] #test five spaces\ntest &lt;- Article_total2[(Article_total2$body==\"      \"), ] #test six spaces\ntest &lt;- Article_total2[(Article_total2$body==\"       \"), ] #test seven spaces\n\n## Indeed all the rows correspond to paragraphs with 2 to 5 spaces\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\"  \"), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\"   \"), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\"    \"), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==\"     \"), ]\n\n## Visual inspection: Still empty rows, such as line number 29\nArticle_total2$body[29] \n\n## I don't really know what this is, check using the row number instead of the symbol itself\ntest &lt;- Article_total2[(Article_total2$body==Article_total2$body[29]), ]\n\n## Extract the symbol\nToRemove &lt;- Article_total2$body[29] \n\n## Indeed all the rows correspond to paragraphs with a space --&gt; Removing using the symbol itself\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==ToRemove), ]\n\n## Visual inspection: Still empty rows, such as line number 13974\nArticle_total2$body[13974] ## corresponds to nothing, but wasn't caught earlier\n\n## Extract the symbol, test and remove\nToRemove &lt;- Article_total2$body[13974] \ntest &lt;- Article_total2[(Article_total2$body==ToRemove), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==ToRemove), ]\n\n## Visual inspection: Still empty rows, such as line number 8368\nArticle_total2$body[8368] ## I don't know what kind of space it is\n\n## Extract the symbol, test and remove\nToRemove &lt;- Article_total2$body[8368] \ntest &lt;- Article_total2[(Article_total2$body==ToRemove), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==ToRemove), ]\n\n## Visual inspection: Still empty rows, such as line number 67\nArticle_total2$body[67] ## I don't know what kind of space it is\n\n## Extract the symbol, test and remove\nToRemove &lt;- Article_total2$body[67] \ntest &lt;- Article_total2[(Article_total2$body==ToRemove), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==ToRemove), ]\n\n## Visual inspection: Still empty rows, such as line number 15279\nArticle_total2$body[15279] ## I don't know what kind of space it is\n\n## Extract the symbol, test and remove\nToRemove &lt;- Article_total2$body[15279] \ntest &lt;- Article_total2[(Article_total2$body==ToRemove), ]\nArticle_total2 &lt;- Article_total2[!(Article_total2$body==ToRemove), ]\n\n\n\nRemove weird scraping instances\nHTML Cleanup: The code uses grepl(\"&lt;[^&gt;]+&gt;\", ...) to identify rows that still contain HTML tags (text starting with &lt; and ending with &gt;).\n\n## This correspond to cases where the signs \"&lt;\" + other signs appear, these are rows with HTML language\ntest &lt;- Article_total2\ntest_HTML &lt;- grepl(\"&lt;[^&gt;]+&gt;\", Article_total2$body)\ntest$HTML &lt;- test_HTML\ntest &lt;- test[(test$HTML==\"TRUE\"), ]\n\n## Indeed corresponds to rows to remove\nArticle_total2$HTML &lt;- test_HTML\nArticle_total2 &lt;- Article_total2[!(Article_total2$HTML==\"TRUE\"), ]\n\nArticle_total2$HTML &lt;- NULL\n\nAction: It filters out any rows where test_HTML is TRUE, removing residual web code that wasn‚Äôt caught by the initial scraper.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "Weeks11.html#save-the-data",
    "href": "Weeks11.html#save-the-data",
    "title": "Week 11: Data preprocessing - Tidying/Cleaning data",
    "section": "3.3 Save the data",
    "text": "3.3 Save the data\nThe final section exports the cleaned dataset in two formats for future use.\n\nSave as an Excel file\nUses write.xlsx to save Article_total2 as a standard Excel spreadsheet.\n\n\nSave as an RData file\nUses save to store the dataframe as an R object, which preserves the data types (like factors vs strings) better than Excel for future R sessions.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 11: Data preprocessing - Tidying/Cleaning data"
    ]
  },
  {
    "objectID": "More2.html",
    "href": "More2.html",
    "title": "More 2: Data analysis with jiebaR and quanteda",
    "section": "",
    "text": "As mentioned in the tutorial to analyze data, dealing with Mandarin characters is quite tricky, especially when we need to cut the sentences into words. Again, people developed packages especially for that. The most popular one is the ‚ÄújiebaR‚Äù packages (link). However, it is not possible to install this package directly from R. So you will need to install the ‚Äúdevtools‚Äù package first, such that you can install the ‚ÄújiebaR‚Äù and ‚ÄújiebaRD‚Äù directly from the source (Github website).\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"qinwf/jiebaRD\")\ninstall_github(\"qinwf/jiebaR\")"
  },
  {
    "objectID": "More2.html#some-notes-when-dealing-with-mandarin-data",
    "href": "More2.html#some-notes-when-dealing-with-mandarin-data",
    "title": "More 2: Data analysis with jiebaR and quanteda",
    "section": "",
    "text": "As mentioned in the tutorial to analyze data, dealing with Mandarin characters is quite tricky, especially when we need to cut the sentences into words. Again, people developed packages especially for that. The most popular one is the ‚ÄújiebaR‚Äù packages (link). However, it is not possible to install this package directly from R. So you will need to install the ‚Äúdevtools‚Äù package first, such that you can install the ‚ÄújiebaR‚Äù and ‚ÄújiebaRD‚Äù directly from the source (Github website).\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"qinwf/jiebaRD\")\ninstall_github(\"qinwf/jiebaR\")"
  },
  {
    "objectID": "More2.html#explanation-of-the-markdown-script",
    "href": "More2.html#explanation-of-the-markdown-script",
    "title": "More 2: Data analysis with jiebaR and quanteda",
    "section": "0.2 Explanation of the Markdown script",
    "text": "0.2 Explanation of the Markdown script\n[Disclaimer: The explanation of the R codes is made by Gemini as an illustration of the use of such tools to decode a script.]\n\nPrepare the environment\nThis section loads the necessary packages for text analysis. Notably, it introduces quanteda, a powerful package for managing textual data, and jiebaR, which is essential for segmenting (cutting) Chinese text.\n\nLoad the libraries\n\nlibrary(quanteda)           # Main package for quantitative text analysis\nlibrary(quanteda.textstats) # Statistics for quanteda (frequencies, etc.)\nlibrary(jiebaR)             # Chinese text segmentation\nlibrary(tidytext)           # Tidy tools for text mining\nlibrary(dplyr)              # Data manipulation\nlibrary(openxlsx)           # Saving Excel files\n\n\n\nLoad the originally scraped data\nThis loads the .Rdata file containing the cleaned Article_total2 dataframe from the previous week‚Äôs cleaning process.\n\nload(file = \"ArticleETToday_CorpusCourse_CLEAN.Rdata\")\n\n\n\n\nKey Word In Context (KWIC)\nThis section performs a KWIC analysis on the character ‚ÄúÊúâ‚Äù you (to have). The goal is to identify what words appear immediately after ‚ÄúÊúâ‚Äù (you) and filter for cases where that following word is a verb.\n\nSet the segmenter (for Chinese)\nTwo different ‚Äúworkers‚Äù (segmentation engines) are initialized:\n\nseg_word: Cuts sentences into individual words.\nseg_POS: Cuts sentences and tags the Part-Of-Speech (POS) for each word.\n\n\nseg_word &lt;- worker(bylines = T, symbol=T)\nseg_POS &lt;- worker(type = \"tag\", symbol = F)\n\n\n\nPrepare the dataset for the analyses\nBefore analysis, the text must be tokenized. It creates a unique docname for every row. It uses segment() to cut the Chinese text in the body column. It converts the result into a tokens object (the format quanteda requires).\n\nArticle_total2$docname &lt;- paste0(\"text\", 1:nrow(Article_total2))\n\nArticle_tokens &lt;- Article_total2$body %&gt;%\n  segment(jiebar = seg_word) %&gt;%\n  as.tokens \n\n\n\nPerform the KWIC segmentation\n\nCorpus with POS information on the following word\nThis is a clever workaround to get specific POS tags for context words:\n\nKWIC Run 1: It looks for ‚ÄúÊúâ‚Äù with a window = 1. This isolates the single word immediately before and after.\nTagging: It takes the post column (the word after ‚ÄúÊúâ‚Äù) and runs the POS segmenter on it.\nCleaning: It converts the list of tags into a dataframe and cleans up the tag names (e.g., removing numbers).\nMerging: It joins this POS information back to the original KWIC data.\n\n\nkwic_data &lt;- kwic(Article_tokens,\n                  pattern = \"Êúâ\",\n                  window = 1)\n\nRightPost_Annot &lt;- segment(kwic_data$post, seg_POS)\n\n## Convert list to dataframe\nRightPost_Annot &lt;- do.call(rbind, lapply(RightPost_Annot, as.data.frame))\n\n## Add POS column and clean row names\nRightPost_Annot &lt;- cbind(POS = rownames(RightPost_Annot), RightPost_Annot)\nrownames(RightPost_Annot) &lt;- 1:nrow(RightPost_Annot)\nnames(RightPost_Annot)[2] &lt;- \"RightPost\"\nRightPost_Annot$POS &lt;- gsub(\"[0-9]+\", \"\", RightPost_Annot$POS)\n\n## Remove duplicates to ensure clean join\nRightPost_Annot &lt;- RightPost_Annot[!duplicated(RightPost_Annot), ]\nnames(RightPost_Annot)[2] &lt;- \"post\"\n\n## Join POS data back to KWIC data\nkwic_data &lt;- right_join(kwic_data, RightPost_Annot, by = \"post\")\n\n\n\nCorpus with longer sentences\nSince a window of 1 is too short to understand the meaning, a second KWIC analysis is run with a window = 15 to capture the full sentence context.\n\nkwic_data2 &lt;- kwic(Article_tokens,\n                  pattern = \"Êúâ\",\n                  window = 15)\n\n\n\nCombine the two datasets together\nThe script now merges the ‚ÄúPOS info‚Äù with the ‚ÄúSentence context‚Äù:\n\nIt creates a unique Index key (combining document name and position) to match the exact same instance of the word ‚ÄúÊúâ‚Äù across both datasets.\nIt uses right_join to merge them, ensuring we have both the grammatical category of the following word and the full sentence.\n\n\n### Prepare the dataset with longer sentences\nkwic_data2 &lt;- as.data.frame(kwic_data2)\n\nkwic_data2$Index &lt;- paste0(kwic_data2$docname,\n                           kwic_data2$from)\n\nkwic_data2_selected &lt;- kwic_data2 %&gt;% \n  select(docname, pre, post, Index)\n\n### Prepare the dataset with the POS infomation\nkwic_data &lt;- as.data.frame(kwic_data)\n\nnames(kwic_data)[6] &lt;- \"post_1word\"\n\nkwic_data_selected &lt;- kwic_data %&gt;% \n  select(docname, from, to, post_1word, keyword, POS)\n\nkwic_data_selected$Index &lt;- paste0(kwic_data_selected$docname,\n                                   kwic_data_selected$from)\n\n### Join the two datasets\nkwic_data &lt;- right_join(kwic_data_selected,\n                   kwic_data2_selected,\n                   by = \"Index\")\n\n### Reorder columns for readability\nkwic_data &lt;- kwic_data %&gt;% \n  relocate(keyword, .after = pre) %&gt;%\n  relocate(post_1word, .after = keyword)\n\n\n\nSelect the sentences we are interested in\nThis filters the data to keep only rows where the word following ‚ÄúÊúâ‚Äù is a Verb (POS == ‚Äúv‚Äù). It then creates a frequency table of these specific verbs.\n\nkwic_you_verb &lt;- kwic_data[kwic_data$POS == \"v\", ]\n\ntable_YouVerb &lt;- table(kwic_you_verb$post_1word)\ntable_YouVerb &lt;- as.data.frame(table_YouVerb)\nnames(table_YouVerb)[1] &lt;- \"Verb\"\n\n## Sort by frequency\ntable_YouVerb &lt;- table_YouVerb %&gt;% arrange(desc(Freq))\nhead(table_YouVerb, 10)\n\n\n\n\nSave the data\nSaves the specific ‚ÄúYou + Verb‚Äù dataset to Excel and RData formats.\n\nwrite.xlsx(kwic_you_verb, \"ArticleETToday_KWIC_You.xlsx\")\nsave(kwic_you_verb, file = \"ArticleETToday_KWIC_You.Rdata\")\n\n\n\n\nFrequency tables\nThis section calculates which words appear most often in the entire corpus, with specific cleaning steps.\n\nCreate the overall frequency table\nIt converts the tokens into a Document-Feature Matrix (DFM) and then calculates statistics.\n\nArticle_tokens_frequency &lt;- dfm(Article_tokens)\nArticle_tokens_frequency &lt;- textstat_frequency(Article_tokens_frequency)\nhead(Article_tokens_frequency, 100)\n\n\n\nClean it up a little bit\nThe raw frequency list includes punctuation and numbers. This code manually removes them using grep to exclude specific characters (comma, period, quotes, etc.) and digits.\n\n## Remove punctuation\ntable_FreqWord &lt;- Article_tokens_frequency[-grep(\"Ôºå\", Article_tokens_frequency$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„ÄÇ\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„ÄÅ\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„Äå\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„Äç\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"Ôºà\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"Ôºâ\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"Ôºü\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"Ôºõ\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"ÔºÅ\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„Ää\", table_FreqWord$feature),]\ntable_FreqWord &lt;- table_FreqWord[-grep(\"„Äã\", table_FreqWord$feature),]\n\n## Remove numbers\ntable_FreqWord &lt;- table_FreqWord[-grep(\"[[:digit:]]\", table_FreqWord$feature),]\n\n\n\nFinal table, addition of the percentage\nCalculates the relative frequency (percentage) of the top words.\n\ntable_FreqWord_Top100 &lt;- head(table_FreqWord, 100)\ntable_FreqWord_Top100$percentage &lt;- round(table_FreqWord_Top100$frequency/sum(table_FreqWord$frequency)*100, 4)\n\n\n\nSelect only the 100 most frequent nouns\nThe goal is to find the top nouns. However, POS tagging the entire corpus is computationally expensive.\nStrategy: Take the top 500 most frequent words first.\n\nTagging: POS tag only those 500 words.\nFiltering: Keep only those tagged as ‚Äún‚Äù (noun).\n\n\nSet segmenter and Annotate\nThis tags the list of frequent words.\n\nseg_POS_ByLines &lt;- worker(type = \"tag\", bylines = FALSE, symbol = F)\ntable_FreqWord_Top500 &lt;- head(table_FreqWord, 500)\n\nTop500_WordFreqPOS &lt;- segment(table_FreqWord_Top500$feature, seg_POS_ByLines)\n\n## Convert to dataframe\nTop500_WordFreqPOS_Annotated &lt;- do.call(rbind, \n                                        lapply(Top500_WordFreqPOS, \n                                               as.data.frame))\n\nTop500_WordFreqPOS_Annotated &lt;- cbind(POS = rownames(Top500_WordFreqPOS_Annotated),\n                                      Top500_WordFreqPOS_Annotated)\n\nrownames(Top500_WordFreqPOS_Annotated) &lt;- 1:nrow(Top500_WordFreqPOS_Annotated)\n\nnames(Top500_WordFreqPOS_Annotated)[2] &lt;- \"Word\"\n\nTop500_WordFreqPOS_Annotated$POS &lt;- gsub(\"[0-9]+\", \"\", Top500_WordFreqPOS_Annotated$POS)\n\n\n\nExtract the nouns\nFilters for POS == ‚Äún‚Äù, joins this back to the frequency table to ensure we have the counts, calculates percentages, and keeps the top 100.\n\nTopFreqNoun &lt;- Top500_WordFreqPOS_Annotated[Top500_WordFreqPOS_Annotated$POS == \"n\", ]\nTopFreqNoun$Index &lt;- \"TopNouns\"\n\n## Join back to original frequency table to get counts\nnames(table_FreqWord)[1] &lt;- \"Word\"\nTopFreqNoun &lt;- right_join(TopFreqNoun, table_FreqWord, by = \"Word\")\n\n## Filter and Sort\nTopFreqNoun &lt;- TopFreqNoun[+grep(\"TopNouns\", TopFreqNoun$Index),]\ntable_FreqNoun_Top100 &lt;- head(TopFreqNoun, 100) %&gt;% arrange(desc(frequency))\n\n\n\n\n\nSave the data\nSaves the top 100 nouns list to Excel and RData.\n\nwrite.xlsx(table_FreqNoun_Top100, \"ArticleETToday_Top100nouns.xlsx\")\nsave(table_FreqNoun_Top100, file = \"ArticleETToday_Top100nouns.Rdata\")"
  },
  {
    "objectID": "Week12.html#kwic-key-word-in-context",
    "href": "Week12.html#kwic-key-word-in-context",
    "title": "Week 12: Data analysis",
    "section": "1.1 KWIC (Key Word In Context)",
    "text": "1.1 KWIC (Key Word In Context)\nThe first type of analysis to introduce is called Key Word In Context, more often found under the acronym kwic. The idea is that we can better understand the meaning of a word based on how it is used in a sentence. KWIC analyses are therefore very suitable when we target a specific word or syntactic construction.\nKWIC analyses can be used for:\n\nSemantic analyses: Idea that the meaning of a specific word can be retrieved from its use in a sentences, based on the meaning of its neighbors.\nMorphosyntactic analyses: Idea that the morphosyntactic context where a word or a construction is used is helpful in understanding its specific morphological and syntactic features, as well as the meaning it conveys.\n\nHere is an example below, with the Mandarin word keai ‚Äòcute‚Äô (notice that these are fake data just for an illustration):\n\n\n\n\n\n\n\n\n\nSentence index\nBefore keyword\nKeyword\nAfter keyword\n\n\n\n\nSentence 1\nhen\n(very)\nkeai\n(cute)\nde mao\n(DE cat)\n\n\nSentence 2\nfeichang\n(very, extremely)\nkeai\n(cute)\nde gou\n(DE dog)\n\n\n\nBased on these data, and assuming that these are the most frequent instances that we found in the corpus, we can infer that:\n\nSemantically, the word keai is most often used to describe animals or pets, based on the following segment;\nMorphosyntactically, the word keai behaves as an adjective (or stative verb, depending on the analysis), based on the preceding segment.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Week12.html#word-lists-based-on-frequency",
    "href": "Week12.html#word-lists-based-on-frequency",
    "title": "Week 12: Data analysis",
    "section": "1.2 Word lists based on frequency",
    "text": "1.2 Word lists based on frequency\nAnother extremely common way to analyse corpus data is to count the frequency of each word, in order to have an idea of the most frequently used ones. It is easy to understand: we just need to count how many times a word occurred in the dataset we have. Hopefully, we obtain a list like the one below (again, these are fake data):\n\n\n\n\n\n\n\nWord\nFrequency\n\n\n\n\nkeai (cute)\n368\n\n\nkuaile (happy)\n354\n\n\n\nThe reality is that it is a little bit more complex that it seems to be, and we need to keep several remarks in mind:\n\nWithout any further data handling, it is more than likely that the most frequent words are (a) punctuation marks, and (b) grammatical markers (the so-called ‚Äòclosed-class‚Äô words), since they are limited and appear obligatory in each sentence. The bad news is that you need further steps to obtain the table you wish for. The good news is that you can use this piece of information as a sanitary check. If you compute the frequency tables and it is not the case that grammatical words are most frequent, then something bad happened!\nDefining what a ‚Äúword‚Äù is is not easy. In English, the simplest way is to say that words can be separated with a space (even if this too simple definition is misleading). In Mandarin, there are no spaces between words‚Ä¶ People created packages with dictionaries where words are listed such that we can still cut the sentences into words, but be aware that less common or newly created ones will not be detected! If your research question is really about new words, then you may consider adding them in the computer‚Äôs dictionary beforehand.\nFrequency tables can be further annotated, as you can add the rank of the word, the frequency in terms of percentage in addition to raw count numbers, etc.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Week12.html#combining-kwic-and-frequency-based-word-lists",
    "href": "Week12.html#combining-kwic-and-frequency-based-word-lists",
    "title": "Week 12: Data analysis",
    "section": "1.3 Combining KWIC and frequency-based word lists",
    "text": "1.3 Combining KWIC and frequency-based word lists\nEvery kind of analysis has pros and cons, and we cannot say that one is better than another. Again, there are just better suited ways to analyze your data according to your research question. This even means that you can combine two types of analyses to obtain more insights!\nFor example, you can first proceed with a KWIC analysis, and you obtain the table as above. Then, in a second step, you can create the frequency table of the first word following or preceding the keyword. So it is a ‚ÄúKWIC + word list‚Äù analysis!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Week12.html#the-quanteda-package-to-conduct-corpus-analyses",
    "href": "Week12.html#the-quanteda-package-to-conduct-corpus-analyses",
    "title": "Week 12: Data analysis",
    "section": "2.1 The quanteda package to conduct corpus analyses",
    "text": "2.1 The quanteda package to conduct corpus analyses\nWe are very lucky that many smart and generous people around the world created R packages especially to deal with corpus data, and such packages are still being updated at the moment I am writing this section.\nWe are going to use the package called ‚Äúquanteda‚Äù. You can find more information by clicking on this link.\n\nWe will also use another package developed by the same team, called ‚Äúquanteda.textstats‚Äù.\n\ninstall.packages(\"quanteda\")\ninstall.packages(\"quanteda.textstats\")\n\n\n\n\n\n\n\nNote\n\n\n\nThere exist several packages used to segment Mandarin sentences into words. Here, we will use the built-in functions of the ‚Äúquanteda‚Äù package. If you browse the Internet, you will notice that some people prefer using the ‚ÄújiebaR‚Äù package. The problem is that this package is not available on the CRAN anymore, and it can be quite tricky to install it on your computer. So for this week, we keep it simple!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Week12.html#workflow-for-the-analysis-of-corpus-data-in-r",
    "href": "Week12.html#workflow-for-the-analysis-of-corpus-data-in-r",
    "title": "Week 12: Data analysis",
    "section": "2.2 Workflow for the analysis of corpus data in R",
    "text": "2.2 Workflow for the analysis of corpus data in R\nHere is an overview of the workflow. First, we start with the clean corpus, and then we create a new dataset where the sentences are cut into words. Based on this new dataset, we can perform a KWIC analysis, create a word list, or combine the two types of analyses. Finally, we clean a little bit (as in the example below; punctuation marks, digits, selecting only the sentence/phrase, etc.), we add back the information from the original corpus, and we are done!",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Week12.html#explanation-of-the-r-script",
    "href": "Week12.html#explanation-of-the-r-script",
    "title": "Week 12: Data analysis",
    "section": "2.3 Explanation of the R script",
    "text": "2.3 Explanation of the R script\n\nPrepare the environment\n\nLoad the libraries\nFirst, we need to load the necessary packages for our analysis.\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textstats)\nlibrary(tidytext)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(openxlsx)\n#Sys.setlocale(category = \"LC_ALL\", locale = \"cht\")\n\n\nlibrary(quanteda): Loads the core package we are using for text analysis, which allows us to create tokens and document-feature matrices.\nlibrary(quanteda.textstats): A companion package to quanteda that provides statistical functions, such as calculating word frequencies.\nlibrary(tidytext): Useful for converting text data into ‚Äútidy‚Äù formats if we need to switch between quanteda and tidyverse workflows.\nlibrary(dplyr): Essential for data manipulation (like joining tables or filtering data).\nlibrary(stringr): Provides easy-to-use functions for string manipulation and Regular Expressions.\nlibrary(openxlsx): Used at the end of the script to export our results into Excel files.\nSys.setlocale(‚Ä¶): This line is commented out (#), but it is there in case you run into encoding issues on Windows. It sets the system locale to Traditional Chinese.\n\n\n\nLoad the originally scraped data R\n\nload(file = \"ArticleETToday_CorpusCourse_CLEAN.Rdata\")\n\nload(‚Ä¶): We load the .Rdata file containing the cleaned ETToday corpus we prepared in previous weeks. This brings the Article_total2 object into our environment.\n\n\n\nKey Word In Context (KWIC)\n\nPrepare the dataset for the analyses\nBefore we can analyze the text, we need to ensure our documents have IDs and are properly tokenized.\n\nArticle_total2$docname &lt;- paste0(\"text\", 1:nrow(Article_total2))\n\nArticle_tokens &lt;- tokens(Article_total2$body)\n\n\nArticle_total2$docname &lt;- ‚Ä¶: We create a new column called docname. We use paste0 to generate a unique ID for each article (e.g., ‚Äútext1‚Äù, ‚Äútext2‚Äù, etc.) based on the row number (nrow).\nArticle_tokens &lt;- tokens(‚Ä¶): We use the quanteda function tokens() to break the text in the body column into individual words (tokens). This creates a specialized tokens object required for the next steps.\n\n\n\nPerform the KWIC segmentation\n\nKWIC segmentation\nNow we search for a specific keyword to see how it is used in context.\n\nkwic_data &lt;- kwic(Article_tokens, pattern = \"Êúâ\", window = 30)\n\n\nkwic(‚Ä¶): This function (‚ÄúKey Word In Context‚Äù) searches our tokenized text.\n\npattern = ‚ÄúÊúâ‚Äù: We are searching for the character ‚ÄúÊúâ‚Äù (to have/there is).\nwindow = 30: We tell R to capture 30 tokens to the left (pre) and 30 tokens to the right (post) of our keyword.\n\n\n\n\nAnnotate the KWIC dataset\nThe kwic function gives us the context, but we lose the original metadata (like the article date or category). We need to put it back.\n\nkwic_data &lt;- as.data.frame(kwic_data)\n\nkwic_data &lt;- right_join(kwic_data, Article_total2, by = \"docname\")\n\nkwic_data &lt;- na.omit(kwic_data)\n\n\nas.data.frame(kwic_data): The output of kwic is a special object; we convert it into a standard data frame so we can manipulate it easily.\nright_join(‚Ä¶): We merge our KWIC results with the original Article_total2 dataframe. We match them using the docname column we created earlier.\nna.omit(kwic_data): We remove any rows that have missing values (NAs) to ensure our dataset is clean for analysis.\n\n2.2.3 (Optional) Clean the context to keep only the phrase where the keyword is found\nhe window of 30 words might include parts of previous or subsequent sentences. We want to ‚Äútrim‚Äù the context to only the sentence containing the keyword.\n\n## Keep original information just in case\nkwic_data$pre_original &lt;- kwic_data$pre\nkwic_data$post_original &lt;- kwic_data$post\n\n## Post context\nsymbol1 &lt;- \"\\\\„ÄÇ\" \nkwic_data$post &lt;- sub(paste0(\"(\", symbol1, \").*\"), \"\\\\1\", kwic_data$post)\n\nsymbol2 &lt;- \"\\\\Ôºå\" \nkwic_data$post &lt;- sub(paste0(\"(\", symbol2, \").*\"), \"\\\\1\", kwic_data$post)\n\nsymbol3 &lt;- \"\\\\Ôºü\" \nkwic_data$post &lt;- sub(paste0(\"(\", symbol3, \").*\"), \"\\\\1\", kwic_data$post)\n\nsymbol4 &lt;- \"\\\\ÔºÅ\" \nkwic_data$post &lt;- sub(paste0(\"(\", symbol4, \").*\"), \"\\\\1\", kwic_data$post)\n\n## Pre context\nkwic_data$pre &lt;- sub(\".*„ÄÇ([^*„ÄÇ]*)$\", \"„ÄÇ\\\\1\", kwic_data$pre)\nkwic_data$pre &lt;- sub(\".*Ôºå([^*Ôºå]*)$\", \"Ôºå\\\\1\", kwic_data$pre)\nkwic_data$pre &lt;- sub(\".*Ôºü([^*Ôºü]*)$\", \"Ôºü\\\\1\", kwic_data$pre)\nkwic_data$pre &lt;- sub(\".*ÔºÅ([^*ÔºÅ]*)$\", \"ÔºÅ\\\\1\", kwic_data$pre)\n\n\nkwic_data$pre_original &lt;- ‚Ä¶: We back up the original context columns before modifying them.\nsymbol1 &lt;- ‚Äú\\„ÄÇ‚Äù: We define the punctuation mark we want to stop at (the Chinese period). The double backslash escapes the character for Regex.\nsub(paste0(‚Äú(‚Äù, symbol1, ‚Äú).*‚Äù), ‚Äú\\1‚Äù, ‚Ä¶): This Regular Expression looks for the first period in the post context and deletes everything after it. It effectively cuts the text off at the end of the sentence.\nsub(‚Äú.„ÄÇ([^*„ÄÇ])$‚Äù, ‚Äú„ÄÇ\\1‚Äù, ‚Ä¶): This mirrors the operation for the pre context. It looks for the last period occurring before our keyword and deletes everything before it, so the context starts at the beginning of the current sentence.\n\nNote: The code repeats this process for commas (Ôºå), question marks (Ôºü), and exclamation marks (ÔºÅ) to handle different sentence boundaries.\n\n## Have a look at the data (I delete some columns so that it is easier to display on the website)\nkwic_data_for_website &lt;- kwic_data\nkwic_data_for_website$original_article &lt;- NULL\nkwic_data_for_website$body &lt;- NULL\nkwic_data_for_website$pre_original &lt;- NULL\nkwic_data_for_website$post_original &lt;- NULL\nknitr::kable(head(kwic_data_for_website))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\ntime\nclass\ntitle\nurl\nyear\nmonth\nday\n\n\n\n\ntext1\n19\n19\nÔºå Á´üÁÑ∂\nÊúâ\nÁé©ÂÆ∂ Á™ÅÁôº Â•áÊÉ≥ Â∞á ÂÖ© Ê¨æ ÈÅäÊà≤ Â∞¨ Âú®‰∏ÄËµ∑ Ôºå\nÊúâ\n2024Âπ¥01Êúà01Êó• 10:57\nÊîøÊ≤ª\nÁ•û‰∫∫Êää„ÄäÊàëÁöÑ‰∏ñÁïå„ÄãÊîπÊàê„ÄäË°ÄÊ∫êË©õÂíí„Äã„ÄÄÈÇÑÂéüÂ∫¶Ë∂ÖÈ´òÁé©ÂÆ∂ÁãÇÊï≤Á¢óÔºöÂø´ÈªûÂá∫\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\ntext6\n55\n55\nÔºå ‰ΩÜ Â∑≤Á∂ì\nÊúâ\n‰∏çÂ∞ë Áé©ÂÆ∂ Ë°®Á§∫ Áõ∏Áï∂ ÊúüÂæÖ Ôºå\nÊúâ\n2024Âπ¥01Êúà01Êó• 10:57\nÊîøÊ≤ª\nÁ•û‰∫∫Êää„ÄäÊàëÁöÑ‰∏ñÁïå„ÄãÊîπÊàê„ÄäË°ÄÊ∫êË©õÂíí„Äã„ÄÄÈÇÑÂéüÂ∫¶Ë∂ÖÈ´òÁé©ÂÆ∂ÁãÇÊï≤Á¢óÔºöÂø´ÈªûÂá∫\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\ntext12\n18\n18\nÔºå Â∏∏Ë¶ã Ë£úÂìÅ\nÊúâ\nÁáíÈÖíÈõû „ÄÅ Ëñë ÊØçÈ¥® „ÄÅ ÁæäËÇâÁàê „ÄÅ Ëó• Ááâ ÊéíÈ™® Á≠â Ôºå\nÊúâ\n2024Âπ¥01Êúà01Êó• 09:07\nÁ§æÊúÉ\nË∑®Âπ¥ÂÜ¨‰ª§ÈÄ≤Ë£úÁàêÁÅ´ÈúÄÁïôÊÑè„ÄÄÂÆâË£ù‰ΩèÂÆÖÁî®ÁÅ´ÁÅΩË≠¶Â†±Âô®ÈÅøÂÖçÊÇ≤Âäá\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\ntext14\n57\n57\nÔºå ÈÅ∏Êìá\nÊúâ\nÁÜÑÁÅ´ ÂÆâÂÖ® Ë£ùÁΩÆ Âèä Ê∫´Â∫¶ ÊÑüÁü• ÂäüËÉΩ Áàê ÂÖ∑ Ôºå\nÊúâ\n2024Âπ¥01Êúà01Êó• 09:07\nÁ§æÊúÉ\nË∑®Âπ¥ÂÜ¨‰ª§ÈÄ≤Ë£úÁàêÁÅ´ÈúÄÁïôÊÑè„ÄÄÂÆâË£ù‰ΩèÂÆÖÁî®ÁÅ´ÁÅΩË≠¶Â†±Âô®ÈÅøÂÖçÊÇ≤Âäá\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\ntext17\n30\n30\nÔºå ÁôºÁèæ 29 Ê≠≤ Êûó Âßì Áî∑Â≠ê Ê∂â\nÊúâ\nÈáç Â´å Ôºå\nÊúâ\n2024Âπ¥01Êúà01Êó• 10:29\nÁ§æÊúÉ\nÂçäÂ∑•ÂçäËÆÄË≤∑ÁöÑÊ©üËªäË¢´ÂÅ∑ÔºÅ23Ê≠≤Â•≥‰∫∫ÁîüÁ¨¨‰∏ÄËºõ„ÄÄË≠¶Âüã‰ºè10hrsÊäìË≥ä\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\ntext19\n46\n46\nÔºå Á∂≤Âèã Âßã Áü• Âè∞ÁÅ£ Êîπ Ëªä Áïå\nÊúâ\nÈÄô Ëôü ‰∫∫Áâ© Â≠òÂú® „ÄÇ\nÊúâ\n2024Âπ¥01Êúà01Êó• 13:00\nÁ§æÊúÉ\nÊè≠ÂØÜÂªñËÄÅÂ§ßÊâìÈæúËôüÈÄ≤ÂåñÂè≤ÔºÅ‰ªñÊ£ÑÂè∞Á©çÈõªÂ∑•Â∏´„ÄÄÂÖ©Â≤∏ÊîπË£ùÈÅî‰∫∫‰πãÁà≠Êõù\nhttps://www.ettoday.net/news/20231231/2652951.htm\n2024\n01\n01\n\n\n\n\n\n\n\nCombined analysis: Frequency table of the first word following you ‚Äòto have‚Äô\nWe can now analyze what words typically follow ‚ÄúÊúâ‚Äù.\n\n## Extract the first word\nkwic_data$post_first_word &lt;- word(kwic_data$post, 1)\n\n## We need to tranform the tokenized data into a 'dfm' dataset\nkwic_data_freq &lt;- dfm( tokens(kwic_data$post_first_word, remove_punct = TRUE) )\n\nkwic_data_freq &lt;- textstat_frequency(kwic_data_freq)\n\n## Clean a little bit\nkwic_data_freq &lt;- kwic_data_freq[-grep(\"[[:digit:]]\", kwic_data_freq$feature),]\n\n## Recreate the rank\nkwic_data_freq$rank &lt;- 1:length(kwic_data_freq$rank)\n\nknitr::kable(head(kwic_data_freq, 100))\n\n\n\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\n1\nÊ∞ëÁúæ\n1291\n1\n1291\nall\n\n\n3\nÂ§ö\n694\n2\n694\nall\n\n\n4\nÁ∂≤Âèã\n656\n3\n656\nall\n\n\n5\nÂæàÂ§ö\n608\n4\n608\nall\n\n\n6\nÁï∞\n600\n5\n600\nall\n\n\n7\nÂïèÈ°å\n588\n6\n588\nall\n\n\n8\n‰ªÄÈ∫º\n580\n7\n580\nall\n\n\n10\nÂ™íÈ´î\n550\n8\n550\nall\n\n\n11\nÁöÑ\n533\n9\n533\nall\n\n\n12\n‰∏ÄÂêç\n503\n10\n503\nall\n\n\n14\n‰ªª‰Ωï\n476\n11\n476\nall\n\n\n15\nÈÄÉ‰∫°\n445\n12\n445\nall\n\n\n16\nÂèØËÉΩ\n437\n13\n437\nall\n\n\n17\nË®±Â§ö\n399\n14\n399\nall\n\n\n18\nÊ©üÊúÉ\n395\n15\n395\nall\n\n\n19\n‰∏ÄÂÄã\n338\n16\n338\nall\n\n\n21\n‰ø°ÂøÉ\n298\n17\n298\nall\n\n\n22\nÂÖ∂‰ªñ\n298\n18\n298\nall\n\n\n23\nÂú®\n271\n19\n271\nall\n\n\n24\nÊõ¥Â§ö\n264\n20\n264\nall\n\n\n25\n‰∏Ä‰∫õ\n262\n21\n262\nall\n\n\n26\nÂãæ\n256\n22\n256\nall\n\n\n27\nÁõ∏Áï∂\n249\n23\n249\nall\n\n\n28\n‰∏çÂ∞ë\n234\n24\n234\nall\n\n\n29\nÈúÄË¶Å\n228\n25\n228\nall\n\n\n30\nÈáç\n223\n26\n223\nall\n\n\n31\nÈÄôÊ®£\n222\n27\n222\nall\n\n\n32\nË∑ü\n222\n28\n222\nall\n\n\n33\nÂøÖË¶Å\n220\n29\n220\nall\n\n\n35\nÊôÇ\n207\n30\n207\nall\n\n\n36\n‰∏çÂêå\n200\n31\n200\nall\n\n\n37\nÊØíÂìÅ\n198\n32\n198\nall\n\n\n38\nËÉΩÂäõ\n196\n33\n196\nall\n\n\n39\nÂ§öÂ∞ë\n194\n34\n194\nall\n\n\n40\nÈÅï\n191\n35\n191\nall\n\n\n43\nÈÄôÈ∫º\n181\n36\n181\nall\n\n\n44\nÈùûÂ∏∏\n181\n37\n181\nall\n\n\n45\nÈÉ®ÂàÜ\n177\n38\n177\nall\n\n\n46\nÁæàÊäº\n176\n39\n176\nall\n\n\n48\nÁõ∏Èóú\n172\n40\n172\nall\n\n\n49\n‰∏ÄÂÆö\n169\n41\n169\nall\n\n\n50\nÊòéÈ°Ø\n162\n42\n162\nall\n\n\n51\nÂÖ±Ë≠ò\n156\n43\n156\nall\n\n\n53\n‰∏≤\n153\n44\n153\nall\n\n\n54\nË∂ÖÈÅé\n152\n45\n152\nall\n\n\n55\nË¢´\n151\n46\n151\nall\n\n\n56\nË≤¨‰ªª\n149\n47\n149\nall\n\n\n57\nÈáçÂ§ß\n147\n48\n147\nall\n\n\n58\nÂæàÂ§ß\n147\n49\n147\nall\n\n\n59\nÈÅïÂèç\n140\n50\n140\nall\n\n\n60\nÊõ¥\n139\n51\n139\nall\n\n\n61\nÈÖí\n135\n52\n135\nall\n\n\n62\nÁúãÂà∞\n135\n53\n135\nall\n\n\n63\nÁñëÊÖÆ\n134\n54\n134\nall\n\n\n64\nÊÑèÈ°ò\n132\n55\n132\nall\n\n\n65\nÊÑèË¶ã\n130\n56\n130\nall\n\n\n66\nÂ∞ç\n128\n57\n128\nall\n\n\n67\nËààË∂£\n126\n58\n126\nall\n\n\n68\nÁà≠Ë≠∞\n124\n59\n124\nall\n\n\n69\nËá™Â∑±\n124\n60\n124\nall\n\n\n70\nÁôºÁîü\n122\n61\n122\nall\n\n\n71\nÊì¶\n120\n62\n120\nall\n\n\n72\nÂ§ßÈáè\n115\n63\n115\nall\n\n\n73\nÁ≥æÁ¥õ\n111\n64\n111\nall\n\n\n74\nÂπ´Âä©\n111\n65\n111\nall\n\n\n75\n‰∏Ä‰Ωç\n111\n66\n111\nall\n\n\n76\nÈÅé\n110\n67\n110\nall\n\n\n77\nÂêë\n108\n68\n108\nall\n\n\n78\nÁñèÂ§±\n106\n69\n106\nall\n\n\n79\nÊ¢ù‰ª∂\n106\n70\n106\nall\n\n\n80\nÂÇµÂãô\n106\n71\n106\nall\n\n\n81\n‰∫ãÂØ¶\n104\n72\n104\nall\n\n\n82\n‰∏ç\n103\n73\n103\nall\n\n\n83\nÂÖ©ÂÄã\n102\n74\n102\nall\n\n\n84\nÈ´òÂ∫¶\n101\n75\n101\nall\n\n\n85\nÁï∞ÁãÄ\n100\n76\n100\nall\n\n\n86\nÈÅïÊ≥ï\n99\n77\n99\nall\n\n\n87\nÊìö\n96\n78\n96\nall\n\n\n88\nÂ§öÊ¨°\n94\n79\n94\nall\n\n\n89\nÊîøÊ≤ª\n93\n80\n93\nall\n\n\n90\nÊÄß\n93\n81\n93\nall\n\n\n92\nÂÖ©\n91\n82\n91\nall\n\n\n94\nÂÅö\n90\n83\n90\nall\n\n\n95\nÈ´òÈÅî\n89\n84\n89\nall\n\n\n96\nÁî∑Â≠ê\n89\n85\n89\nall\n\n\n97\nË´∏Â§ö\n88\n86\n88\nall\n\n\n98\nÁï∞Â∏∏\n86\n87\n86\nall\n\n\n99\nÁëïÁñµ\n86\n88\n86\nall\n\n\n100\nÂ§ß\n86\n89\n86\nall\n\n\n101\nÈÅéÂ§±\n86\n90\n86\nall\n\n\n102\nÂì™‰∫õ\n85\n91\n85\nall\n\n\n103\nÂà•\n85\n92\n85\nall\n\n\n104\nÂπæÂÄã\n84\n93\n84\nall\n\n\n105\nÂèØËÉΩÊòØ\n84\n94\n84\nall\n\n\n106\nËàá\n83\n95\n83\nall\n\n\n107\nËªäËºõ\n82\n96\n82\nall\n\n\n109\nÂéª\n81\n97\n81\nall\n\n\n110\n‰∏ÄËºõ\n81\n98\n81\nall\n\n\n111\nÈÄôÁ®Æ\n80\n99\n80\nall\n\n\n112\nËóç\n78\n100\n78\nall\n\n\n\n\n\n\nword(kwic_data$post, 1): Uses stringr to extract specifically the first word from the post (context after) column.\ntokens(‚Ä¶): We tokenize this list of ‚Äúfirst words‚Äù.\ndfm(‚Ä¶): We convert those tokens into a Document-Feature Matrix.\ntextstat_frequency(‚Ä¶): We calculate how often each word appears.\ngrep(‚Äú[[:digit:]]‚Äù, ‚Ä¶): We use grep to find any words that contain numbers (digits) and remove them (using the minus sign -) to clean up our results.\n1:length(‚Ä¶): Since we removed some rows, we reset the rank column so it goes from 1 to N sequentially.\n\n\n\n\nSave the data\nFinally, we save our hard work.\n\nwrite.xlsx(kwic_data, \"ArticleETToday_KWIC_You.xlsx\")\nsave(kwic_data, file = \"ArticleETToday_KWIC_You.Rdata\")\n\n\nwrite.xlsx: Exports the dataframe to an Excel file for manual inspection.\nsave: Saves the R object to an .Rdata file so we can load it quickly in future R sessions.\n\n\n\n\nFrequency tables\n\nCreate the overall frequency table\n\nCreation of the first table\nNow, let‚Äôs look at the frequency of words across the entire corpus, not just around a keyword.\n\nArticle_tokens_frequency &lt;- dfm(\n  tokens(Article_total2$body,\n         remove_punct = TRUE)\n  )\nArticle_tokens_frequency &lt;- textstat_frequency(Article_tokens_frequency)\n\ntable_AllWordsFreq_Top100 &lt;- head(Article_tokens_frequency, 100) \nknitr::kable(table_AllWordsFreq_Top100)\n\n\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nÁöÑ\n357709\n1\n159298\nall\n\n\nÂú®\n116544\n2\n88581\nall\n\n\nÊó•\n84106\n3\n66187\nall\n\n\nÂæå\n71392\n4\n57501\nall\n\n\n‰∫∫\n69084\n5\n49761\nall\n\n\nÊôÇ\n67573\n6\n53558\nall\n\n\nÊúâ\n65657\n7\n53298\nall\n\n\nÁî∑\n63140\n8\n32390\nall\n\n\nËàá\n61733\n9\n49197\nall\n\n\nÊòØ\n59999\n10\n48402\nall\n\n\n‰πü\n59965\n11\n51054\nall\n\n\nÂèä\n49873\n12\n38094\nall\n\n\nË°®Á§∫\n49840\n13\n47384\nall\n\n\nÂπ¥\n48807\n14\n33965\nall\n\n\n‰ΩÜ\n47947\n15\n43097\nall\n\n\n‰ªñ\n47910\n16\n34208\nall\n\n\nÂ∞á\n45675\n17\n39070\nall\n\n\nÁ≠â\n44905\n18\n36914\nall\n\n\nË¢´\n44599\n19\n36588\nall\n\n\nÂà∞\n44505\n20\n38318\nall\n\n\n2\n43707\n21\n34342\nall\n\n\nÊúà\n43394\n22\n32669\nall\n\n\nÂßì\n42361\n23\n28582\nall\n\n\nÂ∞ç\n41664\n24\n35874\nall\n\n\nÈô≥\n41052\n25\n26393\nall\n\n\nÊ∞ëÁúæ\n40981\n26\n29991\nall\n\n\nË¶Å\n39541\n27\n31461\nall\n\n\n‰∏¶\n38658\n28\n34703\nall\n\n\nÂè∞ÁÅ£\n38028\n29\n24641\nall\n\n\nÊ°à\n36770\n30\n29132\nall\n\n\n1\n36107\n31\n28584\nall\n\n\nÁÇ∫\n35360\n32\n30056\nall\n\n\nË≠¶Êñπ\n35205\n33\n26808\nall\n\n\n‰∏≠\n34824\n34\n30380\nall\n\n\n3\n34319\n35\n28447\nall\n\n\nË™™\n33596\n36\n29140\nall\n\n\nÊüØ\n33164\n37\n18553\nall\n\n\nÊñá\n32255\n38\n21397\nall\n\n\n‰ª•\n32162\n39\n28049\nall\n\n\nÈªÉ\n31311\n40\n20937\nall\n\n\n‰∏ä\n31281\n41\n27774\nall\n\n\n‰∏ç\n30456\n42\n26271\nall\n\n\nÊ∞ëÈÄ≤Èª®\n29648\n43\n20912\nall\n\n\nÂ∞±\n29633\n44\n25843\nall\n\n\nÊûó\n29426\n45\n19296\nall\n\n\n‰πã\n29209\n46\n18968\nall\n\n\nÂ•≥\n29041\n47\n16593\nall\n\n\nËªä\n28849\n48\n18869\nall\n\n\nÊ≠≤\n27927\n49\n19862\nall\n\n\nËÆì\n27838\n50\n24172\nall\n\n\nÂêç\n27598\n51\n22325\nall\n\n\nÊñº\n27440\n52\n23486\nall\n\n\nËÄå\n27073\n53\n25039\nall\n\n\nÂ•π\n26964\n54\n17524\nall\n\n\nÊúÉ\n26866\n55\n22316\nall\n\n\nÁôºÁîü\n26700\n56\n23129\nall\n\n\n‰∫Ü\n25947\n57\n21844\nall\n\n\nÂâç\n25547\n58\n22155\nall\n\n\nÂ§ß\n25397\n59\n21497\nall\n\n\nÂúãÊ∞ëÈª®\n25126\n60\n17804\nall\n\n\nÊ™¢\n24367\n61\n18605\nall\n\n\nÁôºÁèæ\n23872\n62\n20970\nall\n\n\nË≥¥\n23781\n63\n16125\nall\n\n\nÁ´ãÂßî\n23697\n64\n17483\nall\n\n\nÂõ†\n23330\n65\n21330\nall\n\n\nÂ∑≤\n23243\n66\n21044\nall\n\n\nË™øÊü•\n23167\n67\n19220\nall\n\n\nÈÉΩ\n23092\n68\n20346\nall\n\n\nÊåáÂá∫\n23045\n69\n22728\nall\n\n\n4\n22781\n70\n19855\nall\n\n\nÂì≤\n22453\n71\n14297\nall\n\n\nËá™Â∑±\n22438\n72\n19022\nall\n\n\nÊ≤íÊúâ\n21977\n73\n19189\nall\n\n\nÂíå\n21507\n74\n17793\nall\n\n\n5\n21402\n75\n18870\nall\n\n\nË∑ü\n21213\n76\n17565\nall\n\n\nÁî∑Â≠ê\n20713\n77\n15449\nall\n\n\nÊàë\n20494\n78\n13075\nall\n\n\n‰æù\n20250\n79\n18416\nall\n\n\nÂ§ö\n20149\n80\n17965\nall\n\n\nÈÄô\n20140\n81\n18193\nall\n\n\nÈª®\n19999\n82\n13527\nall\n\n\nËê¨ÂÖÉ\n19934\n83\n14724\nall\n\n\nÈÅ≠\n19778\n84\n17403\nall\n\n\nÂàÜ\n19477\n85\n16186\nall\n\n\nÂ∞è\n19266\n86\n12596\nall\n\n\n‰ªä\n19204\n87\n18762\nall\n\n\nÂæ∑\n19140\n88\n14233\nall\n\n\nË©≤\n19098\n89\n16180\nall\n\n\nÊùé\n18916\n90\n12213\nall\n\n\n‰∫∫Âì°\n18698\n91\n14908\nall\n\n\nÂêë\n18625\n92\n17039\nall\n\n\nË®±\n18586\n93\n15277\nall\n\n\n10\n18579\n94\n16677\nall\n\n\nÈï∑\n18579\n94\n15239\nall\n\n\nÊàñ\n18303\n96\n14481\nall\n\n\n6\n18281\n97\n15998\nall\n\n\nÊîøÂ∫ú\n18281\n97\n14640\nall\n\n\nÊ∏Ö\n17969\n99\n13651\nall\n\n\nÁõ∏Èóú\n17961\n100\n16032\nall\n\n\n\n\n\n\ntable_AllWordsFreq_Top100 tokens(Article_total2$body, ‚Ä¶): We tokenize the full body text of all articles, removing punctuation.\ndfm(‚Ä¶): We turn that huge list of tokens into a Document-Feature Matrix.\ntextstat_frequency(‚Ä¶): We calculate the frequency of every unique word in the corpus.\nhead(‚Ä¶, 100): We create a smaller table containing only the top 100 most frequent words.\n\n\n\nClean it up a little bit\nWe often find ‚Äúnoise‚Äù in the data, like numbers, which we want to filter out.\n\n## Example with numbers\ntable_FreqWord &lt;- Article_tokens_frequency[-grep(\"[[:digit:]]\", Article_tokens_frequency$feature),]\n\n## Redo the ranking\ntable_FreqWord$rank &lt;- 1:length(table_FreqWord$rank)\n\n\ngrep(‚Äú[[:digit:]]‚Äù, ‚Ä¶): Similar to before, we search for any features (words) containing numbers and remove them from the list.\n1:length(‚Ä¶): We re-calculate the rank column to fill in the gaps left by the removed words.\n\n\n\nFinal table, addition of the percentage\nFrequencies are good, but percentages help us understand the relative importance of a word.\n\ntable_FreqWord_Top100 &lt;- head(table_FreqWord, 100)\n\ntable_FreqWord_Top100$percentage &lt;- round(table_FreqWord_Top100$frequency/sum(table_FreqWord$frequency)*100, 5)\nknitr::kable(table_FreqWord_Top100)\n\n\n\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\npercentage\n\n\n\n\n1\nÁöÑ\n357709\n1\n159298\nall\n2.37874\n\n\n2\nÂú®\n116544\n2\n88581\nall\n0.77501\n\n\n3\nÊó•\n84106\n3\n66187\nall\n0.55930\n\n\n4\nÂæå\n71392\n4\n57501\nall\n0.47475\n\n\n5\n‰∫∫\n69084\n5\n49761\nall\n0.45940\n\n\n6\nÊôÇ\n67573\n6\n53558\nall\n0.44936\n\n\n7\nÊúâ\n65657\n7\n53298\nall\n0.43661\n\n\n8\nÁî∑\n63140\n8\n32390\nall\n0.41988\n\n\n9\nËàá\n61733\n9\n49197\nall\n0.41052\n\n\n10\nÊòØ\n59999\n10\n48402\nall\n0.39899\n\n\n11\n‰πü\n59965\n11\n51054\nall\n0.39876\n\n\n12\nÂèä\n49873\n12\n38094\nall\n0.33165\n\n\n13\nË°®Á§∫\n49840\n13\n47384\nall\n0.33143\n\n\n14\nÂπ¥\n48807\n14\n33965\nall\n0.32456\n\n\n15\n‰ΩÜ\n47947\n15\n43097\nall\n0.31884\n\n\n16\n‰ªñ\n47910\n16\n34208\nall\n0.31860\n\n\n17\nÂ∞á\n45675\n17\n39070\nall\n0.30374\n\n\n18\nÁ≠â\n44905\n18\n36914\nall\n0.29862\n\n\n19\nË¢´\n44599\n19\n36588\nall\n0.29658\n\n\n20\nÂà∞\n44505\n20\n38318\nall\n0.29596\n\n\n22\nÊúà\n43394\n21\n32669\nall\n0.28857\n\n\n23\nÂßì\n42361\n22\n28582\nall\n0.28170\n\n\n24\nÂ∞ç\n41664\n23\n35874\nall\n0.27706\n\n\n25\nÈô≥\n41052\n24\n26393\nall\n0.27299\n\n\n26\nÊ∞ëÁúæ\n40981\n25\n29991\nall\n0.27252\n\n\n27\nË¶Å\n39541\n26\n31461\nall\n0.26295\n\n\n28\n‰∏¶\n38658\n27\n34703\nall\n0.25707\n\n\n29\nÂè∞ÁÅ£\n38028\n28\n24641\nall\n0.25288\n\n\n30\nÊ°à\n36770\n29\n29132\nall\n0.24452\n\n\n32\nÁÇ∫\n35360\n30\n30056\nall\n0.23514\n\n\n33\nË≠¶Êñπ\n35205\n31\n26808\nall\n0.23411\n\n\n34\n‰∏≠\n34824\n32\n30380\nall\n0.23158\n\n\n36\nË™™\n33596\n33\n29140\nall\n0.22341\n\n\n37\nÊüØ\n33164\n34\n18553\nall\n0.22054\n\n\n38\nÊñá\n32255\n35\n21397\nall\n0.21449\n\n\n39\n‰ª•\n32162\n36\n28049\nall\n0.21388\n\n\n40\nÈªÉ\n31311\n37\n20937\nall\n0.20822\n\n\n41\n‰∏ä\n31281\n38\n27774\nall\n0.20802\n\n\n42\n‰∏ç\n30456\n39\n26271\nall\n0.20253\n\n\n43\nÊ∞ëÈÄ≤Èª®\n29648\n40\n20912\nall\n0.19716\n\n\n44\nÂ∞±\n29633\n41\n25843\nall\n0.19706\n\n\n45\nÊûó\n29426\n42\n19296\nall\n0.19568\n\n\n46\n‰πã\n29209\n43\n18968\nall\n0.19424\n\n\n47\nÂ•≥\n29041\n44\n16593\nall\n0.19312\n\n\n48\nËªä\n28849\n45\n18869\nall\n0.19184\n\n\n49\nÊ≠≤\n27927\n46\n19862\nall\n0.18571\n\n\n50\nËÆì\n27838\n47\n24172\nall\n0.18512\n\n\n51\nÂêç\n27598\n48\n22325\nall\n0.18352\n\n\n52\nÊñº\n27440\n49\n23486\nall\n0.18247\n\n\n53\nËÄå\n27073\n50\n25039\nall\n0.18003\n\n\n54\nÂ•π\n26964\n51\n17524\nall\n0.17931\n\n\n55\nÊúÉ\n26866\n52\n22316\nall\n0.17866\n\n\n56\nÁôºÁîü\n26700\n53\n23129\nall\n0.17755\n\n\n57\n‰∫Ü\n25947\n54\n21844\nall\n0.17255\n\n\n58\nÂâç\n25547\n55\n22155\nall\n0.16989\n\n\n59\nÂ§ß\n25397\n56\n21497\nall\n0.16889\n\n\n60\nÂúãÊ∞ëÈª®\n25126\n57\n17804\nall\n0.16709\n\n\n61\nÊ™¢\n24367\n58\n18605\nall\n0.16204\n\n\n62\nÁôºÁèæ\n23872\n59\n20970\nall\n0.15875\n\n\n63\nË≥¥\n23781\n60\n16125\nall\n0.15814\n\n\n64\nÁ´ãÂßî\n23697\n61\n17483\nall\n0.15758\n\n\n65\nÂõ†\n23330\n62\n21330\nall\n0.15514\n\n\n66\nÂ∑≤\n23243\n63\n21044\nall\n0.15456\n\n\n67\nË™øÊü•\n23167\n64\n19220\nall\n0.15406\n\n\n68\nÈÉΩ\n23092\n65\n20346\nall\n0.15356\n\n\n69\nÊåáÂá∫\n23045\n66\n22728\nall\n0.15325\n\n\n71\nÂì≤\n22453\n67\n14297\nall\n0.14931\n\n\n72\nËá™Â∑±\n22438\n68\n19022\nall\n0.14921\n\n\n73\nÊ≤íÊúâ\n21977\n69\n19189\nall\n0.14615\n\n\n74\nÂíå\n21507\n70\n17793\nall\n0.14302\n\n\n76\nË∑ü\n21213\n71\n17565\nall\n0.14107\n\n\n77\nÁî∑Â≠ê\n20713\n72\n15449\nall\n0.13774\n\n\n78\nÊàë\n20494\n73\n13075\nall\n0.13628\n\n\n79\n‰æù\n20250\n74\n18416\nall\n0.13466\n\n\n80\nÂ§ö\n20149\n75\n17965\nall\n0.13399\n\n\n81\nÈÄô\n20140\n76\n18193\nall\n0.13393\n\n\n82\nÈª®\n19999\n77\n13527\nall\n0.13299\n\n\n83\nËê¨ÂÖÉ\n19934\n78\n14724\nall\n0.13256\n\n\n84\nÈÅ≠\n19778\n79\n17403\nall\n0.13152\n\n\n85\nÂàÜ\n19477\n80\n16186\nall\n0.12952\n\n\n86\nÂ∞è\n19266\n81\n12596\nall\n0.12812\n\n\n87\n‰ªä\n19204\n82\n18762\nall\n0.12771\n\n\n88\nÂæ∑\n19140\n83\n14233\nall\n0.12728\n\n\n89\nË©≤\n19098\n84\n16180\nall\n0.12700\n\n\n90\nÊùé\n18916\n85\n12213\nall\n0.12579\n\n\n91\n‰∫∫Âì°\n18698\n86\n14908\nall\n0.12434\n\n\n92\nÂêë\n18625\n87\n17039\nall\n0.12386\n\n\n93\nË®±\n18586\n88\n15277\nall\n0.12360\n\n\n95\nÈï∑\n18579\n89\n15239\nall\n0.12355\n\n\n96\nÊàñ\n18303\n90\n14481\nall\n0.12171\n\n\n98\nÊîøÂ∫ú\n18281\n91\n14640\nall\n0.12157\n\n\n99\nÊ∏Ö\n17969\n92\n13651\nall\n0.11949\n\n\n100\nÁõ∏Èóú\n17961\n93\n16032\nall\n0.11944\n\n\n101\nÂúã\n17914\n94\n13623\nall\n0.11913\n\n\n102\nËôï\n17898\n95\n15036\nall\n0.11902\n\n\n103\nË≠¶\n17804\n96\n15484\nall\n0.11840\n\n\n104\nÁ∏ΩÁµ±\n17774\n97\n13169\nall\n0.11820\n\n\n105\nÂê≥\n17680\n98\n12860\nall\n0.11757\n\n\n107\nÂºµ\n17594\n99\n11644\nall\n0.11700\n\n\n108\nÈÄ≤Ë°å\n17570\n100\n15739\nall\n0.11684\n\n\n\n\n\n\ntable_FreqWord_Top100 head(‚Ä¶, 100): We isolate the top 100 words again after our cleaning process.\ntable_FreqWord_Top100$frequency/sum(table_FreqWord$frequency): We divide the frequency of a specific word by the total frequency of all words in the corpus.\n*100: Convert the decimal to a percentage.\nround(‚Ä¶, 5): Round the result to 5 decimal places for readability.\n\n\n\n\nSave the data R\n\nwrite.xlsx(table_FreqWord_Top100, \"ArticleETToday_Top100nouns.xlsx\") save(table_FreqWord_Top100, file = \"ArticleETToday_Top100nouns.Rdata\") \n\n\nwrite.xlsx: Saves the top 100 words table to Excel.\nsave: Saves the R object for later use.",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 12: Data analysis"
    ]
  },
  {
    "objectID": "Weeks14.html",
    "href": "Weeks14.html",
    "title": "Week 14: Presenting a corpus-based study",
    "section": "",
    "text": "Objectives\n\n\n\nThis week, you will learn how to present a corpus-based study, and especially: - What are the most relevant information to present? How to find them in the R scripts that you wrote? - What information can you skip? - What document should you provide?\n\n\n[Page to be completed later]",
    "crumbs": [
      "Welcome page",
      "Part 3: Simple preprocessing and analyses of corpus data",
      "Week 14: Presenting a corpus-based study"
    ]
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "Week 3: Types of corpora: Selecting or building a corpus?",
    "section": "",
    "text": "Objectives\n\n\n\nThis week, we will discuss what a corpus is. Especially, you will learn: - What are the basics of a corpus - What corpora are available on the Internet - When you should decide to build your own corpus\n\n\nWe will have in-class discussions based on the following chapters.\n\n\n\n\n\n\n\nBook\nChapters to read\n\n\n\n\n\n\nChapter 2, Chapter 8\n\n\n\n\nChapter 15\n\n\n\n\n\n\n\n\n\nHomeworks for next week\n\n\n\nYou have two tasks for next week:\n1/ You can try to install R and RStudio on your computer before the class, and read the documentation.\n2/ You can keep thinking about a topic that you find interesting.\n\n\n\n\n\n\n\n\nVERY IMPORTANT\n\n\n\nStarting from next week, you will need to bring your laptop to the class!",
    "crumbs": [
      "Welcome page",
      "Part 1: Principles of corpus studies",
      "Week 3: Types of corpora: Selecting or building a corpus?"
    ]
  },
  {
    "objectID": "InstallationRRStudio_slides.html#what-is-r-what-is-rstudio",
    "href": "InstallationRRStudio_slides.html#what-is-r-what-is-rstudio",
    "title": "Section 1.1",
    "section": "What is R? What is RStudio?",
    "text": "What is R? What is RStudio?\nWe are actually referring to two programs:\n\n\n\nMain program: R\n\nThis is where all the magic happens.\n\nInterface: RStudio\n\nUser-friendly interface such that it is easier to work with."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#what-is-the-difference-between-the-two-a-comparison",
    "href": "InstallationRRStudio_slides.html#what-is-the-difference-between-the-two-a-comparison",
    "title": "Section 1.1",
    "section": "What is the difference between the two? A comparison",
    "text": "What is the difference between the two? A comparison\n\nHow you are using your computerHow R is used\n\n\nr fontawesome::fa(\"computer\", a11y = \"sem\", height = \"2.1em\", width = \"2.3em\") r fontawesome::fa(\"circle-right\", a11y = \"sem\", height = \"2.1em\", width = \"2em\", margin_left = \"0.9em\", margin_right = \"0.9em\") r fontawesome::fa(\"folder-open\", a11y = \"sem\", fill = \"#2b579a\", height = \"2.1em\", width = \"2em\") r fontawesome::fa(\"circle-right\", a11y = \"sem\", height = \"2.1em\", width = \"2em\", margin_left = \"0.9em\", margin_right = \"0.9em\") r fontawesome::fa(\"file-word\", a11y = \"sem\", fill = \"#2b579a\", height = \"2.1em\", width = \"1.9em\", margin_right = \"0.7em\") r fontawesome::fa(\"file-excel\", a11y = \"sem\", fill = \"#217346\", height = \"2.1em\", width = \"1.9em\", margin_right = \"0.7em\") r fontawesome::fa(\"file-powerpoint\", a11y = \"sem\", fill = \"#d24726\", height = \"2.1em\", width = \"1.9em\") r fontawesome::fa(\"circle-right\", a11y = \"sem\", height = \"2.1em\", width = \"2em\", margin_left = \"0.9em\", margin_right = \"0.9em\") r fontawesome::fa(\"floppy-disk\", a11y = \"sem\", height = \"2.1em\", width = \"1.9em\")\n\n\n\n\n\n\n\n\nAnd now, think about it\n\n\nYou have done these tasks by using your mouse or your touchpad and clicking on icons (or figures) on your screen.\nWhat you may not know is that when doing so, you have run many lines of codes. Your computer ran them in programs able to perform computations.\nYour role has been to trigger these programs by using the user-friendly interface installed in your computer.\n\n\n\nThis is the exact same thing with R and RStudio.\nR will perform the computations that you ask for. And you will ask R to do stuff by clicking on icons and writing commands using the user-friendly interface of RStudio.\nAnd this is why you need to download two programs!"
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-r-15",
    "href": "InstallationRRStudio_slides.html#download-and-install-r-15",
    "title": "Section 1.1",
    "section": "Download and install R (1/5)",
    "text": "Download and install R (1/5)\nYou can download R for free from the official website here.\n\nYou have to look for the link in the middle of the first paragraph on the front page."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-r-25",
    "href": "InstallationRRStudio_slides.html#download-and-install-r-25",
    "title": "Section 1.1",
    "section": "Download and install R (2/5)",
    "text": "Download and install R (2/5)\nThen, you need to choose your CRAN. Let‚Äôs choose the one in Taiwan. You will need to scroll down the page to find the link."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-r-35",
    "href": "InstallationRRStudio_slides.html#download-and-install-r-35",
    "title": "Section 1.1",
    "section": "Download and install R (3/5)",
    "text": "Download and install R (3/5)\nNow you need to choose the files to download according to the system of your computer. The likeliest to use are for MacOS (if you have a MacBook computer) or for Windows (if your system is Microsoft)."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-r-45",
    "href": "InstallationRRStudio_slides.html#download-and-install-r-45",
    "title": "Section 1.1",
    "section": "Download and install R (4/5)",
    "text": "Download and install R (4/5)\nOn the next page, you will need to choose the subdirectory that you need. For our purposes, we‚Äôll only need the ‚Äúbase‚Äù subdirectory."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-r-55",
    "href": "InstallationRRStudio_slides.html#download-and-install-r-55",
    "title": "Section 1.1",
    "section": "Download and install R (5/5)",
    "text": "Download and install R (5/5)\nWe finally got to the last page!\n\nJust click on the first link to download the files, as in the image. Wait until the file is downloaded, open it and follow the instructions to install R on your computer."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-rstudio-13",
    "href": "InstallationRRStudio_slides.html#download-and-install-rstudio-13",
    "title": "Section 1.1",
    "section": "Download and install RStudio (1/3)",
    "text": "Download and install RStudio (1/3)\nNow that R is installed on your computer, it is time to do the same with RStudio. First, let‚Äôs go to the RStudio website by clicking here , and you will see something like that (if not, it is just that the RStudio website has changed):"
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-rstudio-23",
    "href": "InstallationRRStudio_slides.html#download-and-install-rstudio-23",
    "title": "Section 1.1",
    "section": "Download and install RStudio (2/3)",
    "text": "Download and install RStudio (2/3)\nOnce you are on the front page, just scroll down to look for the links to download the installing files. Again, you will need to choose the right file to download according to the system of your computer: Window, macOS (for the most common), or another one."
  },
  {
    "objectID": "InstallationRRStudio_slides.html#download-and-install-rstudio-33",
    "href": "InstallationRRStudio_slides.html#download-and-install-rstudio-33",
    "title": "Section 1.1",
    "section": "Download and install RStudio (3/3)",
    "text": "Download and install RStudio (3/3)\n\nJust click on the link and the file will start to be downloaded! Again, give it some minutes, then open the file and follow the instructions to install RStudio on your computer."
  },
  {
    "objectID": "Week5_2.html",
    "href": "Week5_2.html",
    "title": "Week 5: Introduction to R (2/2): How to create reproducible data",
    "section": "",
    "text": "Objectives\n\n\n\nIn this section, we will focus more on one type of R scripts: R Markdown files. The reason is that we are going to use R Markdowns instead of raw R scripts from now on, so we need to know more about how R Markdown files work in addition to their potential.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): How to create reproducible data"
    ]
  },
  {
    "objectID": "Week5_2.html#configuring-the-file-the-yaml-block",
    "href": "Week5_2.html#configuring-the-file-the-yaml-block",
    "title": "Week 5: Introduction to R (2/2): How to create reproducible data",
    "section": "2.1 Configuring the file: The YAML block",
    "text": "2.1 Configuring the file: The YAML block\n\n2.1.1 Basic features of the YAML block\nLet‚Äôs start with configuring the file: This corresponds to the first lines, or the header, of the script that you have just created. This is called the YAML part. It looks like this so far:\n\n\n\nYou will remark that there are pre-filled information, such as the title of the document, the name(s) of the author(s), the date and the output format. These are what we were prompted to fill when creating the document! Now you can try to change some things, for instance:\n\nTitle: Change to ‚ÄúMy first R Markdown script‚Äù\nAuthor: Change to you name\nDate: Change to the date you wish, or even delete this line if you do not wish to add a date\n\n\n\n2.1.2 More advanced features of the YAML block\nYou can also add other pieces of information, for instance:\n\nDate:\nYou have more options, such as defining the format of the date, and maybe more interestingly, create a variable such that you do not have to change the date anytime you save the document. In other words, you may want to have a line specifying something like ‚ÄúThis document was last modified on ‚ÄòYear-Month-Day‚Äô‚Äù. Just write as follows:\n\n\ndate: \"This document was last modified on `r Sys.Date()`\"\n\nThe code 2025-11-30 will look at the date of the system of your computer.\n\nTable of contents:\nThe addition of titles in the document is here to help when navigating it. You can also add a table of contents such that it is even easier to navigate. You can also specify how many layers you want to display in the table of contents.\n\n\ntoc: true\ntoc-depth: 4 ## Change this value depending on how many layers you wish to display\n\n\nFormat:\nFor now, the output format is ‚Äúhtml‚Äù. Just replace with other values according to the output format you want (for example: ‚Äúpdf_document‚Äù).\n\nIn the end, the YAML block should look like this:\n\n    ---\n    title: \"My first R Markdown script\"\n    author: \"Your name\"\n    date: \"This document was last modified on `r Sys.Date()`\"\n    output: pdf_document\n    toc: yes\n    toc-depth: 4\n    ---",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): How to create reproducible data"
    ]
  },
  {
    "objectID": "Week5_2.html#navigating-the-file-titles-and-subparts",
    "href": "Week5_2.html#navigating-the-file-titles-and-subparts",
    "title": "Week 5: Introduction to R (2/2): How to create reproducible data",
    "section": "2.2 Navigating the file: Titles and subparts",
    "text": "2.2 Navigating the file: Titles and subparts\nThe titles and subparts are here to give the overall structure of the document. Please note that they are optional: It is possible to write a whole R Markdown document without any title! But as the code gets longer, it can easily become messy to the point that it is very difficult to understand what we are doing. In other words, the titles and subparts are here to clarify everything!\nAdding titles and subparts is very simple. You just need to click on the button where the header options can be found. For example, if you want to write the title of a main part (let‚Äôs say, ‚Äú1. Introduction‚Äù), you click on ‚ÄúHeader 1‚Äù, and you write the name of the title. It is the same process for subtitles. Let‚Äôs say you want a subpart called ‚Äú1.1 Introduction within the introduction‚Äù, you just click on ‚ÄúHeader 2‚Äù and you write what you want for the subtitle.\n\n\n\n\n\n\nTip\n\n\n\nI personally prefer when the structure is numbered, this is why I add the number of the sections like ‚Äú1. Introduction‚Äù, or ‚Äú1.1 Introduction of the introduction‚Äù. But this is not obligatory and depends on your style!\n\n\nThe titles and subparts are also very important to set the table of contents!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): How to create reproducible data"
    ]
  },
  {
    "objectID": "Week5_2.html#writing-the-content",
    "href": "Week5_2.html#writing-the-content",
    "title": "Week 5: Introduction to R (2/2): How to create reproducible data",
    "section": "2.3 Writing the content",
    "text": "2.3 Writing the content\n\n2.3.1 Writing the narrative\nIt couldn‚Äôt be simpler: You can write directly inside the document. Just do not forget to check that the style is set as ‚ÄúNormal‚Äù and not ‚ÄúHeader 1‚Äù, ‚ÄúHeader 2‚Äù, etc. when doing so!\nThis is a big difference with R Script. Everything you write in an R Script is considered as a code, unless you use the hashtag signs to tell R that this is a comment. In other words, everything that you write in the document is considered as a comment, so do not hesitate to write as much as you want as long as it makes things clearer for you!\n\n\n2.3.2 Writing the code\nIf what we write inside the document is not considered as a code, then how to add one?\nYou Just need to click on ‚ÄúInsert‚Äù, then ‚ÄúCode Chunk‚Äù, and finally ‚ÄúR‚Äù. Normally, a grey code block will appear with {r} at the top line to indicate that this is an R code block. Everything written in this grey block will be considered as R code. You can also add comments inside the block using hashtags as you can do in a typical R Script. Sometimes, R will display simple messages and/or warnings at the same time as you run the code. It is possible to ‚Äúhide‚Äù them, you just need to add the following lines below {r}, as below.\nYour code chunk will look like this:\n\n\n\nFinally, you can run the code chunk without running ALL the document. Can you see the green ‚Äòplay‚Äô logo on the top right of the code chunk? You just need to click on it!",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Week 5: Introduction to R (2/2): How to create reproducible data"
    ]
  },
  {
    "objectID": "Weeks8_9.html#what-is-the-task",
    "href": "Weeks8_9.html#what-is-the-task",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "2.1 What is the task",
    "text": "2.1 What is the task\nAgain, you need to be very clear about what you want to do, and the best way is to start with manually annotating what you want to do.\nSo here is your task:\n\nGo to the website, and search for the section related to classic Chinese literature. Then act as if you wanted to copy and paste the content of one book, of a second bool, etc.\nDescribe each step you take to get to the chapters/sections of a book, and document the changes that happen regarding the address of the website.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "Weeks8_9.html#documentation-of-the-manual-steps",
    "href": "Weeks8_9.html#documentation-of-the-manual-steps",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "2.2 Documentation of the manual steps",
    "text": "2.2 Documentation of the manual steps\n\nAccessing the individual books\nFirst, you went to the website by clicking on the link, and you looked for the relevant section. Then, you selected one book, and you picked the first chapter. You copied and pasted in a Word or Excel document. Third, you repeated this action with the other chapters, and once you completed the first book, you continued with the second one, third one, etc.\nThis procedure looks like this:\n\n\n\n\n\n\nNow you can try one more time and pay more attention to the web address, and track the changes that occur, as you did in Weeks 6 and 7. You will remark that while the changes make sense (use of the first letter of the romanization of the name of the authors), but it is not an easy task to predict these by yourself.\nWe have to use an alternative method. If you recall the steps we went through in the past two weeks, we know that the web address of the individual books and chapters must be somewhere on the website‚Ä¶ but where? Again, this is the moment when it is important to understand the structure of the website.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "Weeks8_9.html#more-about-the-structure-of-the-website",
    "href": "Weeks8_9.html#more-about-the-structure-of-the-website",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "2.3 More about the structure of the website",
    "text": "2.3 More about the structure of the website\nWe are clear about the task now: We want to get the data from classic Chinese books. We could start with the original link of the website, and try to find the link to the webpage by parsing the pages one by one. But sometimes, it is just easier to go manually, so we will directly start from the page listing all the classic Chinese books: https://millionbook.net/gd/gdxs.html.\nThe real game begins: How to find the links to the webpages of the individual books? We know from our manual check in the previous section that we cannot really guess the links. So we have no choice but to look at the structure of the website.\n\n\n\n\n\n\nDo you remember how to access the structure of the website?\n\n\n\nThis is the same step as in Weeks 6 and 7 described here.\n\n\nOnce you accessed the structure of the webpage, it is time to play hide and seek and look for one book, let‚Äôs sat the first one: Á¥ÖÊ®ìÂ§¢ (Dream of the Red Chamber). You will find these lines:\n\n&lt;TR&gt;\n  &lt;TD WIDTH=\"33%\" class=\"tt1\"&gt;\n    &lt;A HREF=\"c/caoxueqing/hlm/index.html\" &gt;Á¥ÖÊ®ìÂ§¢&lt;/A&gt;(ÊõπÈõ™Ëäπ)\n  &lt;/TD&gt;\n\n  &lt;TD WIDTH=\"33%\" class=\"tt1\"&gt;\n    &lt;B&gt;\n      &lt;A HREF=\"l/luoguanzhong/index.html\" &gt;ÁæÖË≤´‰∏≠ÊñáÈõÜ&lt;/A&gt;\n    &lt;/B&gt;\n  &lt;/TD&gt;\n\n  &lt;TD WIDTH=\"33%\" class=\"tt1\"&gt;\n    &lt;A HREF=\"s/shinaian/shz/index.html\" &gt;Ê∞¥Êª∏ÂÇ≥&lt;/A&gt;(ÊñΩËÄêÂ∫µ)\n  &lt;/TD&gt;\n&lt;/TR&gt;\n\n\n\n\n\n\n\nPay attention\n\n\n\nEach website has its own organization, and here you have an example:\n\nSome HTML codes are written in capital letters, but this is not relevant to scrape the data\nThe links to the individual books are shorten, which means that we will have to recreate them\nThe structure is not always consistent, so we need to keep this in mind during the scraping process\n\n\n\nThe most challenging part here is the fact the the links are shorten. If you click on them, you actually obtain something like that:\n\nhttps://millionbook.net/gd/c/caoxueqing/hlm/index.html\n\nThis will be an additional step to add in the code.\n\nAccessing the individual chapters\nLet‚Äôs access the first book with the link above. The website looks like that:\n\n\n\nWe have a table with the chapter number on the left grey part, and on the right the title of the chapter with the hyperlink. The structure looks like this:\n\n\n\n&lt;tr&gt;\n  &lt;td&gt;Á¨¨ ‰∏Ä Âõû&nbsp;&lt;/td&gt;\n  &lt;td&gt;\n    &lt;a href=\"001.htm\" &gt;ÁîÑÂ£´Èö±Â§¢ÂπªË≠òÈÄöÈùà&nbsp;&lt;/a&gt;\n    &lt;br&gt;\n    &lt;a href=\"001.htm\" &gt;Ë≥àÈõ®ÊùëÈ¢®Â°µÊÄÄÈñ®ÁßÄ&nbsp;&lt;/a&gt;\n  &lt;/td&gt;\n\n  &lt;td&gt;Á¨¨ ‰∫å Âõû&nbsp;&lt;/td&gt;\n  &lt;td&gt;\n    &lt;a href=\"002.htm\" &gt;Ë≥àÂ§´‰∫∫‰ªôÈÄùÊèöÂ∑ûÂüé&nbsp;&lt;/a&gt;\n    &lt;br&gt;\n    &lt;a href=\"002.htm\" &gt;ÂÜ∑Â≠êËààÊºîË™™Ê¶ÆÂúãÂ∫ú&lt;/a&gt;\n  &lt;/td&gt;\n&lt;/tr&gt;\n\nThe &lt;tr&gt; and &lt;td&gt; elements are here to create the table. &lt;br&gt; means ‚Äúgo to the next line‚Äù, and this is why the titles of the chapters are on two lines on the website.\nBut you will remark two things when observing the links:\n\nAgain, they are shorten. If you click on them, you actually have something like that: https://millionbook.net/gd/c/caoxueqing/hlm/001.htm. So we know that this is another link we will have to reconstruct. The good news is that this is very predictable.\nThe same links appear two times. This will require extra work when writing the code, but we already encountered the same problem in the past two weeks. So nothing impossible to do!\n\n\n\nAccessing the content of the chapter\nLet‚Äôs click on the link to Chapter 1, and let‚Äôs have a look at the webpage:\n\nWe have here:\n\nThe title of the chapter in red\nThe text of the chapter in black\nThree links leading to the previous chapter, the next chapter and the table of contents in blue\n\nAgain, let‚Äôs look at the structure of the website:\n\n&lt;TD class=\"tt2\" bgcolor=\"#F1F1F3\" width=\"84%\"&gt;\n  &lt;center&gt;\n    &lt;FONT style=\"FONT-SIZE: 12pt\" COLOR=\"#FF5555\"&gt;\n      &lt;B&gt;Á¨¨‰∏ÄÂõû„ÄÄÁîÑÂ£´Èö±Â§¢ÂπªË≠òÈÄöÈùà„ÄÄË≥àÈõ®ÊùëÈ¢®Â°µÊÄÄÈñ®ÁßÄ\n      &lt;/B&gt;\n  &lt;/center&gt;\n    &lt;/FONT&gt;\n\n&lt;hr color=\"#EE9B73\" size=\"1\" width=\"94%\"&gt;\n  \n&lt;BR&gt;\n„ÄÄ„ÄÄÊ≠§ÈñãÂç∑Á¨¨‰∏ÄÂõû‰πüÔºé‰ΩúËÄÖËá™‰∫ëÔºöÂõ†ÊõæÊ≠∑ÈÅé‰∏ÄÁï™Â§¢Âπª‰πãÂêéÔºåÊïÖÂ∞áÁúü‰∫ãÈö±ÂéªÔºåËÄåÂÄü\"ÈÄöÈùà\"‰πãË™™ÔºåÊí∞Ê≠§„ÄäÁü≥È†≠Ë®ò„Äã‰∏ÄÊõ∏‰πüÔºéÊïÖÊõ∞\"ÁîÑÂ£´Èö±\"‰∫ë‰∫ëÔºé‰ΩÜÊõ∏‰∏≠ÊâÄË®ò‰Ωï‰∫ã‰Ωï‰∫∫ÔºüËá™Âèà‰∫ëÔºö‚Äú‰ªäÈ¢®Â°µÁ¢åÁ¢åÔºå‰∏Ä‰∫ãÁÑ°ÊàêÔºåÂøΩÂøµÂèäÁï∂Êó•ÊâÄÊúâ‰πãÂ•≥Â≠êÔºå‰∏Ä‰∏ÄÁ¥∞ËÄÉËºÉÂéªÔºåË¶∫ÂÖ∂Ë°åÊ≠¢Ë¶ãË≠òÔºåÁöÜÂá∫‰∫éÊàë‰πã‰∏äÔºé‰ΩïÊàëÂ†ÇÂ†ÇÈ†àÁúâÔºåË™†‰∏çËã•ÂΩºË£ôÈáµÂìâÔºüÂØ¶ÊÑßÂâáÊúâ‰ΩôÔºåÊÇîÂèàÁÑ°Áõä‰πãÂ§ßÁÑ°ÂèØÂ¶Ç‰Ωï‰πãÊó•‰πüÔºÅÁï∂Ê≠§ÔºåÂâáËá™Ê¨≤Â∞áÂ∑≤ÂæÄÊâÄË≥¥Â§©ÊÅ©Á•ñÂæ∑ÔºåÈå¶Ë°£Á¥àË§≤‰πãÊôÇÔºåÈ£´ÁîòÈ•úËÇ•‰πãÊó•ÔºåËÉåÁà∂ÂÖÑÊïôËÇ≤‰πãÊÅ©ÔºåË≤†Â∏´ÂèãË¶èË´á‰πãÂæ∑Ôºå‰ª•Ëá≥‰ªäÊó•‰∏ÄÊäÄÁÑ°ÊàêÔºåÂçäÁîüÊΩ¶ÂÄí‰πãÁΩ™ÔºåÁ∑®Ëø∞‰∏ÄÈõÜÔºå‰ª•ÂëäÂ§©‰∏ã‰∫∫ÔºöÊàë‰πãÁΩ™Âõ∫‰∏çÂÖçÔºåÁÑ∂Èñ®Èñ£‰∏≠Êú¨Ëá™Ê≠∑Ê≠∑Êúâ‰∫∫Ôºå‰∏á‰∏çÂèØÂõ†Êàë‰πã‰∏çËÇñÔºåËá™Ë≠∑Â∑±Áü≠Ôºå‰∏ÄÂπ∂‰ΩøÂÖ∂Ê≥ØÊªÖ‰πüÔºéÈõñ‰ªäÊó•‰πãËåÖÊ§ΩËì¨ÁâñÔºåÁì¶ÁÅ∂Áπ©Â∫äÔºåÂÖ∂Êô®Â§ïÈ¢®Èú≤ÔºåÈöéÊü≥Â∫≠Ëä±Ôºå‰∫¶Êú™ÊúâÂ¶®Êàë‰πãË•üÊÄÄÁ≠ÜÂ¢®ËÄÖÔºéÈõñÊàëÊú™Â≠∏Ôºå‰∏ãÁ≠ÜÁÑ°ÊñáÔºåÂèà‰ΩïÂ¶®Áî®ÂÅáË™ûÊùëË®ÄÔºåÊï∑ÊºîÂá∫‰∏ÄÊÆµÊïÖ‰∫ã‰æÜÔºå‰∫¶ÂèØ‰ΩøÈñ®Èñ£Êò≠ÂÇ≥ÔºåÂ§çÂèØÊÇÖ‰∏ñ‰πãÁõÆÔºåÁ†¥‰∫∫ÊÑÅÊÇ∂Ôºå‰∏ç‰∫¶ÂÆú‰πéÔºü\"ÊïÖÊõ∞\"Ë≥àÈõ®Êùë\"‰∫ë‰∫ëÔºé\n&lt;BR&gt;\n&lt;BR&gt;\n„ÄÄ„ÄÄÊ≠§Âõû‰∏≠Âá°Áî®‚ÄúÂ§¢‚ÄùÁî®‚ÄúÂπª‚ÄùÁ≠âÂ≠óÔºåÊòØÊèêÈÜíÈñ±ËÄÖÁúºÁõÆÔºå‰∫¶ÊòØÊ≠§Êõ∏Á´ãÊÑèÊú¨Êó®Ôºé\n&lt;BR&gt;\n&lt;BR&gt;\n„ÄÄ„ÄÄÂàó‰ΩçÁúãÂÆòÔºö‰Ω†ÈÅìÊ≠§Êõ∏Âæû‰ΩïËÄå‰æÜÔºüË™™Ëµ∑Ê†πÁî±ÈõñËøëËçíÂîêÔºåÁ¥∞ÊåâÂâáÊ∑±ÊúâË∂£Âë≥ÔºéÂæÖÂú®‰∏ãÂ∞áÊ≠§‰æÜÊ≠∑Ê≥®ÊòéÔºåÊñπ‰ΩøÈñ±ËÄÖ‰∫ÜÁÑ∂‰∏çÊÉëÔºé\n&lt;/TD&gt;\n\nThere are many things to unpack here:\n\nYou will remark that the text as within a &lt;td&gt;&lt;/td&gt; element, and its ‚Äúclass‚Äù is ‚Äútt2‚Äù\nWithin this element, you have:\n\n&lt;center&gt; and &lt;font&gt; elements (not very well organized), and these correspond to the title in red\nan &lt;hr&gt; element, which correspond to the orange line between the title and the main text\nthe &lt;br&gt; element one more time\nand finally the main text, without specific class\n\n\n\n\n\n\n\n\nThink about it\n\n\n\nThe text is therefore directly within the &lt;td&gt; element called ‚Äútt2‚Äù. So this is the only way to access the text‚Ä¶ but the title is also within this element!\nThis means that we have no choice but scraping the title of the chapter as well. This is important to note such details as we will have to handle them when preprocessing the data.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "Weeks8_9.html#prepare-the-environment",
    "href": "Weeks8_9.html#prepare-the-environment",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "3.1 Prepare the environment",
    "text": "3.1 Prepare the environment\nThis initial chunk loads all the necessary R packages (libraries) required for the script to run.\n\nlibrary(rvest): The main package for web scraping. It‚Äôs used to read HTML web pages and extract information from them.\nlibrary(dplyr): A powerful data manipulation package. It‚Äôs used here for its %&gt;% (pipe) operator, which makes the code more readable by chaining commands together, and for functions like subset.\nlibrary(xml2): A package that rvest depends on to parse and handle the underlying HTML and XML data.\nlibrary(openxlsx): Used in the final step to save the scraped data as a .xlsx Excel file.\nlibrary(stringr): A package for working with text (strings). It‚Äôs used here for functions like str_detect (to check if a URL contains ‚Äú.jpg‚Äù or ‚Äúindex.html‚Äù) and gsub (to modify URLs).\n\nHere is an explanation of the R code from your file, broken down by each section.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "Weeks8_9.html#scraping",
    "href": "Weeks8_9.html#scraping",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "3.2 Scraping",
    "text": "3.2 Scraping\nThis section is the core of the script, performing the actual data extraction from the website.\n\nList the URLs of the Chinese classic books\n\nPageUrl &lt;- \"https://millionbook.net/gd/gdxs.html\" \nPageClassChinLit &lt;- read_html(PageUrl)\n\nListBooks &lt;- PageClassChinLit %&gt;% \n  html_nodes(\".tt1\") %&gt;% \n  html_text()\n\nListBooks &lt;- ListBooks[-c(90)] # Remove blank one\n\nListURL &lt;- PageClassChinLit %&gt;%\n  html_nodes(\".tt1\") %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"href\")\n\nListBooks &lt;- as.data.frame(ListBooks)\nListBooks$ListURL &lt;- paste0(\"https://millionbook.net/gd/\", ListURL)\n\n## Check if books finish with \"index.html\"\nListBooks$has_index &lt;- str_detect(ListBooks$ListURL, \"index.html\")\n\n## Remove books without index (other scraping process)\nListBooks$has_index &lt;- as.factor(ListBooks$has_index)\nListBooks &lt;- droplevels(subset(ListBooks, has_index == TRUE))\nrownames(ListBooks) &lt;- NULL\n\nThis part gets the list of all available books from the main classical literature index page.\n\nPageUrl &lt;- \"...\": Sets the target URL to the main index page.\nread_html(PageUrl): Downloads the HTML content of that page.\nhtml_nodes(\".tt1\"): Selects all HTML elements that have the CSS class ‚Äútt1‚Äù. Based on the code, these elements contain the book titles.\nhtml_text(): Extracts the plain text (the titles) from those elements.\nListBooks[-c(90)]: Removes the 90th item, which the author noted was a blank entry.\nhtml_nodes(\"a\") %&gt;% html_attr(\"href\"): This finds the &lt;a&gt; (link) tags within the ‚Äú.tt1‚Äù elements and extracts their href attribute (the actual URL, e.g., ‚Äúbook_name/index.html‚Äù).\nListBooks &lt;- as.data.frame(ListBooks): Converts the list of titles into a data frame.\npaste0(...): Creates full, absolute URLs by combining the base URL (https://millionbook.net/gd/) with the relative URLs (e.g., ‚Äúbook_name/index.html‚Äù) extracted earlier.\nstr_detect(...) & subset(...): This is a data cleaning step. The code checks which URLs end in ‚Äúindex.html‚Äù (which are assumed to be book-level index pages) and removes any that don‚Äôt, ensuring the list only contains valid books to scrape.\n\n\n\nRetrieve the URLs of the chapters of a book\nThis section digs one level deeper. It goes into each book‚Äôs index page to find the URLs for all of its chapters.\n\nOne page to test\n\nIndBook &lt;- read_html(ListBooks$ListURL[81])\n\nChapterURL &lt;- IndBook %&gt;%\n  html_nodes(\"td\") %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\")\n\nChapterURL &lt;- ChapterURL[-c(1, 2, 3)]\nprint(ChapterURL)\nChapterURL &lt;- unique(ChapterURL)\nprint(ChapterURL)\n\n## Reconstruct the full URL\nTempBookURL &lt;- gsub(\"index.html\", \"\", ListBooks$ListURL[81])\nFullChapterURL &lt;- paste0(TempBookURL, ChapterURL)\nprint(FullChapterURL)\n\nThis chunk is a test run on a single book (ListBooks$ListURL[81]) to make sure the logic works before applying it to all books.\n\nread_html(...): Downloads the HTML for one book‚Äôs index page.\nhtml_nodes(\"td\") %&gt;% html_nodes(\"a\") %&gt;% html_attr(\"href\"): Finds all links (&lt;a&gt;) that are inside table cells (&lt;td&gt;) and gets their URLs. This is how the chapter list is structured on that page.\nChapterURL &lt;- ChapterURL[-c(1, 2, 3)]: Removes the first three links, which are likely navigation links (e.g., ‚ÄúHome,‚Äù ‚ÄúBack‚Äù) and not chapters.\nunique(ChapterURL): Removes any duplicate chapter links.\ngsub(...) & paste0(...): This reconstructs the full chapter URLs. It takes the book‚Äôs URL (e.g., .../book_name/index.html), removes the ‚Äúindex.html‚Äù part to get a base directory, and then appends the specific chapter file (e.g., 001.html) to it.\n\n\n\nFind the URLs of the chapters from all the books\n\nURL_BookChapters_total &lt;- data.frame(matrix(ncol = 1, nrow = 0))\n\nfor (i in 1:length(ListBooks$ListURL)){\n  IndBook &lt;- read_html(ListBooks$ListURL[i])\n  \n  ChapterURL &lt;- IndBook %&gt;%\n    html_nodes(\"td\") %&gt;%\n    html_nodes(\"a\") %&gt;%\n    html_attr(\"href\")\n  \n  ChapterURL &lt;- ChapterURL[-c(1, 2, 3)]\n  ChapterURL &lt;- unique(ChapterURL)\n  \n  TempBookURL &lt;- gsub(\"index.html\", \"\", ListBooks$ListURL[i])\n  FullChapterURL &lt;- paste0(TempBookURL, ChapterURL)\n  \n  URL_BookChapters &lt;- (data.frame(ChapterURL = FullChapterURL,\n                       Book = ListBooks$ListBooks[i]))\n  \n  URL_BookChapters_total &lt;- rbind(URL_BookChapters_total, URL_BookChapters)\n}\n\n## Remove links to images (.jpg)\n## Check if books finish with \"index.html\"\nURL_BookChapters_total$image &lt;- str_detect(URL_BookChapters_total$ChapterURL, \".jpg\")\nURL_BookChapters_total$image &lt;- as.factor(URL_BookChapters_total$image)\nURL_BookChapters_total &lt;- droplevels(subset(URL_BookChapters_total, image != TRUE))\nrownames(URL_BookChapters_total) &lt;- NULL\n\nThis part applies the logic from the test (2.2.1) to every book found in section 2.1.\n\nURL_BookChapters_total &lt;- data.frame(...): Creates an empty data frame to store all the chapter URLs from all books.\nfor (i in 1:length(ListBooks$ListURL)): A loop that iterates through each book URL.\nInside the loop, it repeats the exact same logic from 2.2.1 (read page, find links, clean, create full URLs).\nURL_BookChapters &lt;- (data.frame(...)): Creates a small data frame for the current book‚Äôs chapters, helpfully including the book‚Äôs title.\nrbind(...): Appends this small data frame to the main URL_BookChapters_total data frame.\nPost-loop Cleaning: After the loop finishes, str_detect is used to find and remove any links that mistakenly point to .jpg image files instead of text chapters. The comments note that inconsistent site structure for a few books caused issues, which this cleaning step helps fix.\n\n\n\n\nScrape the content of the chapters\nThis is the final scraping step, where the script visits every single chapter URL and downloads the actual text.\n\nTest with one page\n\nChapter &lt;- data.frame()\n\nj = 3672\n\nOneChapter &lt;- read_html(URL_BookChapters_total$ChapterURL[j])\n\nChapterContent &lt;- OneChapter %&gt;%\n  html_nodes(\".tt2\") %&gt;%\n  html_text()\n\nChapterTitle &lt;- sub(\"\\r.*\", \"\", ChapterContent)\n\nChapter &lt;- (data.frame(Book = URL_BookChapters_total$Book[j],\n                       Title = ChapterTitle,\n                       Content = ChapterContent,\n                       URL = URL_BookChapters_total$ChapterURL[j]))\n\nhead(Chapter)\n\nAgain, this is a test on a single chapter (j = 3672) to finalize the logic.\n\nread_html(...): Downloads the HTML for one chapter page.\nhtml_nodes(\".tt2\") %&gt;% html_text(): Selects the HTML element with the class ‚Äútt2‚Äù (where the main content is stored) and extracts all the text from it.\nsub(\"\\r.*\", \"\", ChapterContent): A clever trick to get the title. It assumes the title is the very first line of the text. This command finds the first carriage return (\\r, a line break) and removes everything after it, leaving just the first line.\ndata.frame(...): Organizes the scraped data (Book, Title, Content, URL) into a data frame.\n\n\n\nScrape all the chapters for all book\n\nBook_total &lt;- data.frame()\n\n\nfor (j in 1:length(URL_BookChapters_total$ChapterURL)){\n  tryCatch({\n\n    OneChapter &lt;- read_html(URL_BookChapters_total$ChapterURL[j])\n    \n    ChapterContent &lt;- OneChapter %&gt;%\n      html_nodes(\".tt2\") %&gt;%\n      html_text()\n    \n    ChapterTitle &lt;- sub(\"\\r.*\", \"\", ChapterContent)\n    \n    Chapter &lt;- (data.frame(Book = URL_BookChapters_total$Book[j],\n                           Title = ChapterTitle,\n                           Content = ChapterContent,\n                           URL = URL_BookChapters_total$ChapterURL[j]))\n    Book_total &lt;- rbind(Book_total, Chapter)\n    \n        # Update the progress \n    percentage_completed &lt;- round(j/length(URL_BookChapters_total$ChapterURL) * 100, 2)\n    cat(c(\"Process\", j, \"out of\", length(URL_BookChapters_total$ChapterURL), \"percentage completed:\", percentage_completed, \"%\", \"\\n\"))\n    \n  }, error=function(e){cat(\"ERROR :\",conditionMessage(e), \"\\n\")})\n}\n\nhead(Book_total)\n\nThis loop executes the final scraping.\n\nBook_total &lt;- data.frame(): Creates a new empty data frame to store the final content.\nfor (j in 1:length(URL_BookChapters_total$ChapterURL)): A loop that iterates through every single chapter URL collected in section 2.2.2.\ntryCatch({...}, error=...): This is a very important safety feature. It ‚Äútries‚Äù to run the scraping code inside {...}. If any error occurs (e.g., a broken link, a network timeout), the error=... function will catch it, print an error message, and continue to the next iteration of the loop. Without this, the entire script would crash on a single bad link.\nInside the tryCatch, it repeats the logic from 2.3.1 (read page, get text, extract title).\nrbind(...): Appends the new chapter‚Äôs data to the Book_total data frame.\ncat(...): Prints a progress message to the console so the user can see how far along the script is.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "Weeks8_9.html#save-the-data",
    "href": "Weeks8_9.html#save-the-data",
    "title": "Weeks 8 & 9: Building a corpus from literature sources",
    "section": "3.3 Save the data",
    "text": "3.3 Save the data\nAfter the main loop has finished collecting all the text, this final section saves the complete dataset.\n\nSave as an Excel file: write.xlsx(Book_total, ...) saves the entire Book_total data frame into an Excel file named ‚ÄúClassicChineseLiterature_CorpusCourse.xlsx‚Äù.\nSave as an RData file: save(Book_total, file = ...) saves the data frame in R‚Äôs native .Rdata format. This format is much faster to load back into R for future analysis.",
    "crumbs": [
      "Welcome page",
      "Part 2: Building a corpus from the internet with modern tools",
      "Weeks 8 & 9: Building a corpus from literature sources"
    ]
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Aymeric Collart, Ph.D\nAssistant Professor Department of Chinese  National Taiwan Normal University   Personal website: https://aymeric-collart.github.io/ Contact information: aymeric.collart at ntnu.edu.tw"
  },
  {
    "objectID": "intro_slides.html#structure-and-workflow-of-the-tutorial",
    "href": "intro_slides.html#structure-and-workflow-of-the-tutorial",
    "title": "Introduction",
    "section": "Structure and workflow of the tutorial",
    "text": "Structure and workflow of the tutorial\n\n\nClick on the image to zoom in and out"
  },
  {
    "objectID": "intro_slides.html#chapters-and-section-of-the-tutorial",
    "href": "intro_slides.html#chapters-and-section-of-the-tutorial",
    "title": "Introduction",
    "section": "Chapters and section of the tutorial",
    "text": "Chapters and section of the tutorial\n\nChapter 1Chapter 2Chapter 3\n\n\n\n\n\nChapter 1: Getting started\n\n\n\n\n\nDownload and install R/RStudio\nlink\n\n\nPresentation of R/RStudio\nlink\n\n\nWorking with markdown files\nlink\n\n\n\n\n\n\n\n\nChapter 2: Let‚Äôs get our data\n\n\n\n\n\nUnderstanding where the data are on the Internet\nlink\n\n\nScraping the data from the web with R\nlink\n\n\n\n\n\n\n\n\nChapter 3: Cleaning and analyzing the data\n\n\n\n\n\nPreprocess the data: Cleaning and transformations\nlink\n\n\nAnalyzing the data\nlink"
  },
  {
    "objectID": "intro_slides.html#what-this-tutorial-does-not-cover",
    "href": "intro_slides.html#what-this-tutorial-does-not-cover",
    "title": "Introduction",
    "section": "What this tutorial does NOT cover",
    "text": "What this tutorial does NOT cover\n\n\nFundamental concepts of corpus linguistics\nHow to ask a research question and how to motivate it\nAdvanced statistical methods in corpus linguistics (classification analyses, vector space representations)\n\n\n\nIf you are interested in it, you can read the book published by Anatol Stefanowitsch in 2020, available for free here."
  },
  {
    "objectID": "intro_slides.html#how-to-get-through-the-exercises",
    "href": "intro_slides.html#how-to-get-through-the-exercises",
    "title": "Introduction",
    "section": "How to get through the exercises",
    "text": "How to get through the exercises"
  }
]